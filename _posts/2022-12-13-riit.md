---
layout: distill
title: Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-agent Reinforcement Learning
tags: [multi-agent, reinforcement-learning, experimental techniques, monotonicity]

authors:
  - name:  Jian Hu
    url: https://hujian.website/
    affiliations:
      name: National Taiwan University
  - name: Siying Wang
    affiliations:
      name: University of Electronic Science and Technology of China
  - name: Siyang Jiang
    url: https://siyang-jiang.github.io/
    affiliations:
      name: Huizhou University
  - name: Weixun Wang
    url: https://wwxfromtju.github.io/
    affiliations:
      name: Tianjin University, Netease Fuxi AI Lab

toc:
  - name: Background
    subsections:
    - name: From RL to MARL
    - name: Decentralized Partially Observable Markov Decision Process
    - name: Centralized Training with Decentralized Execution and Value Decomposition
    - name: Notations
  - name: QMIX and Monotonicity Constraint
  - name: Extension to QMIX
    subsections:
    - name: Experimental Design
    - name: Optimizer
    - name: Rollout Process Number
    - name: Replay Buffer Size
    - name: Eligibility Traces
    - name: Hidden Size
    - name: Exploration Steps
    - name: Integrating the Techniques
  - name: Role of Monotonicity Constraint
    subsections:
    - name: Amazing Performance in Policy-Based Methods
    - name: What is Under the Hood?
  - name: Conclusion
  - name: Authorship, Credit Attribution and Acknowledgement
  - name: Appendix
  - name: Reference

_styles: >
  figure {
    text-align: center;
  }

  .img-center img {
    margin: 0 auto;
  }

  .img-height-180 img {
    height: 160px;
  }

  .img-height-200 img {
    height: 180px;
  }

  .img-height-210 img {
    height: 180px;
  }

  img-height-240 img {
    height: 220px;
  }

  .img-height-300 img {
    height: 280px;
  }

  .img-height-310 img {
    height: 280px;
  }

  .img-height-340 img {
    height: 315px;
  }

  .img-height-400 img {
    height: 370px;
  }

  .img-height-600 img {
    height: 600px;
  }

  .text{text-align:center;}
---
>QMIX [[8](#8)], a very classical multi-agent reinforcement learning (MARL) algorithm, is often considered to be a weak performance baseline due to its representation capability limitations. However, we found that by improving the implementation techniques of QMIX we can enable it to achieve state-of-the-art on the StarCraft Multi-Agent Challenge (SMAC) testbed [[10](#10)]. Specifically, we evaluated the effect of optimizer, number of rollout processes, replay buffer size, eligibility traces, hidden size and the degree of annealed exploration on QMIX, and tried to explain their role in MARL. Furthermore, the key factor of the monotonicity constraint of QMIX was found in this post, we tried to explain its role and corroborated its superior performance by combining it with another actor-critic style algorithm. We have open-sourced the code at <a href="https://github.com/hijkzzz/pymarl2"> https://github.com/hijkzzz/pymarl2 </a> for researchers to evaluate the effects of these proposed techniques. We hope this research will advance the MARL community and contribute to the establishment of new baselines of QMIX.


## <a name="Background">Background</a>
### <a name="From_RL_to_MARL">From RL to MARL</a>
Since AlphaZero beats humans at Go, RL has become a consistent hot spot in academia and industry. The agent of RL can obtain some rewards by interacting with the environment and taking actions to maximize these cumulative rewards. Actually, almost all the RL problems can be described as **Markov Decision Processes** as illustrated in Figure <a href="#mdp">1</a>.

<div id="mdp" class="img-height-200 img-center"> {% include figure.html path="assets/img/2022-12-13-riit/mdp.png" class="img-fluid rounded z-depth-1" %} </div>
<center>Figure 1: The agent-environment interaction in a Markov decision process. (Image source: Sec. 3.1 Sutton & Barto (2017) <a ref="#14">[14]</a>)). $R_t, S_t, A_t$ denote the reward, state and action at timestep $t$.</center><br/>

Just as its name implies, MARL contains multiple agents trained by RL algorithms in the same environment. Many complex multi-agent systems such as robot swarm control, autonomous vehicle coordination, and sensor networks, can be modeled as MARL tasks. The interaction of these agents would make them work together to achieve a common goal.

<div style="display:flex; margin-bottom:-30px; margin-left :150px; margin-right :150px">
<div id="chase" class="img-height-100"> {% include figure.html path="assets/img/2022-12-13-riit/chase.gif"  class="img-fluid rounded z-depth-1" %} </div>
<div id="magent" class="img-height-100"> {% include figure.html path="assets/img/2022-12-13-riit/magent.gif"  class="img-fluid rounded z-depth-1" %} </div></div>
<div style="display:flex; margin-top:-30px; margin-left :50px; margin-right :50px">
<div id="hide" class="img-height-200"> {% include figure.html path="assets/img/2022-12-13-riit/hide.gif"  class="img-fluid rounded z-depth-1" %} </div>
<div id="smac" class="img-height-200"> {% include figure.html path="assets/img/2022-12-13-riit/smac.gif"  class="img-fluid rounded z-depth-1" %} </div></div>

<div style="margin-bottom: 20px"><center>Figure 2: Some multi-agent cooperative scenarios [from-left-to-right].
<a href="https://github.com/openai/multiagent-particle-envs"> <br/>
(a) Chasing in Multi-Agent Particle Environment (Predator-Prey); </a>
<a href="https://github.com/geek-ai/MAgent"> (b) MAgent Environment; </a>
<a href="https://openai.com/blog/emergent-tool-use"> <br/> (c) Hide & Seek; </a>
<a href="https://github.com/oxwhirl/smac"> (d) StarCraft Multi-Agent Challenge. </a></center></div>

In this general setting, agents usually have a limited sight range to observe their surrounding environment. As shown in Figure <a href="#smac_obs">3</a>, the cyan border indicates the sight and shooting range of the agent, which means the agent could only obtain the information of terrain or other agents in that range. This restricted field of view may also result in the difficulty of agents to access to global state information, making its policy updates subject to bias and unsatisfactory performance. In general, these kinds of multi-agent scenarios can be modeled as **Decentralized Partially Observable Markov Decision Processes** (Dec-POMDP) [[6](#6)]. 

Even though many RL algorithms [[14](#14)] and their variants have been successfully extended to the cooperative scenarios in MARL setting, few of their performance is satisfactory. One of the most troublesome issues is *Non-Stationarity*. Specifically, as a part of the environment, the changing policies of other agents during training would make the observation non-stationary from the perspective of any individual agent [[28](#28)] and significantly slow down the policy optimization of MARL. This situation has forced researchers to seek a method that can exploit global information during training but does not destroy the ability of the agents to only use their respective observations during execution, to find a joint policy $\boldsymbol{\pi} = \langle \pi^{1},...,\pi^{n}\rangle$ to maximize global reward. Naturally, the simplicity and effectiveness of the **Centralized Training with Decentralized Execution** (CTDE) paradigm have attracted the attention of the community, and many MARL algorithms based on CTDE were proposed, making a remarkable contribution to MARL.

In the rest of this section, we briefly introduce Dec-POMDP and CTDE to facilitate the understanding of the contents of MARL, the QMIX algorithm and the following text.

<div style="float:left; margin-left :150px; margin-right :150px;" ><div name="smac_obs" class="img-height-100"> {% include figure.html path="assets/img/2022-12-13-riit/smac_agent_obs.jpg"  class="img-fluid rounded z-depth-1" %} </div>
<center>Figure 3: The partial observation of agents<br/>(Image source: SMAC <a ref="#10">[10]</a>). </center><br/></div>

### <a name="Decentralized_Partially_Observable_Markov_Decision_Process">Decentralized Partially Observable Markov Decision Process</a>

A **Decentralized Partially Observable Markov Decision Process** (Dec-POMDP) model, as described in [[8](#8)][[28](#28)], is typically used to represent a full cooperative multi-agent task. The model consists of a tuple denoted by $G=(S, U, P, r, Z, O, n, \gamma)$, and involves $n$ agents, where $n$ is an integer between 1 and $n$, inclusive. The true state of the environment, denoted by $s \in S$, describes global information that is relevant to both agents and other auxiliary features. At each timestep $t$, a transition in the environment occurs via a joint action $\mathbf{u} \in \mathbf{U} \equiv U^{n}$, which is composed of an action $u^i \in U$, chosen by each agent. This transition is driven by the state transition function $P\left(s^{\prime} \mid s, \mathbf{u}\right): S \times \mathbf{U} \times S \rightarrow[0,1]$. Additionally, there is a shared global reward function, denoted by $r(s, \mathbf{u}): S \times \mathbf{U} \rightarrow \mathbf{R}$, which is optimized by the whole team. Finally, each agent has a partial observation described by $o^i \in O$, which is derived from the observation function $Z(o^i \mid s, u^i) : S \times U \rightarrow O$. All agents work cooperatively to maximize the shared global reward $R_{t}=\sum_{k=0}^{T} \gamma^{k} r_{t+k}$, which is described by the joint value function $$Q^{\boldsymbol{\pi}}\left(s_{t}, \mathbf{u}_{t}\right) = \mathbb{E}_{s_{t+1: \infty}, \mathbf{u}_{t+1: \infty}}\left[R_{t} \mid s_{t}, \mathbf{u}_{t}\right]$$.

### <a name="Centralized_Training_with_Decentralized_Execution_and_Value_Decomposition">Centralized Training with Decentralized Execution and Value Decomposition</a>

To better explore the factors affecting the QMIX algorithm, our focus lies in the **Centralized Training with Decentralized Execution** (CTDE) paradigm of MARL algorithms. These algorithms under this paradigm have access to the true state $s$ and the action-observation histories $\tau^{i}$ of all agents to centrally train policies, but each agent can only rely on its local observation $o^{i}$ for decision-making. Some value-based algorithms implemented under CTDE follow the Individual-Global-Max (**IGM**) principle [[11](#11)], ensuring consistency between the joint action-value function $Q_{tot} \left(\boldsymbol{\tau}, \mathbf{u}\right)$ and individual agent-utilities $[Q_i\left(\tau^i, u^i\right)] _{i=1} ^{n}$:

$$
\underset{\mathbf{u}}{\operatorname{argmax}}\ Q_{tot} \left(\boldsymbol{\tau}, \mathbf{u}\right) = (\underset{u^{1}}{\operatorname{argmax}}\ Q_{1} \left(\tau^{1}, u^{1}\right), \ldots, \underset{u^{n}}{\operatorname{argmax}}\ Q_{n} \left(\tau^{n} , u^{n}\right)). \tag{1} \label{eq1}
$$

One of the most typical ways to efficiently train the joint value function $$Q_{tot} \left(\boldsymbol{\tau}, \mathbf{u}\right)$$ is to decompose it into the utility functions $$[Q_i\left(\tau^i, u^i\right)] _{i=1} ^{n}$$ and maintain updating consistency between them via IGM. The simplest factorization structure, called *additivity*, has been proposed by VDN [[13](#13)], which makes VDN simply factorize $Q_{tot}$ into a sum of per-agent utilities $$Q_{tot}^{\mathrm{VDN}} \left(\boldsymbol{\tau}, \boldsymbol{u}\right)=\sum_{i=1}^{n} Q_{i} \left(\tau^{i}, u^{i}\right)$$. VDN's simplicity and equal weighting of each utility in the joint value function makes it ineffective for cooperative tasks, which has motivated the QMIX structure and other more efficient decomposition approaches.<br/><br/>

### <a name="Notations">Notations</a>

In this subsection, we define the notations used in this post. Specifically, in traditional RL, time steps $t$ are usually represented in the update formula and the value function of RL is considered to be estimated by the pairwise variables at the current time step $t$ and the next time step $t+1$. Since the *ID* of the agent also needs to be represented in the MARL algorithm, it may cause ambiguity when expressed in the same formula as the time step $t$. For simplicity of expression, variables without $t$ are indicated to be implemented at the current time step, while variables at the next time step are indicated with an apostrophe in the upper right corner in the rest of the context, e.g., $s$ means the current state and $s^{\prime}$ indicates the next time step state, the same approach applies to actions $u$ and observations $o$. All the notations are listed in Table [1](#table1).

<a name="table1"> </a>
<center>
    Table 1: All the notations used in this post.
</center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-c3ow">Notation</th>
    <th class="tg-c3ow">Description</th>
    <th class="tg-c3ow">Notation</th>
    <th class="tg-c3ow">Description</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-c3ow">$s$</td>
    <td class="tg-c3ow">the current state (at time $t$)</td>
    <td class="tg-c3ow">$S$</td>
    <td class="tg-c3ow">the set of all states</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$s^{\prime}$</td>
    <td class="tg-c3ow">the next state (at time $t+1$)</td>
    <td class="tg-c3ow">$U$</td>
    <td class="tg-c3ow">the set of all actions</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$u^{i}$</td>
    <td class="tg-c3ow">the action of agent $i$</td>
    <td class="tg-c3ow">$N$</td>
    <td class="tg-c3ow">the set of all agents</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$\mathbf{u}$</td>
    <td class="tg-c3ow">the joint actions (at time $t$)</td>
    <td class="tg-c3ow">$\tau^{i}$</td>
    <td class="tg-c3ow">the action-observation history of agent $i$</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$o^{i}$</td>
    <td class="tg-c3ow">the observation of agent $i$</td>
    <td class="tg-c3ow">$${\tau}$$</td>
    <td class="tg-c3ow">the joint action-observation histories</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$$o$$</td>
    <td class="tg-c3ow">the joint observation</td>
    <td class="tg-c3ow">$r(s, \mathbf{u})$</td>
    <td class="tg-c3ow">the joint reward supplied by environments</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$Q_{i}(\tau^{i}, u^{i})$</td>
    <td class="tg-c3ow">the utility function of agent $i$</td>
    <td class="tg-c3ow">$\gamma$</td>
    <td class="tg-c3ow">the discount factor</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$Q_{tot}({\tau}, \mathbf{u})$</td>
    <td class="tg-c3ow">the joint value function </td>
    <td class="tg-c3ow">$P(s^{\prime} \mid s, \mathbf{u})$</td>
    <td class="tg-c3ow">the transition function</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$Z(o^{i} \mid s, u^{i})$</td>
    <td class="tg-c3ow">the observation function</td>
    <td class="tg-c3ow">$\epsilon$</td>
    <td class="tg-c3ow">action selection probability of $\epsilon$-greedy</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$N$</td>
    <td class="tg-c3ow">the set of all agents with $n$ agents</td>
    <td class="tg-c3ow">$$\theta$$</td>
    <td class="tg-c3ow">the set of parameters of agents network, with $[\theta^{i}]_{i=1}^{n}$</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$b$</td>
    <td class="tg-c3ow">sampled batch size for training</td>
    <td class="tg-c3ow">$\phi$</td>
    <td class="tg-c3ow">the parameter of mixing network</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$TS$</td>
    <td class="tg-c3ow">the $T$otal rollout $S$amples</td>
    <td class="tg-c3ow">$PP$</td>
    <td class="tg-c3ow">the number of rollout $P$rocesses in $P$arallel</td>
  </tr>
  <tr>
    <td class="tg-c3ow">$SE$</td>
    <td class="tg-c3ow">the number of $S$amples in each <br/> $E$pisode</td>
    <td class="tg-c3ow">$PI$</td>
    <td class="tg-c3ow">the $P$olicy $I$teration number</td>
  </tr>
</tbody>
</table>

## <a name="QMIX_and_Monotonicity_Constraint">QMIX and Monotonicity Constraint</a>

To deal with the relationship between the individual agent and the cooperative group, QMIX [[8](#8)] learns a joint action-value function $Q_{tot}$ and factorizes the joint policy into the individual policy of each agent. In other words, as illustrated in Figure <a href="#frame">4</a>, QMIX integrates all the individual $Q_{i}$ with a mixing network to obtain a centralized value function $Q_{tot}$, which can be more appropriately updated by the global reward.

<div id="frame" class="img-height-310 image-center"> {% include figure.html path="assets/img/2022-12-13-riit/qmix_frame.png"  class="img-fluid rounded z-depth-1" %} </div>
<center>Figure 4: Framework of QMIX. (Image source: QMIX <a ref="#8">[8]</a>). On the left is Mixing Network (A Hypernetwork), and on the right is the Agent network.</center><br/>

Still, it also can be represented in Eq.(\ref{eq2})

$$
Q_{tot}(s, \boldsymbol{u} ; \boldsymbol{\theta}, \phi)
= g_{\phi}\left(s, Q_{1}\left(\tau^{1}, u^{1} ; \theta^{1}\right), \ldots, Q_{n}\left(\tau^{n}, u^{n} ;  \theta^{n}\right)\right);
$$

$$
with \quad \frac{\partial Q_{tot}(s, \boldsymbol{u} ; \boldsymbol{\theta}, \phi)}{\partial Q_{i}\left(\tau^{i}, u^{i}; \theta^{i}\right)} \geq 0, \quad \forall i \in N. \tag{2} \label{eq2}
$$

where $\theta^i$ is the parameter of the agent network $i$, $u^{i}$ denotes the action of agent $i$, and $\phi$ is the trainable parameter of the mixing network. The the mixing network $g_{\phi}(\cdot)$ is responsible to factorize $Q_{tot}$ to each utility $Q_{i}$. The *Monotonicity Constraint* is also implemented in the mixing network $g_{\phi}(\cdot)$, which inputs the global state $s$ and outputs *non-negative* wights through a *hyper-network* as illustrated in the left part of Figure <a href="#frame">4</a>, which will result in $$\frac{\partial Q_{tot}(s, \boldsymbol{u} ; \boldsymbol{\theta}, \phi)}{\partial Q_{i}\left(\tau^{i}, u^{i}; \theta^{i}\right)} \geq 0$$. This delicate design ensures consistency between joint actions and the individual actions of each agent, then guarantees the Individual-Global-Max (IGM) principle. Benefiting from the monotonicity constraint in Eq. (\ref{eq2}), maximizing joint $Q_{tot}$ is precisely the equivalent of maximizing individual $Q_i$, which would also allow the optimal individual action to maintain consistency with optimal joint action. Furthermore, QMIX learns the centralized value function $Q_{tot}$ by sampling a multitude of transitions from the replay buffer and minimizing the mean squared temporal-difference (TD) error loss:

$$
\mathcal{L}(\theta)= \frac{1}{2} \sum_{i=1}^{b}\left[\left(y_{i}^{}-Q_{tot}(s, u ; \theta, \phi)\right)^{2}\right] \tag{3} \label{eq3}
$$

where the TD target value $$y=r+\gamma \underset{u^{\prime}}{\operatorname{max}} Q_{tot}(s^{\prime},u^{\prime};\theta^{-},\phi^{-})$$, and $\theta^{-}, \phi^{-}$ are the target network parameters copied periodically from the current network and kept constant for a number of iterations. $b$ is the sampled training batch size. Due to the strong constraints in Eq.(\ref{eq2}), QMIX is still criticized for the insufficient expressive capacity of the joint value function [[3](#3)]. 


## <a name="Extension_to_QMIX">Extension to QMIX</a>
### <a name="Experimental_Design">Experimental Design</a>
To facilitate the study of proper techniques affecting the training effectiveness and sample efficiency of QMIX, we perform a set of experiments designed to provide insight into some methods that have been proven effective in single-agent RL but may be ambiguous in MARL. In particular,  we investigate the effects of **Adam optimizer with parallel rollout process; the incremental replay buffer size; the number of parallel rollout processes; $\epsilon$-exploration steps; the implementation of $Q(\lambda)$ in centralized value function; the hidden size of the recurrent network of agents**. And we also dive into the **role of monotonicity constraints in QMIX**. For all experiments, we generally implement PyMARL [[10](#10)] framework to implement QMIX. To ensure fairness we run independent 3 to 6 experimental trials for each evaluation, each with a random seed. Unless otherwise mentioned, we use default settings as in PyMARL whenever possible, while incorporating the techniques of interest. To prevent the training process of the algorithm from crashing by chance, we remove the highest and lowest scores when counting the calculated returns and win rates for the test episode. All the results are plotted with the median and shaded the interval, and the final scores were ***not*** smoothed for the sake of image aesthetics, and we did so to verify exactly what direct effect the proposed techniques could have on QMIX.

**StarCraft Multi-Agent Challenge (SMAC)** As a commonly used testing environment, SMAC [[10](#10)] sets an example to offer a great opportunity to tackle the cooperative control problems in the multi-agent domain. We focus on the micromanagement challenge in SMAC, which means each agent is controlled by an independent agency that conditions on a limited observation area, and these groups of units are trained to conquer the enemy consisting of built-in AI. According to the quantity and type of enemy, all testing scenarios could be divided into *Easy, Hard*, and *Super-Hard* levels. Since QMIX can effectively solve the *Easy* tasks, we pay attention to some *Hard* and *Super-Hard* scenarios that QMIX failed to win, especially in *Corridor, 3s5z_vs_3s6z*, and *6h_vs_8z*.

**Predator-Prey (PP)**  is representative of another classical problem called *relative overgeneralization* [[16](#16)] . The cooperating predators are trained to chase a faster running prey, and hope to capture this escaping robot with the fewest steps possible. We leverage Predator-Prey-2 (a variant of Predator-Prey) proposed in FACMAC [[29](#29)], whose policy of prey is replaced with a hard-coded heuristic policy. The heuristic policy asks the prey to move to the farthest sampled position to the closest predator. If one of the cooperative agents collides with the prey, a team reward of +10 is emitted; otherwise, no reward is given. In the original simple tag environment, each agent can observe the relative positions of the other two agents, the relative position and velocity of the prey, and the relative positions of the landmarks. This means each agent’s private observation provides an almost complete representation of the true state of the environment. 

To introduce partial observability to the environment, the view radius is added to the agent, which restricts the agents from receiving information about other entities (including all landmarks, the other two agents, and the prey) that are out of range. Specifically, we set the view radius such that the agents can only observe other agents roughly 60% of the time. These environments require greater cooperation between agents.

### <a name="Optimizer">Optimizer</a>
As an important part of training neural networks, the selection of an optimizer is very important since it could seriously affect the training effect of the reinforcement learning agent. Without a further illustration, QMIX uses RMSProp [[21](#21)] to optimize the neural networks of agents as they prove stable in SMAC. While Adam [[1](#1)] is famous for the fast convergence benefiting from the momentum in training, which seems to be the first choice for AI researchers. We reckon that the momentum property in Adam would have some advantages in learning the sampled data which is generated by agents interacting with the environment as in MARL. And then, on the other hand, QMIX is criticized for performing sub-optimally and sampling inefficiency when equipped with the A2C framework, which is implemented to promote the training efficiency of the RL algorithm. VMIX [[12](#12)] argues this limitation is brought about by the value-based inherent Q function, so they extend QMIX to the actor-critic style algorithm to take advantage of the A2C framework. This controversy attracts our attention to evaluate the performance of QMIX using Adam, as well as the parallel sampling paradigm.

<div id="optimizer" class="img-height-210 image-center"> {% include figure.html path="assets/img/2022-12-13-riit/optimizer.svg"  class="img-fluid rounded z-depth-1" %} </div>
<center>Figure 5: The performance of QMIX optimized by Adam and RMSProp.</center><br/>


**Results** As shown in Figure <a href="#optimizer">5</a>, we run the Adam-supported QMIX with **8 rollout processes**. Different from what was described in VMIX, the performance and efficiency of QMIX could be greatly improved by Adam. We speculate the reason is the momentum property in Adam could fastly fit the newly sampled data from the parallel rollout processes and then enhance the performance, while RMSProp failed.

### <a name="Rollout_Process_Number">Rollout Process Number</a>
Naturally, we come to focus on the benefits of parallel data sampling in QMIX. A2C [[5](#5)] provides an excellent example to reduce training time and improve the training efficiency in single-agent RL. As we implement the algorithms under the paradigm of A2C, there is usually a defined total number of samples and an unspecified number of rollout processes. The total number of samples $TS$ can be calculated as $TS = SE \cdot PP \cdot PI$, where $TS$ is the total sum of sampled data, $SE$ denotes the number of samples in each episode, $PP$ and $PI$ denote the number of rollout processes in parallel and the policy iteration number, respectively. This section aims to perform analysis and spur discussion on the impact of the parallel rollout process on the final performance of QMIX.

<div id="process_number" class="img-height-210 image-center">  {% include figure.html path="assets/img/2022-12-13-riit/process_number.svg"  class="img-fluid rounded z-depth-1" %} </div>
<center>Figure 6: The performance of different rollout process numbers of QMIX. When given the total number of samples, the performance of fewer processes achieves better performance.</center><br/>

**Results** Still, we use Adam-supported QMIX to evaluate the effect of the number of the rollout process. Since we could choose the *Parallel* model to sample the interacting data of the agent with the environment in PyMARL, we can theoretically get more **on-policy** data which is close to the updating policy in training. Figure <a href="#process_number">6</a> shows that when $TS$ and $PP$ is given, the performance enhancement of QMIX is not consistent with the increase in rollout process number. The intuitive explanation is when we set the fewer rollout processes, the greater the quantity of policy would iterate [[14](#14)]. Besides, too fast updated data in parallel may cause the factitious unstable training in policy updating, i.e., it is difficult for agents to learn effective information from rapidly sampled data from the replay buffer. The more times policies are iterated, the more information the agents would learn which lead to an increase in performance. However, it also causes longer training time and loss of stability. We suggest trying the fewer rollout process in the beginning and then balancing between training time and performance.

### <a name="Replay_Buffer_Size">Replay Buffer Size</a>
Replay buffer plays an important role in improving sample efficiency in off-policy single-agent RL. Its capacity would greatly affect the performance and stability of algorithms. Researchers usually set a very large capacity of replay buffer in Deep Q-network (DQN) [[4](#4)]  to stabilize the training. Some research on the effect of replay buffer in single-agent RL has already been carried out in [[22](#22)] , which poses the distribution of sampled training data should be close as possible to the agents' policies to be updated. Actually, there are two factors affected when we change the capacity of the replay buffer: (1) the replay capacity (total number of transitions/episodes stored in the buffer); and (2) the replay ratio (the number of gradient updates per environment transition/episode) of old policies. When we increase the capacity of the replay buffer, the aged experiences of old policies would grow as the replay ratio is fixed. Then the distribution of outdated experiences would also be much different from the updating policy, which would bring additional difficulty to the training agents. From the results in [[22](#22)], there seems to be an optimal range of choices between replay buffer size and replay ratio of experiences in RL, where we would like to know whether it is consistent with the results in MARL.

<div id="replay_buffer" class="img-height-210 image-center"> {% include figure.html path="assets/img/2022-12-13-riit/buffer_size.svg"  class="img-fluid rounded z-depth-1" %} </div>
<center>Figure 7:  Setting the replay buffer size to 5000 episodes allows for QMIX’s learning to be stable.</center><br/>

**Results** The results seem not to be consistent with that in single-agent RL. Figure <a href="#replay_buffer">7</a> shows the large replay buffer size of QMIX would cause instability during training. When we increase the buffer size from the default setting in PyMARL, the performance would almost continuously declines. We speculate the reason is the fast-changing distribution of experiences in a larger buffer would make it more difficult to fit sampled data due to the enormous joint action space. Since the samples become obsolete more quickly, these aged policies would also be more different from the updating policy, which brings additional difficulty. On the other hand, we find the same performance decline when we squeeze the buffer. We reckon that a small buffer would accelerate the updating speed of sampling data in a disguised way, which makes it tough to fit the data and learn a good policy. We believe that researchers should be cautious to increase the buffer size in other multi-agent applications.

### <a name="Eligibility_Traces">Eligibility Traces</a>
The well-known trade-off between bias and variance of bootstrapping paradigm is a classic research topic in RL. Since we implement the Centralized Value Function (CVF) to alleviate the *Non-Stationarity*  multi-agent settings, the estimated accuracy of CVF is critical to MARL and then guides the policies of agents to update. Eligibility traces such as TD($\lambda$)[[14](#14)], Peng's Q($\lambda$)[[2](#2)], and TB($\lambda$)[[7](#7)] achieve a balance between return-based algorithms (where return refers to the sum of discounted rewards $\sum_{k} \gamma^{k} r_{t+k}$) and bootstrap algorithms (where return refers $r_t + V(s_{t+1})$), then speed up the convergence of agents' policies. As a pioneer, SMIX [[20](#20)]  equipped QMIX with the SARSA($\lambda$) to estimate the accurate CVF and get decent performance. As another example of eligibility trace in Q-learning, we study the estimation of CVF using Peng's Q$(\lambda)$ for QMIX.

<div id="qlambda1" class="img-height-210 image-center">  {% include figure.html path="assets/img/2022-12-13-riit/td_lambda.svg"  class="img-fluid rounded z-depth-1" %} </div>
<center>Figure 8:  Q(λ)  significantly improves the performance of QMIX, but large values of λ lead to instability in the algorithm.</center><br/>

**Results** As the same in single-agent RL, the Q-networks without sufficient training usually have a large bias in bootstrapping returns. Figure <a href="#qlambda1">8</a> shows that, with the help of Q$(\lambda)$, the performance of QMIX has generally improved across all scenarios. It means the more accurate estimate of CVF would still provide a better direction of policy updating for each agent. However, the value of $\lambda$ in Peng's Q$(\lambda)$ is not so radical as in single-agent RL, which would lead to failed convergence due to the large variance. We recommend a small $\lambda$, such as $0.5$, when using $Q(\lambda)$ in MARL.

### <a name="Hidden_Size">Hidden Size</a>
Searching for an optimal scale and architecture of neural networks is a very tough problem in the field of machine learning. Researchers typically use empirically small networks to train the agents in deep reinforcement learning. Since the role of neural networks is to extract the features of input states and actions, the size of the neural network would also have a great impact on the performance of MARL algorithms. The study in [[23](#23)]  has revealed that networks with a complex structure like ResNet[[25](#25)] and DenseNet[[26](#26)] can extract more useful information for training, while Ba [[24](#24)] poses that the width of neural networks is probably more important than its depth. The subsequent study on QMIX [[19](#19)] makes preliminary research on the depth of neural networks, which showed a limited improvement in performance. Though, there is little research on the width of neural networks in MARL. Instead of searching for an optimal network architecture here, we just want to make a pilot study on the effect of the hidden size of network width in QMIX.

<div id="hiddensize" class="img-height-210 image-center"> {% include figure.html path="assets/img/2022-12-13-riit/hidden_size.svg"  class="img-fluid rounded z-depth-1" %} </div>
<center>Figure 9:  Impact of the hidden size of network in QMIX.</center><br/>

**Results** The study in [[24](#24)]  illustrates the ability of infinity-width networks to fit any complex function, which would theoretically provide the performance gain from increasing network width. As shown in Figure <a href="#hiddensize">9</a>, the final performance or the efficiency of policy training would have varying degrees of improvement when we increase the hidden size of the network from 64 to 256 in QMIX, where **QMIX-ALL-Hidden refers to the size of the network including Recurrent Neural Network (RNN) and mixing part, while QMIX-RNN-Hidden just refers to RNN**. Also, the results reveal the spectacular effect of increasing the network width of RNN, which would allow for about a 20% increase in the Super-Hard scenarios *3s5z_vs_3s6z*. While the performance improvement is limited in enlarging the mixing network. We speculate that more units in the network are needed to represent the complex temporal context information in RNN, which is not included in the mixing network. We advise researchers to appropriately increase the network width of RNN to achieve better performance.

### <a name="Exploration_Steps">Exploration Steps</a>
Exploration and exploitation are other classic trade-offs in reinforcement learning. Agents need some directed mechanisms to explore the states that may be of higher value or inexperienced. The most versatile method of exploration in RL is $\epsilon$-greedy action, which makes the agent select random actions with probability $\epsilon$, or select the greedy action with $1 - \epsilon$. The value of $\epsilon$ would drop-down with training and then stays at a small constant. The annealing period of $\epsilon$-greedy determines how fast the drop down will be. This exploration mechanism is
usually implemented for each agent to select their action, which has been criticized by MAVEN [[3](#3)] for lacking joint exploratory policy over an entire episode. However, we can still get more exploration when $\epsilon$ drops slower, then we evaluate the performance of the annealing period of $\epsilon$-greedy in some Super-Hard scenarios in SMAC.

<div id="exploration" class="img-height-210 image-center">  {% include figure.html path="assets/img/2022-12-13-riit/exploration.svg"  class="img-fluid rounded z-depth-1" %} </div>
<center>Figure 10: Experinments for the impact of ε anneal period.</center><br/>

**Results** Apparently, appropriately increasing the annealing period of $\epsilon$-greedy from 100K steps to 500K would get explicit performance gain in those hard exploration scenarios, where QMIX failed with the default setting. However, as shown in Figure <a href="#exploration">10</a>, too large steps like 1000K would also bring additional exploration noise even making the training collapse. The results above confirm the $\epsilon$-greedy mechanism is still the proper and simplest choice in MARL but should be elaboratively tuned for different tasks.

### <a name="Integrating_the_Techniques">Integrating the Techniques</a>
These techniques mentioned above indeed impact QMIX in hard cooperative scenarios of SMAC, which really catches our attention to exhaust the extreme performance of QMIX. We combine these techniques and finetune all the hyperparameters in QMIX for each scenario of SMAC. As shown in Table [2](#table2), the Finetuned-QMIX would almost conquer all the scenarios in SMAC and exceed the effect of the original QMIX by a large margin in some Hard and Super-Hard scenarios.

<a name="table2"> </a>
<center>
    Table 2: Best median test win rate of Finetuned-QMIX and QMIX (batch size=128)
</center>
<center>
    in all testing scenarios.
</center>
<table style="text-align: center; width: 600px; margin: 0 auto; margin-bottom:20px; margin-top:20px">
  <thead>
    <tr>
      <td><b>Senarios</b></td>
      <td><b>Difficulty</b></td>
      <td><b>QMIX</b></td>
      <td><b>Finetuned-QMIX</b></td>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>10m_vs_11m</td>
      <td>Easy</td>
      <td>98%</td>
      <td><b>100%</b></td>
    </tr>
    <tr>
      <td>8m_vs_9m</td>
      <td>Hard</td>
      <td>84%</td>
      <td><b>100%</b></td>
    </tr>
    <tr>
      <td>5m_vs_6m</td>
      <td>Hard</td>
      <td>84%</td>
      <td><b>90%</b></td>
    </tr>
    <tr>
      <td>3s_vs_5z</td>
      <td>Hard</td>
      <td>96%</td>
      <td><b>100%</b></td>
    </tr>
    <tr>
      <td>bane_vs_bane</td>
      <td>Hard</td>
      <td><b>100%</b></td>
      <td><b>100%</b></td>
    </tr>
    <tr>
      <td>2c_vs_64zg</td>
      <td>Hard</td>
      <td><b>100%</b></td>
      <td><b>100%</b></td>
    </tr>
    <tr>
      <td>corridor</td>
      <td>Super hard</td>
      <td>0%</td>
      <td><b>100%</b></td>
    </tr>
    <tr>
      <td>MMM2</td>
      <td>Super hard</td>
      <td>98%</td>
      <td><b>100%</b></td>
    </tr>
    <tr>
      <td>3s5z_vs_3s6z</td>
      <td>Super hard</td>
      <td>3%</td>
      <td><b>93% (Hidden Size = 256)</b></td>
    </tr>
    <tr>
      <td>27m_vs_3s6z</td>
      <td>Super hard</td>
      <td>56%</td>
      <td><b>100%</b></td>
    </tr>
    <tr>
      <td>6h_vs_8z</td>
      <td>Super hard</td>
      <td>0%</td>
      <td><b>93% (λ = 0.3)</b></td>
    </tr>
  </tbody>
</table>

## <a name="Role_of_Monotonicity_Constraint">Role of Monotonicity Constraint</a>
### <a name="Amazing_Performance_in_Policy-Based_Methods">Amazing Performance in Policy-Based Methods</a>
<div id="qmix_sy" class="img-height-180 image-center img-margin-left-30"> {% include figure.html path="assets/img/2022-12-13-riit/riit.svg" class="img-fluid rounded z-depth-1" %} </div>

<center>Figure 11: Architecture for AC-MIX: <b>|·|</b> denotes <b>absolute value operation</b>, implementing the monotonicity constraint of QMIX. <b>W</b> denotes the non-negative mixing weights. Agent $i$ denotes the agent's network, which can be trained end-to-end by maximizing the $Q_{tot}$.</center><br/>

The novelty of QMIX is the IGM consistency between $\text{argmax} Q_{tot}$ and $\text{argmax} \sum_{i}^{n} Q_{i}$, which is implemented in the mixing network. **We still expect to study the role of *monotonicity constraint* in MARL**. Therefore, we propose an actor-critic style algorithm called Actor-Critic-Mixer (AC-MIX), which has a similar architecture to QMIX. As illustrated in Figure <a href="#qmix_sy">11</a>, we use the monotonic mixing network as a centralized critic, which integrates $Q_{i}$ of each agent, to optimize the decentralized policy networks $π^i_{θ_i}$ in an end-to-end pattern. We still add the Adaptive Entropy $\mathcal{H}(\cdot)$ [[18]](#18) of each agent in the optimization object of Eq.(\ref{eq4}) to get more exploration, and the detail of the algorithm will be described in Appendix [A](#A).

$$
\max _{\theta} \mathbb{E}_{t, s_{t}, \tau_{t}^{1}, \ldots, \tau_{t}^{n}}\left[Q_{\theta_{c}}^{\pi}\left(s_{t}, \pi_{\theta_{1}}^{1}\left(\cdot \mid \tau_{t}^{1}\right), \ldots, \pi_{\theta_{n}}^{n}\left(\cdot \mid \tau_{t}^{n}\right)\right)
+ \mathbb{E}_{i}\left[\mathcal{H}\left(\pi_{\theta_{i}}^{i}\left(\cdot \mid \tau_{t}^{i}\right)\right)\right]\right] \tag{4} \label{eq4}
$$

<div id="riit_abla" class="img-height-210 image-center"> {% include figure.html path="assets/img/2022-12-13-riit/monotonicity_riit.svg" class="img-fluid rounded z-depth-1" %} </div>

<center>Figure 12: Comparing AC-MIX w./ and w./o. monotonicity constraint</center>
<center>(remove absolute value operation) on SMAC and Predator-Prey-2</center><br/>

As the monotonicity constraint on the critic of AC-MIX is theoretically no longer required as the critic is not used for greedy action selection. We can evaluate the effects of the monotonicity constraint by removing the absolute value operation in the mixing network. The results in Figure <a href="#riit_abla">12</a> demonstrate the *monotonicity constraint* significantly improves the performance of AC-MIX. Then to explore the generality of *monotonicity constraints* in the parallel sampling framework of MARL, we extend the above experiments to VMIX [[12](#12)] . VMIX adds the monotonicity constraint to the value network of A2C, and learns the policy of each agent by advantage-based policy gradient [[14](#14)]  as illustrated in Figure <a href="#vmix_net">13</a>. Still, the result from Figure <a href="#vmix_abla">14</a> shows that the monotonicity constraint improves the sample efficiency in value networks.

<div id="vmix_net" class="img-height-180 image-center img-margin-left-60"> {% include figure.html path="assets/img/2022-12-13-riit/vmix.svg" class="img-fluid rounded z-depth-1" %} </div>
<center>Figure 13. Architecture for VMIX: |·| denotes absolute value operation</center><br/>

<div id="vmix_abla" class="img-height-210 image-center"> {% include figure.html path="assets/img/2022-12-13-riit/monotonicity_vmix.svg" class="img-fluid rounded z-depth-1" %} </div>

<center>Figure 14: Comparing VMIX w./ and w./o. monotonicity constraint</center>
<center>(remove absolute value operation) on SMAC</center><br/>


### <a name="What_is_Under_the_Hood">What is Under the Hood?</a>
Observed from the results of previous experiments, **the *monotonicity constraints* in the mixing network indeed improve performance and sample efficiency of training**, but on the flip side of the coin, QMIX is still criticized for the insufficient expressive capacity of the centralized critic [[3](#3)], which may cause poor performance. The abnormal question naturally occurred to us: *Why the performance of AC-MIX would be better than AC-MIX-nonmonotonic which aims to relax the monotonicity constraint of mixing network*? 

To answer this question we first need to reexamine the **IGM** principle. Since in QMIX, $Q_{tot}$ is decomposed by the mixing network into the sum of the weighted $[Q_i] _{i=1}^{n}$, as shown in Figure <a href="#frame">4</a>, where the weights and bias of mixing network are generated by the *Hypernetwork*, then the monotonicity in QMIX can be defined simplistically as a constraint on the relationship between $$Q_{tot}$$ and each $$Q_{i}$$ :

$$
Q_{tot} = \sum_{i=1}^{N}w_{i}(s_{t}) \cdot Q_{i} + b(s_{t}), \\
w_{i} = \frac{\partial Q_{tot}}{\partial Q_{i}} \geq 0, \forall i \in N.
\tag{5} \label{5}
$$

From the sufficient condition above, the weight $w_{i}$ in *Mixing Network* would be forced to be greater or equal to zero $w_{i} \geq 0$. To put it another way, it makes the parameter space smaller for searching $w_{i}$ weights to decompose $Q_{tot}$. As illustrated in the schematic diagram <a href="#diagram">15</a>, assume there is only 1 agent in the environment, the parameter searching space will be directly halved and the optimal $w_{1}$ will be found in the region where $w \geq 0$, i.e., the green region. Similarly, when the number of agents is 2 or 3, its parameter searching space for $w_i$ will be restricted to the first quadrant, and the same can be recursively extended to the case of high-dimensional parameter space. **In other words, the search area of exhausting the whole joint state-action space would also be decreased exponentially by $(\frac{1}{2})^{N}$ ($N$ denotes the number of $w_{i}$, as well as the number of agents).** Then the optimal solution in the original domain cannot be expressed correctly in the restricted region. Since the essence of learning in MARL is to search for the optimal joint-policy parameterized by weights and bias of agents and mixing network, QMIX could find a satisfying policy more quickly in these **reduced** parameter spaces.

<div id='diagram' style="display:flex; margin:20px 0; gap:5px">
<div id="1_agent" class="img-height-100"> {% include figure.html path="assets/img/2022-12-13-riit/1_agent.svg"  class="img-fluid rounded z-depth-1" %} </div>
<div id="2_agent" class="img-height-100"> {% include figure.html path="assets/img/2022-12-13-riit/2_agent.svg"  class="img-fluid rounded z-depth-1" %} </div>
<div id="3_agent" class="img-height-100"> {% include figure.html path="assets/img/2022-12-13-riit/3_agent.svg"  class="img-fluid rounded z-depth-1" %} </div></div>

<div style="margin-bottom: 20px"><center>Figure 15: the weight parameter space diagram of different number of agents in QMIX [from-left-to-right]. (a) weight parameter space of only 1 agent; (b) weight parameter space of 2 agents; (c) weight parameter space of 3 agents.</center></div>

As a side effect, the global optimum may not be in the parameter space that QMIX needs to search at all due to the monotonicity of the mixing network. One effective way is to estimate the $Q_{tot}$ as accurately as possible in the hope that it could find the global optimum, this probably explains why $Q(\lambda)$ in the previous section could result in such a performance improvement in SMAC. On the other hand, we could delicately design the reward function to be approximately monotonic when we use QMIX to solve cooperative multi-agent tasks. Then adapting the algorithm to the test environment is not a good idea, after all, we still need to figure out how to use QMIX more effectively or develop other more efficient algorithms.

## <a name="Conclusion">Conclusion</a>
In this post, we revisited the performance of the QMIX as a baseline algorithm in the SMAC environment. We found that the application of hyperparameters and other RL techniques have a great impact on the effectiveness of QMIX. We evaluated the effect of optimizer, number of rollout processes, replay buffer size, eligibility traces, hidden size and the degree of annealed exploration on QMIX, and tried to explain their role in MARL. Furthermore, we dived into the monotonicity in QMIX, and found the absolute operation in mixing network would decrease the parameter searching space of the joint state-action area exponentially by $(\frac{1}{2})^{N}$, which would make QMIX find the satisfying policy more quickly but with the drawback of inaccurate evaluated joint value function of optimal policy. We hope that our findings will stimulate some inspiration for the value decomposition method in MARL and provoke the community to think about the performance of QMIX as a new benchmark.

## <a name="Authorship,_Credit_Attribution_and_Acknowledgement ">Authorship, Credit Attribution and Acknowledgement</a>

Jian Hu was responsible for the key ideas, open source code and all experiments, as well as the first draft of the paper.

Siying Wang was responsible for the writing of the blog.

Siyang Jiang participated in writing the first draft of the paper.

Weixun Wang provided feedback on revisions.

Siyang Jiang was supported by the fund which aims to improve scientific research capability of key construction disciplines in Guangdong province “Light-weight federal learning paradigm and its application” (No:2022ZDJS058) and Foundation for Distinguished Young Talents in Higher Education of Guangdong, China. (NO. 2022KQNCX084)

## <a name="Appendix">Appendix</a>

### A Pseudo-code of AC-MIX<a id="A"> </a>

In this subsection, we show the pseudo-code for the training procedure of AC-MIX. (1) Training the critic network with offline samples and 1-step TD error loss improves the sample efficiency for critic networks; (2) We find that policy networks are sensitive to old sample reuse. Training policy networks end-to-end and critic with TD($\lambda$) and online samples improve the learning stability of AC-MIX.

<div id="algorithm_riit" class="img-height-600 image-center"> {% include figure.html path="assets/img/2022-12-13-riit/algorithm_riit.svg" class="img-fluid rounded z-depth-1" %} </div>

### B HYPERPARAMETERS

In this subsection, we present our hyperparameters tuning process. We get the optimal hyperparameters for each algorithm by grid search, shown in Table [3](#t3).

<center>
   Table 3: Hyperparameters Search on SMAC. The bold type indicates the
</center> 
<center>
    selected hyperparameters.
</center> 
<table style="text-align: center; width: 700px; margin: 0 auto; margin-bottom:20px; margin-top:20px;"><a name="t3"> </a>
  <thead>
    <tr>
      <td><b>Tricks</b></td>
      <td><b>QMIX</b></td>
      <td><b>AC-MIX</b></td>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Optimizer</td>
      <td><b>Adam</b>,RMSProp</td>
      <td><b>Adam</b>,RMSProp</td>
    </tr>
    <tr>
      <td>Learning Rates</td>
      <td>0.0005, <b>0.001</b></td>
      <td>0.0005, <b>0.001</b></td>
    </tr>
    <tr>
      <td>Batch Size (episodes)</td>
      <td>32, 64, <b>128</b></td>
      <td>32, <b>64</b> </td>
    </tr>
    <tr>
      <td>Replay Buffer Size</td>
      <td><b>5000</b>, 10000, 20000</td>
      <td>2000, <b>5000</b>, 10000</td>
    </tr>
    <tr>
      <td>Q(λ)/TD(λ)</td>
      <td>0, 0.3, <b>0.6</b>, 0.9</td>
      <td>0.3, <b>0.6</b>, 0.8</td>
    </tr>
    <tr>
      <td>Entropy/Adaptive Entropy</td>
      <td>-</td>
      <td>0.005, 0.01, <b>0.03</b>, 0.06</td>
    </tr>
    <tr>
      <td>ε Anneal Steps</td>
      <td>50K, <b>100K, 500K</b>, 1000K</td>
      <td>-</td>
    </tr>
  </tbody>
</table><br/>

**Rollout Processes Number**. For SMAC, 8 rollout processes for parallel sampling are used to obtain as many samples as possible from the environments at a high rate.
And 4 rollout processes are used for Predator-Prey-2.

**Other Settings**. We set all discount factors $\gamma$ = 0.99. We update the target network every 200 episodes.

## <a name="Reference">Reference</a>

<a name="1" href="https://arxiv.org/abs/1412.6980">[1] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR 2015, San Diego, CA, USA, May 7-9, 2015, 2015. </a>

<a name="2" href="https://arxiv.org/abs/2103.00107">[2] Tadashi Kozuno, Yunhao Tang, Mark Rowland, Remi Munos, Steven Kapturowski, Will Dabney, Michal Valko, and David Abel. Revisiting peng's q (λ) for modern reinforcement learning. arXiv preprint arXiv:2103.00107, 2021.  </a>

<a name="3" href="https://arxiv.org/abs/1910.07483">[3] Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. MAVEN: multi-agent variational exploration. In NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,pp. 7611–7622, 2019. </a>

<a name="4" href="https://arxiv.org/abs/1312.5602">[4] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, DaanWierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.</a>

<a name="5" href="http://proceedings.mlr.press/v48/mniha16.html">[5] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In ICML 2016, New York City, NY, USA, June 19-24, 2016, pp. 1928–1937, 2016.</a>

<a name="6" href="https://www.comp.nus.edu.sg/~leews/publications/rss09.pdf">[6] Sylvie CW Ong, Shao Wei Png, David Hsu, and Wee Sun Lee. Pomdps for robotic tasks with mixed observability. 5:4, 2009.  </a>

<a name="7" href="https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1079&context=cs_faculty_pubs">[7] Doina Precup, Richard S. Sutton, and Satinder P. Singh. Eligibility traces for off-policy policy evaluation. In ICML 2000, Stanford University, Stanford, CA, USA, June 29 - July 2, 2000, pp.759–766. Morgan Kaufmann, 2000. </a>

<a name="8" href="http://proceedings.mlr.press/v80/rashid18a.html">[8] Tabish Rashid, Mikayel Samvelyan, Christian Schr ̈oder de Witt, Gregory Farquhar, Jakob N.Foerster, and Shimon Whiteson. QMIX: monotonic value function factorization for deep multi-agent reinforcement learning. In ICML 2018, Stockholmsmassan, Stockholm, Sweden, July10-15, 2018, pp. 4292–4301, 2018.  </a>

<a name="9" href="https://ui.adsabs.harvard.edu/abs/2020arXiv200610800R/abstract">[9] Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted QMIX: Expand-ing Monotonic Value Function Factorisation. arXiv preprint arXiv:2006.10800, 2020. </a>

<a name="10" href="https://arxiv.org/abs/1902.04043">[10] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, NantasNardelli, Tim G. J. Rudner, Chia-Man Hung, Philip H. S. Torr, Jakob Foerster, and ShimonWhiteson. The StarCraft Multi-Agent Challenge.arXiv preprint arXiv:1902.04043, 2019. </a>

<a name="11" href="http://proceedings.mlr.press/v97/son19a.html">[11] Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Hostallero, and Yung Yi. QTRAN: learning to factorize with transformation for cooperative multi-agent reinforcement learning. In ICML 2019, 9-15 June 2019, Long Beach, California, USA, pp. 5887–5896, 2019. </a>

<a name="12" href="https://www.aaai.org/AAAI21Papers/AAAI-2412.SuJ.pdf">[12] Jianyu Su, Stephen Adams, and Peter A. Beling. Value-Decomposition Multi-Agent Actor-Critics. arXiv:2007.12306, 2020.  </a>

<a name="13" href="https://arxiv.org/abs/1706.05296">[13] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi,Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Grae-pel. Value-Decomposition Networks For Cooperative Multi-Agent Learning. arXiv preprint arXiv:1706.05296, 2017.  </a>

<a name="14" href="https://go.gale.com/ps/i.do?id=GALE%7CA61573878&sid=googleScholar&v=2.1&it=r&linkaccess=abs&issn=07384602&p=AONE&sw=w">[14] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018. </a>

<a name="15" href="https://arxiv.org/abs/2008.01062">[15] Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. QPLEX: Duplex Dueling Multi-Agent Q-Learning. arXiv:2008.01062, 2020.  </a>

<a name="16" href="https://www.aaai.org/ocs/index.php/SSS/SSS18/paper/viewPaper/17508">[16] Ermo Wei, Drew Wicke, David Freelan, and Sean Luke. Multiagent Soft Q-Learning. arXivpreprint arXiv:1804.09817, 2018.  </a>

<a name="17" href="https://arxiv.org/abs/2002.03939">[17] Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and HongyaoTang. Qatten: A General Framework for Cooperative Multiagent Reinforcement Learning. arXiv preprint arXiv:2002.03939, 2020. </a>

<a name="18" href="https://arxiv.org/abs/2010.09776">[18] Ming Zhou, Jun Luo, and Julian Villella et al. Smarts: Scalable multi-agent reinforcement learning training school for autonomous driving, 2020. </a>

<a name="19" href="https://www.jmlr.org/papers/volume21/20-081/20-081.pdf">[19] Rashid T, Samvelyan M, Schroeder de Witt C, et al. Monotonic value function factorisation for deep multi-agent reinforcement learning. Journal of Machine Learning Research, 2020, 21.</a>

<a name="20" href="https://ojs.aaai.org/index.php/AAAI/article/view/6223">[20] Wen C, Yao X, Wang Y, et al. Smix (λ): Enhancing centralized value functions for cooperative multi-agent reinforcement learning. Proceedings of the AAAI Conference on Artificial Intelligence. 2020, 34(05): 7301-7308.  </a>

<a name="21" href="http://www.cs.toronto.edu/~hinton/coursera/lecture6/lec6.pdf">[21] Hinton G, Srivastava N, Swersky K. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent. Cited on, 2012, 14(8): 2.</a>

<a name="22" href="http://proceedings.mlr.press/v119/fedus20a.html">[22] Fedus W, Ramachandran P, Agarwal R, et al. Revisiting fundamentals of experience replay. International Conference on Machine Learning. PMLR, 2020: 3061-3071.  </a>

<a name="23" href="http://proceedings.mlr.press/v119/ota20a.html">[23] Ota K, Oiki T, Jha D, et al. Can Increasing Input Dimensionality Improve Deep Reinforcement Learning?. International Conference on Machine Learning. PMLR, 2020: 7424-7433.</a>

<a name="24" href="https://arxiv.org/abs/1312.6184">[24] Ba L J, Caruana R. Do deep nets really need to be deep?. arXiv preprint arXiv:1312.6184, 2013.</a>

<a name="25" href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">[25] He K, Zhang X, Ren S, et al. Deep residual learning for image recognition. Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.

<a name="26" href="https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.html">[26] Huang G, Liu Z, Van Der Maaten L, et al. Densely connected convolutional networks. Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 4700-4708.

<a name="27" href="http://proceedings.mlr.press/v48/wangf16.html">[27] Wang Z, Schaul T, Hessel M, et al. Dueling network architectures for deep reinforcement learning. International conference on machine learning. PMLR, 2016: 1995-2003.

<a name="28" href="https://www.ccs.neu.edu/home/camato/publications/OliehoekAmato16book.pdf">[28] Oliehoek, Frans A., and Christopher Amato. A concise introduction to decentralized POMDPs. Springer, 2016.

<a name="29" href="https://proceedings.neurips.cc/paper_files/paper/2021/file/65b9eea6e1cc6bb9f0cd2a47751a186f-Paper.pdf">[29] Peng, Bei, et al. Facmac: Factored multi-agent centralised policy gradients. Advances in Neural Information Processing Systems 34 (2021): 12208-12221.

