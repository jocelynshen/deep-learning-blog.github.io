<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://jocelynshen.com/deep-learning-blog.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jocelynshen.com/deep-learning-blog.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-10-12T18:58:58+00:00</updated><id>https://jocelynshen.com/deep-learning-blog.github.io/feed.xml</id><title type="html">ICLR Blogposts 2023 (staging)</title><subtitle>Staging website for the 2023 ICLR Blogposts track </subtitle><entry><title type="html">Welcome to Jekyll!</title><link href="https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/welcome-to-jekyll/" rel="alternate" type="text/html" title="Welcome to Jekyll!"/><published>2023-10-12T18:14:35+00:00</published><updated>2023-10-12T18:14:35+00:00</updated><id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/welcome-to-jekyll</id><content type="html" xml:base="https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/welcome-to-jekyll/"><![CDATA[<p>You’ll find this post in your <code class="language-plaintext highlighter-rouge">_posts</code> directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run <code class="language-plaintext highlighter-rouge">jekyll serve</code>, which launches a web server and auto-regenerates your site when a file is updated.</p> <p>Jekyll requires blog post files to be named according to the following format:</p> <p><code class="language-plaintext highlighter-rouge">YEAR-MONTH-DAY-title.MARKUP</code></p> <p>Where <code class="language-plaintext highlighter-rouge">YEAR</code> is a four-digit number, <code class="language-plaintext highlighter-rouge">MONTH</code> and <code class="language-plaintext highlighter-rouge">DAY</code> are both two-digit numbers, and <code class="language-plaintext highlighter-rouge">MARKUP</code> is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.</p> <p>Jekyll also offers powerful support for code snippets:</p> <figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="k">def</span> <span class="nf">print_hi</span><span class="p">(</span><span class="nb">name</span><span class="p">)</span>
  <span class="nb">puts</span> <span class="s2">"Hi, </span><span class="si">#{</span><span class="nb">name</span><span class="si">}</span><span class="s2">"</span>
<span class="k">end</span>
<span class="n">print_hi</span><span class="p">(</span><span class="s1">'Tom'</span><span class="p">)</span>
<span class="c1">#=&gt; prints 'Hi, Tom' to STDOUT.</span></code></pre></figure> <p>Check out the <a href="https://jekyllrb.com/docs/home">Jekyll docs</a> for more info on how to get the most out of Jekyll. File all bugs/feature requests at <a href="https://github.com/jekyll/jekyll">Jekyll’s GitHub repo</a>. If you have questions, you can ask them on <a href="https://talk.jekyllrb.com/">Jekyll Talk</a>.</p>]]></content><author><name></name></author><category term="jekyll"/><category term="update"/><summary type="html"><![CDATA[You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.]]></summary></entry><entry><title type="html">A Match Made in Drug Discovery - Marrying Geometric and Diffusion Models</title><link href="https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/diffusion-is-all-you-need/" rel="alternate" type="text/html" title="A Match Made in Drug Discovery - Marrying Geometric and Diffusion Models"/><published>2023-02-17T00:00:00+00:00</published><updated>2023-02-17T00:00:00+00:00</updated><id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/diffusion-is-all-you-need</id><content type="html" xml:base="https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/diffusion-is-all-you-need/"><![CDATA[<h1 id="a-match-made-in-drug-discovery-marrying-geometric-and-diffusion-models">A Match Made in Drug Discovery: Marrying Geometric and Diffusion Models</h1> <h2 id="introduction">Introduction</h2> <p>With the initial breakthrough of the Denoising Diffusion <d-cite key="ho2020denoising"></d-cite>, diffusion models have evolved into a powerful tool for a wide range of applications in machine learning, including image generation and text generation. The rise and the recent release of Stable Diffusion <d-cite key="rombach2022high"></d-cite> in August 2022 has shown superior performance in a wide range of practical applications - from artistic and creative projects to scientific and industrial ones and has recently paved its way into the natural sciences, which includes the field of drug discovery <d-cite key="zhang2023sdegen,jing2022torsional"></d-cite>.</p> <p>Data-driven applications have increasingly been shown to accelerate solving diverse problems in the drug discovery pipeline <d-cite key="CHEN20181241"></d-cite> – from the use of predictive analytical methods for target identification and lead optimization, to the analysis of large-scale biological data for drug repurposing and personalized medicine. To limit the scope of the blog post, we will only focus on the use of generative methods in <strong>molecular conformation generation</strong>.</p> <h2 id="motivation">Motivation</h2> <p>Generating molecular conformations is a task fundamental to cheminformatics and drug discovery. The conformation of a molecule refers to the three-dimensional (3D) coordinates of all the atoms in a molecule in a 3D Euclidean space, which can be interconverted by rotations about formally single bonds <d-cite key="nature_def"></d-cite>. It allows for the prediction of the physical and chemical properties and interactions of molecules based on their possible 3D conformations, as well as their biological activity. In drug discovery, it is integral to obtain all the possible conformations of molecules for various tasks, such as three-dimensional quantitative structure-activity relationships (3D-QSAR), pharmacophore searching, molecular docking and thermodynamic calculations. Deep generative modelling, which aims to learn complex data distributions, is a recent promising approach to tackle the conformation generation problem <d-cite key="zhang2023sdegen"></d-cite>. When studying molecules it is important to understand them as three-dimensional structures formed by atoms bonded to each other. To encode the chemical properties, molecules could be represented as graphs where atoms (nodes) are connected by bonds (edges). Representing molecules as 3D molecular graphs captures the spatial arrangement of a molecule, which in turn determines its chemical property. A molecule could take up any conformation based on all possible permutations and combinations of spatial arrangements of atoms. However, some conformations may not occur physically, due to e.g steric hindrance, which arises when the spatial arrangement of atoms leads to unfavourable interactions/repulsive forces, leading to a higher energy state and less stable conformation.</p> <p>Therefore, we are only interested in conformations that fall in stable low-energy minima, as these low-energy conformations are the ones that the molecule will most likely adopt under natural conditions and play a crucial role in determining the molecule’s behaviour and properties. By identifying and characterizing the low-energy conformations, researchers gain insights into their stability, reactivity and interactions with other molecules.</p> <h2 id="formulating-the-conformation-generation-problem">Formulating the conformation generation problem</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/formulation-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/formulation-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/formulation-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/formulation.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>A formulation of the conformation generation problem, adapted from <d-cite key="xu2022geodiff"></d-cite></p> <table> <tbody> <tr> <td>We can formulate the problem of conformation generation as a conditional generative problem where we aim to generate stable conformations \(C\) from a molecule’s graph \(G\). For each \(G\) given its conformations \(C\) as i.i.d samples from an underlying Boltzmann distribution <d-cite key="noe2019boltzmann"></d-cite>, our goal is to learn a generative model $$p_\theta(C</td> <td>G)$$ to draw possible conformations from.</td> </tr> </tbody> </table> <h3 id="roto-translation-equivariance">Roto-translation equivariance</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/roto-trans-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/roto-trans-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/roto-trans-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/roto-trans.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>A visualisation of roto-translation equivariance, adapted from <d-cite key="xu2022geodiff"></d-cite></p> <p>To generate stable molecular conformations, we need an algorithm that preserves roto-translation equivariance of the conformations that previous work has not focused on. To explain this property, let us delve into what equivariance is. A representation \(φ\) is equivariant with a transformation \(g\) of the input if the transformation can be transferred to the representation output. Invariance is a special case of equivariance obtained when the transformation is the identity map <d-cite key="lenc2015understanding"></d-cite>.</p> <p>In the context of molecular conformations, we have to achieve the special case of equivariance in terms of rotation and translation, namely, roto-translation equivariance of the conformations which ensures that however the molecule is rotated or translated, the estimated (conditional) likelihood should be unaffected. GeoDiff considers the SE(3) Lie group which can be used to represent rotation and translation in 3D space <d-cite key="eade_2017"></d-cite>.</p> <h2 id="decomposing-geodiff">Decomposing GeoDiff</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/geodiff_main-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/geodiff_main-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/geodiff_main-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/geodiff_main.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The diffusion model of GeoDiff, adapted from <d-cite key="xu2022geodiff"></d-cite></p> <p><strong>Legend</strong>:</p> <ul> <li>\(C^{0}\) denotes the ground truth conformations</li> <li>\(C^{t}\), where \(t = 1,···, T\) is the index for diffusion steps and \(C^{t}\), is the sequence of latent variables with the same dimension</li> <li>\(q(C^{t} \mid C^{t-1})\) is the fixed posterior distribution</li> <li>\(p_\theta(C^{t-1} \mid G, C^{t})\) are the Markov kernels through which the conformations are refined</li> </ul> <h3 id="a-primer-on-diffusion-models">A primer on Diffusion Models</h3> <p>A diffusion probabilistic model <d-cite key="sohl2015deep"></d-cite> can be described as a latent variable model with two processes: the forward and the reverse generative processes. Intuitively, the diffusion process progressively injects small noises into \(C^{0}\), while the generative process learns to revert the diffusion process by gradually eliminating the noise to recover the ground truth. Diffusion models are trained by adding noise to the input, which the model then learns how to remove.</p> <p>In this blog post, we use the GeoDiff implementation of diffusion models to explain how the diffusion model works and how it is being used for the geometric representation of molecules. The implementation of the diffusion model in GeoDiff is inspired by the DDPM paper <d-cite key="ho2020denoising"></d-cite>. To give a quick overview, the forward process \(q\) transforms the original input into complete noise over a certain number of timesteps and follows a normal distribution; the \(p_0\) involves denoising complete noise to the actual input using a neural network.</p> <h3 id="forward-process">Forward process</h3> <p>Let \(q(\mathbf{C}^0)\) be the real data distribution of molecular conformation. We can sample from this distribution to get a conformation, \(\mathbf{C}^0 \sim q(\mathbf{C}^0)\). We define the forward diffusion process which adds Gaussian noise at each time step \(t\), according to a known variance schedule \beta_t which can be linear, quadratic, cosine, etc. as follows:</p> \[\begin{align}\tag{1} q\left(\mathcal{C}^{1: T} \mid \mathcal{C}^0\right)=\prod_{t=1}^T q\left(\mathcal{C}^t \mid \mathcal{C}^{t-1}\right) \end{align}\] <p>where \(\quad q\left(\mathcal{C}^t \mid \mathcal{C}^{t-1}\right)=\mathcal{N}\left(\mathcal{C}^t ; \sqrt{1-\beta_t} \mathcal{C}^{t-1}, \beta_t I\right)\)</p> <p>Instead of having to compute \(q\left(\mathcal{C}^t \mid \mathcal{C}^{t-1}\right)\) at every timestep \(t\), we could compute at an arbitrary timestep in closed form:</p> \[\begin{equation}\tag{2} q\left(\mathcal{C}^t \mid \mathcal{C}^0\right)=\mathcal{N}\left(\mathcal{C}^t ; \sqrt{\bar{\alpha}_t} \mathcal{C}^0,\left(1-\bar{\alpha}_t\right) I\right) \end{equation}\] <p>where \(\alpha_t=1-\beta_t\) and \(\bar{\alpha}_t=\prod_{s=1}^t \alpha_s\)</p> <p>Thus with a sufficiently large number of timesteps, the forward process could convert \(\mathcal{C}^0\) to whitened isotropic Gaussian and so we could set \(p\left(\mathcal{C}^T\right)\) as a standard Gaussian distribution.</p> <h3 id="reverse-process">Reverse Process</h3> <p>The reverse process involved recovering the original conformation \(\mathcal{C}^0\) from the white noise \(\mathcal{C}^T\) . We need the conditional distribution \(p_\theta\left(\mathcal{C}^{t-1} \mid \mathcal{G}, \mathcal{C}^t\right)\) to sample some random Gaussian noise \(\mathcal{C}^t\), and “denoise” gradually to end up with a sample from the real distribution \(\mathcal{C}^0\).</p> <p>However, the conditional distribution of \(p_\theta\left(\mathcal{C}^{t-1} \mid \mathcal{G}, \mathcal{C}^t\right)\) is intractable as it requires knowing the distribution of all possible conformations in order to calculate this conditional probability. Hence, a neural network could be used to learn this conditional probability distribution, let’s call it \(p_\theta\), with \(\theta\) being the parameters of the neural network, updated by gradient descent. Thus, we formulate the reverse process as a conditional Markov chain with learnable transitions:</p> \[\begin{align*}\tag{3} p_\theta\left(\mathcal{C}^{0: T-1} \mid \mathcal{G}, \mathcal{C}^T\right)=\prod_{t=1}^T p_\theta\left(\mathcal{C}^{t-1} \mid \mathcal{G}, \mathcal{C}^t\right) \end{align*}\] <p>where \(\quad p_\theta\left(\mathcal{C}^{t-1} \mid \mathcal{G}, \mathcal{C}^t\right)=\mathcal{N}\left(\mathcal{C}^{t-1} ; \mu_\theta\left(\mathcal{G}, \mathcal{C}^t, t\right), \sigma_t^2 I\right)\)</p> <p>Hence, the neural network in the reverse process needs to learn/represent the mean and variance. However, just like the DDPM paper, GeoDiff also lets the variance be user-defined and fixed, and \(\mu_\theta\) is the neural network that estimates means.</p> <p>GeoDiff uses a parametrisation trick inspired by the diffusion model implementation from the DDPM paper such that this parametrisation resembles Langevin dynamics and simplifies the diffusion model’s variational bound to an objective that resembles denoising score matching. Moreover, in the context of molecular conformation generation, this parametrisation trick is analogous to the physical force fields <d-cite key="schutt2017schnet, zhang2018deep, hu2021forcenet, shuaibi2021rotation"></d-cite>, which also gradually push particles towards convergence around the equilibrium states, and is defined by the following equation:</p> \[\begin{align*}\tag{4} \mu_\theta\left(\mathcal{C}^t, t\right)=\frac{1}{\sqrt{\alpha_t}}\left(\mathcal{C}^t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta\left(\mathcal{G}, \mathcal{C}^t, t\right)\right) \end{align*}\] <p>where \(\epsilon_\theta\) are neural networks with trainable parameters \(\theta\).</p> <p>Now, we need to make \(\epsilon_\theta\) roto-translational equivariant which we elaborate on in the next section.</p> <h3 id="making-the-reverse-process-roto-translation-equivariant">Making the reverse process roto-translation equivariant</h3> <p>Firstly, we need to assume the prior distribution of the conformations and the intermediary conformations generated during the forward process are systems with zero centre of mass (CoM) or CoM-free systems <d-cite key="kohler2020equivariant"></d-cite>. By considering CoM-free systems, moving the particles to zero CoM can always ensure translational invariance in the Markov kernels.</p> <p>GeoDiff employs the use of an equivariant convolutional layer, named graph field network (GFN) inspired by <d-cite key="thomas2018tensor, satorras2021n"></d-cite>. In the \(l\)-th layer, GFN takes node embeddings \(h_l \in \mathbb{R}^{n \times b}\) (\(b\) denotes the feature dimension) and corresponding coordinate embeddings \(x_l \in \mathbb{R}^{n \times 3}\) as inputs, and outputs \(h_{l+1}\) and \(x_{l+1}\) as follows:</p> \[\begin{align} \tag{5} &amp; \mathbf{m}_{i j}=\Phi_m\left(\mathbf{h}_i^l, \mathbf{h}_j^l,\left\|\mathbf{x}_i^l-\mathbf{x}_j^l\right\|^2, e_{i j} ; \theta_m\right) \\ \tag{6} &amp; \mathbf{h}_i^{l+1}=\Phi_h\left(\mathbf{h}_i^l, \sum_{j \in \mathcal{N}(i)} \mathbf{m}_{i j} ; \theta_h\right) \\ &amp; \mathbf{x}_i^{l+1}=\sum_{j \in \mathcal{N}(i)} \frac{1}{d_{i j}}\left(\mathbf{c}_i-\mathbf{c}_j\right) \Phi_x\left(\mathbf{m}_{i j} ; \theta_x\right) \tag{7} \end{align}\] <p>where</p> <ul> <li>\(\Phi\) are feed-forward networks</li> <li>\(d_{ij}\) are interatomic distances</li> <li>\(\mathcal{N}(i)\) is the neighbourhood of the \(i\)-th node, which consists of both connected atoms and other ones within a radius threshold \(\tau\).</li> </ul> <p>By introducing the neighbourhood function, we enable the model to accurately represent distant interactions between atoms, as well as the ability to handle partially disconnected molecular graphs. Initial embeddings \(h_0\) are combinations of atom and timestep embeddings, and \(x_0\) are atomic coordinates. A key change in GFN compared to a vanilla GNN is \(x\) being updated as a combination of radial directions weighted by \(\Phi_x\): \(\mathbb{R}^b \rightarrow \mathbb{R}\) as seen in equation \((7)\). This allows the roto-translation equivariance property to be induced in the reverse process.</p> <h3 id="improved-training-objective">Improved Training Objective</h3> <p>Now, we need to set the training objective having considered the reverse process dynamics. We cannot compute the exact log-likelihood of the generative process, as it involves computing the likelihood of the observed molecular conformation given the parameters of the model. However, this likelihood is difficult to compute, as it would require integrating over all possible intermediate conformations, giving us a high-dimensional integral that cannot be solved analytically. Therefore, the authors have opted to maximize the variational lower bound (ELBO), as defined below:</p> \[\begin{aligned} \mathbb{E}\left[\log p_\theta\left(\mathcal{C}^0 \mid \mathcal{G}\right)\right] &amp; =\mathbb{E}\left[\log \mathbb{E}_{q\left(\mathcal{C}^{1: T} \mid \mathcal{C}^0\right)} \frac{p_\theta\left(\mathcal{C}^{0: T} \mid \mathcal{G}\right)}{q\left(\mathcal{C}^{1: T} \mid \mathcal{C}^0\right)}\right] \\ &amp; \geq-\mathbb{E}_q\left[\sum_{t=1}^T D_{\mathrm{KL}}\left(q\left(\mathcal{C}^{t-1} \mid \mathcal{C}^t, \mathcal{C}^0\right) \| p_\theta\left(\mathcal{C}^{t-1} \mid \mathcal{C}^t, \mathcal{G}\right)\right)\right]:=-\mathcal{L}_{\mathrm{ELBO}}\end{aligned}\] <p>where \(q\left(\mathcal{C}^{t-1} \mid \mathcal{C}^t, \mathcal{C}^0\right)\) is analytically tractable as \(\mathcal{N}\left(\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} \mathcal{C}^0+\frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t} \mathcal{C}^t, \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t\right)\).</p> <p>Using the parametrisation trick in the reverse process as seen in equation \((4)\), the ELBO could be simplified by taking the KL divergences between Gaussians as weighted \(\mathcal{L}_2\) distances between the means \(\epsilon_\theta\) and \(\epsilon^3\) as follows: \(\mathcal{L}_{\mathrm{ELBO}}=\sum_{t=1}^T \gamma_t \mathbb{E}_{\left\{\mathcal{C}^0, \mathcal{G}\right\} \sim q\left(\mathcal{C}^0, \mathcal{G}\right), \epsilon \sim \mathcal{N}(0, I)}\left[\left\|\epsilon-\epsilon_\theta\left(\mathcal{G}, \mathcal{C}^t, t\right)\right\|_2^2\right]\) where \(\mathcal{C}^t=\sqrt{\bar{\alpha}_t} \mathcal{C}^0+\sqrt{1-\bar{\alpha}_t} \epsilon\).</p> <p>The idea behind this objective is to independently sample chaotic conformations of different timesteps from \(q\left(C^{t-1} \mid C^t, C^0\right)\), and use \(\epsilon_\theta\) to approximate the noise vector \(\epsilon\).</p> <h3 id="sampling">Sampling</h3> <p>Now, we can generate stable molecular conformations via sampling. Given a graph \(\mathcal{G}\), its geometry \(\mathcal{C}^0\) is generated by first sampling chaotic particles \(\mathcal{C}^T \sim p\left(\mathcal{C}^T\right)\). For each timestep in the reverse process \(t=T, T-\) \(1, \cdots, 1\), we first shift the CoM of the conformation to zero, compute the transition means, \(\mu_\theta\left(\mathcal{G}, \mathcal{C}^t, t\right)\), using equation \((4)\) and sample \(\mathcal{C}^{t-1} \sim\) \(p_\theta\left(\mathcal{C}^{t-1} \mid \mathcal{G}, \mathcal{C}^t\right)\). The sampling algorithm is given in pseudo-code below:</p> <table> <thead> <tr> <th style="text-align: left">Algorithm 1 Sampling Algorithm of GEODIFF</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Input: the molecular graph \(\mathcal{G}\), the learned reverse model \(\epsilon_\theta\).</td> </tr> <tr> <td style="text-align: left">Output: the molecular conformation \(\mathcal{C}\).</td> </tr> <tr> <td style="text-align: left">1: Sample \(\mathcal{C}^T \sim p\left(\mathcal{C}^T\right)=\mathcal{N}(0, I)\)</td> </tr> <tr> <td style="text-align: left">2: for \(s=T, T-1, \cdots, 1\) do</td> </tr> <tr> <td style="text-align: left">3: \(\quad\) Shift \(\mathcal{C}^s\) to zero CoM</td> </tr> <tr> <td style="text-align: left">4: \(\quad\) Compute \(\mu_\theta\left(\mathcal{C}^s, \mathcal{G}, s\right)\) from \(\epsilon_\theta\left(\mathcal{C}^s, \mathcal{G}, s\right)\) using equation 4</td> </tr> <tr> <td style="text-align: left">5: \(\quad\) Sample \(\mathcal{C}^{s-1} \sim \mathcal{N}\left(\mathcal{C}^{s-1} ; \mu_\theta\left(\mathcal{C}^{\mathcal{s}}, \mathcal{G}, s\right), \sigma_t^2 I\right)\)</td> </tr> <tr> <td style="text-align: left">6: end for</td> </tr> <tr> <td style="text-align: left">7: return \(\mathcal{C}^0\) as $\mathcal{C}$$</td> </tr> </tbody> </table> <h2 id="but-why-generative-models">But why generative models?</h2> <p>The purpose of prediciting molecular conformations is to enable human experts to analyse the properties of the molecules and understand how these properties affect the viability of a molecule as a drug candidate. Therefore, it is important that the molecular conformations generated are diverse to capture the different possible conformations that could occur in nature but the conformations generated should not deviate significantly such that the analysis is affected. To set a threshold for the different possible conformations, the standard metric used has been selecting conformations that are within a certain root-mean-square deviation (RMSD), say a few ångströms, of the true structure.</p> <p>However, the objective of maximizing the proportion of predictions with RMSD within some tolerance \(\epsilon\) is not differentiable and thus, cannot be used for training with stochastic gradient descent. Instead, maximizing the expected proportion of predictions with RMSD &lt; \(\epsilon\) corresponds to maximizing the likelihood of the true structure under the model’s output distribution, in the limit as \(\epsilon\) goes to 0. This concept inspires the development of a generative model, whose objective is to minimize an upper bound on the negative log-likelihood of observed molecular structures under the distribution of the model. As a result, the problem of molecular docking is treated as a task of learning a distribution over possible positions of a ligand molecule conditioned on the protein structure and a diffusion generative model is developed to represent this space. Therefore, this observation has motivated several works on the use of generative models for molecular conformation generation, such as GeoDiff.</p> <h2 id="future-work">Future work</h2> <p>While diffusion models have shown promising results over the past months, there is still a need for further research and development to address some of the limitations and drawbacks of the use of diffusion models in conformer generation. As explained in our blog post, generative models are well-suited for molecular conformation generation. However, there are other kinds of established generative models, such as autoencoders that have been used for the same task <d-cite key="gomez2018automatic"></d-cite>. For instance, comparing diffusion models and autoencoders, diffusion models can handle all noise levels with a single set of parameters, without any bottlenecks, which an autoencoder would have <d-cite key="dieleman2022diffusion"></d-cite>. Thus, it is essential to study the advantages and disadvantages of different types and variants of generative models for conformation generation and understand what changes could be done to them to make them more suitable for molecular conformation.</p> <p>It must be noted that the experiments described in the study are done on benchmark geometric datasets, GEOM-QM9 <d-cite key="ramageom"></d-cite> and GEOM-Drugs <d-cite key="axelrod2022geom"></d-cite>, rather than industrial data, with a relatively small number of samples/drug/molecular compounds. Industry data may exhibit greater variability than standard geometric datasets, as they may be more diverse, complex and subject to greater fluctuations than these geometric datasets, which tend to be more standardized and more predictable.</p> <p>GeoDiff as a framework sets a precedent for future work that could marry the concepts of geometric deep learning and diffusion models across various domains. In the context of drug discovery, it would be interesting to extend this framework to more challenging structures, such as proteins, which may enable more accurate prediction of protein folding, protein-protein interactions and protein-ligand binding positions which would facilitate the design of new drugs and treatments. Additionally, this framework could potentially be applied to other complex systems beyond proteins, such as RNA molecules, to enable more efficient and accurate prediction of their behaviour and properties. Continued research in this area has the potential to revolutionize drug discovery and development, as well as advance our understanding of the fundamental principles governing the behaviour of complex biological systems.</p> <p>Research on the application of diffusion models in the life and natural sciences is still in its infancy, with great potential for improvement in terms of both theory as well as empirical testing. The GeoDiff model could be improved in terms of more efficient sampling and improved likelihood maximization methods. Traditionally, generating samples from diffusion models demand iterative approaches that involve a large number of evaluation steps. Recent work, such as the paper on Torsional Diffusion <d-cite key="jing2022torsional, satorras2021n"></d-cite> was able to speed up the sampling process, while also enhancing the quality of the resulting samples. Experimentally, Torsional Diffusion only takes 5 to 20 steps in comparison to GeoDiff which takes around 5000 steps <d-cite key="galkin_2022"></d-cite>.</p> <p>Looking ahead, GeoDiff has set a clear example for the use of diffusion models on geometric representations which could be extended to several problems, especially in the field of drug discovery. The novel contributions made by</p> <d-cite key="xu2022geodiff"></d-cite> <p>are motivated by the physical characteristics of the molecular conformation generation problem, which has resulted in a strong candidate method for conformation generation that could act as a springboard for even more effective and efficient methods that would eventually benefit the field of drug discovery as a whole.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[The chemical space of molecular candidates is vast, and within this space, interesting and powerful drugs are waiting to be found. The use of machine learning has shown to be an effective method to speed up the process of discovering novel compounds, especially the use of (deep) generative models. The recent surge in graph generative models have opened up new avenues for exploring the chemical space of molecular candidates, enabling a more efficient and systematic exploration of the chemical space, increasing the chances of finding novel and potent molecules. One of the recent breakthroughs includes the use of diffusion models, which have proven to yield superior performance in molecular conformation tasks, among others. In this blog post, we aim to highlight one of them, which is the 'GeoDiff - A Geometric Diffusion Model for Molecular Conformation Generation' paper by Xu et al. (2022). We aim to distill the paper in semi-layman terms, to provide researchers and practitioners with a deeper understanding of the (i) methodology and results and (ii) (societal) implications of this breakthrough in the field of drug discovery and (iii) discuss future applications in the field of (bio)medicine.]]></summary></entry><entry><title type="html">Language and (Meta-) RL - An ode to structure</title><link href="https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/language-rl/" rel="alternate" type="text/html" title="Language and (Meta-) RL - An ode to structure"/><published>2023-02-11T00:00:00+00:00</published><updated>2023-02-11T00:00:00+00:00</updated><id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/language-rl</id><content type="html" xml:base="https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/language-rl/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>When Wittgenstein wrote, “Die Grenzen meiner Sprache bedeuten die Grenzen meiner Welt” (The limits of my language constitute the limits of my world), I doubt he would have even remotely imagined a world where one could ask a large language model like ChatGPT ‘Can you provide me Intelligent quotes on language by famous philosophers?’ before writing one’s blog on the role of language for Reinforcement Learning.</p> <p>Yet, here we are, living in a time where sequential decision-making techniques like Reinforcement Learning are making increasingly larger strides, not just in robot manipulation <d-cite key="lee-sciro20a"></d-cite> and games considered once to be the pinnacle of human intelligence <d-cite key="silver-nature16a"></d-cite>, but also in an increasingly novel set of scenarios like chemistry <d-cite key="zhou-acs17a"></d-cite>, logistics <d-cite key="li-aamas19a"></d-cite>, etc. While a lot of the theory in RL existed from classical times, the success of integrating Neural Networks as function approximators has created a sort of Cambrian explosion in the last years. Traditionally, a major focus of the field was on developing techniques that can learn to solve an inherent optimization problem, like learning a solution to a maze. As the field evolved in the last years, its scope has started to broaden to encompass bigger questions, like whether a learned policy to solve a maze can generalize to other configurations (Generalization <d-cite key="kirk-arxiv21a"></d-cite>), or whether a policy can be transferred to scenarios where conditions differ slightly from the training conditions (Robustness, Deployability), or how can we design agents in a data-driven manner (AutoRL <d-cite key="parker-jair22a"></d-cite>). Yet, a major bottleneck in current RL techniques is that they are not yet, largely, ready for real-world deployment.</p> <p>Parallelly, another Cambrian explosion has been happening in the field of Natural Language Processing (NLP). Language models have come a long way since the days of word embedding and sequence-sequence models, to the agent of attention and pre-trained models. Crucially, as this growth continues with newer innovations like ChatGPT, it also leads us to innovative applications of these language models in other fields of Machine Learning, including Reinforcement Learning</p> <p>In this blog post, I will explore the connection between Natural Language and RL through some recent and not-so-recent ICLR publications that I find very interesting. Since this topic is vast, so much so that a full survey has been written on it (<d-cite key="Luketina-ijcai19a"></d-cite>), I will limit the focus to how language can be used to augment RL pipelines (Language-assisted RL), and not on the use of RL for language training (RL for language). Through this blog, my hope it to visit two ideas in using Language for RL that exist at two very different points in the Deep RL timelines, and yet hold significance in the way they use language to augment the RL pipeline.</p> <h3 id="rl-basics">RL basics</h3> <p>To better cater to audiences beyond the RL community, I think it would be good to briefly revise some core concepts. RL folks are more than welcome to skip to the next section. I am going to try my best to keep it less math-oriented, but I apologize in advance on behalf of the symbols I will use.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/RL-pipeline-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/RL-pipeline-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/RL-pipeline-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/RL-pipeline.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>(Picture Credits: <a href="https://jannik-zuern.medium.com/reinforcement-learning-to-survive-in-a-hostile-environment-3658624a5d83">https://jannik-zuern.medium.com/reinforcement-learning-to-survive-in-a-hostile-environment-3658624a5d83</a>)</p> <p>The key idea in RL (shown in the figure below) is to model the learning process as an agent acting in an environment. At every time-step $t$, the environment exists in a state $s$ and the agent can take an action $a$ to change this state to $s’$. Based on this transition, the agent gets a reward $r$. This process is repeated either for some number of steps until a termination condition is reached (also called episodic RL), or indefinitely (Non-episodic RL).</p> <p>A common way to specify such problems is using Markov Decision Processes (MDPs), which can be written as a Tuple $\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, \mathcal{P}. R, \rho \rangle$ where</p> <ul> <li>$\mathcal{S}$ is the state-space i.e. states are sampled from this space</li> <li>$\mathcal{A}$ is the action space</li> <li>$P: \mathcal{S} \times \mathcal{A} \to \Delta(\mathcal{S})$ is a kernel that defines the probability distribution over the next states i.e. given a state and action, it tells us the probability of ending in the next state</li> <li>$R: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$ is the reward function</li> <li>$\rho$ is the initial state distribution</li> </ul> <p>The aim of the agent is to maximize the sum of rewards obtained, a quantity called the return $G_t$, by learning a policy $\pi$ that either maps a given state to an action (deterministic policies), or distributions over actions (stochastic policies). Thus, we can model any task $\tau$ using the MDP formalism and train an RL agent to learn to policy to solve this task.</p> <h3 id="language-generalization-and-multi-task-rl">Language, Generalization, and Multi-Task RL</h3> <p>When we talk about settings that go beyond solving a single task $\tau$ we are faced with the problem of training an agent to solve multiple MDPs $\mathcal{M}_1, \mathcal{M}_2, \dots$, and this is non-trivial. While we want the agent to learn to perform well at these tasks, we don’t want to do it the naive way by training the agent to perform all of these tasks separately. Thus, the key challenge is to figure out a way to make this problem more tractable.</p> <p>A good point to try to look for solutions is to take inspiration from how humans learn in the real world. This is where the idea of knowledge reuse comes into the picture. Consider the case of learning how to ride a motorcycle. This can be broken down into a sequence of tasks like learning how to balance yourself on a motorcycle, learning how to navigate a two-wheeler, learning how to run a motor-based vehicle, etc. A lot of these skills can be acquired even before we come near a motorcycle through other tasks. For example, learning how to walk teaches us navigation, while learning how to ride a bicycle teaches us things about two-wheelers, and so on. Thus, when a human would come to the task of learning how to ride a motorcycle, they would essentially reuse a lot of skills that they have learned before. In other words, humans meta-learn between tasks.</p> <p>If we look a bit deeper into this process, some interesting concepts come to the forefront:</p> <ul> <li>The only reason a human can meta-learn between these tasks incrementally is that there is an overlap between the requirements of solving these individual tasks. In other words, there is some underlying structure that can be captured by a learner between tasks that allows them to transfer learned knowledge between tasks</li> <li>Given a learned set of skills, any problem that can be solved by composing these skills can be potentially solved by a learner who has acquired these skills. IN other words, complex tasks that have an underlying compositional structure can be solved by learning individual components and combining them</li> </ul> <p>So, when we translate this to the problem of solving a collection of MDPs $\mathcal{M}_1, \mathcal{M}_2, \dots$, one of the key bottlenecks is figuring out a way to capture the underlying structure between them. Naively, we could implicitly learn this structure, and a lot of techniques in Meta-RL do exactly this. But what if we had some additional side information that we could leverage to do this more easily?</p> <p>This is where language shows its importance. The world is full of structure in the form of relations between objects (Relational Structure), Causal relationships between events (Causal Structure), etc. Crucially, while humans do meta-learn between individual tasks, this ability is significantly bolstered and catalyzed by the existence of language <d-cite key="edmiston-cognition15a"></d-cite>. Language not only helps us transfer knowledge but also plays a central role in helping us form necessary abstractions that can boost our ability to form associations between tasks <d-cite key="keil-EC00a"></d-cite>. Thus, a very interesting avenue opens up when we start to consider the use of language as a way of incorporating structure into RL pipelines, be it through structure between MDPs in the Multi-task setting, or through structure in the process of inferring things like scene representation and reward signals in the real-world deployment of RL systems.</p> <h2 id="idea-1---policy-sketches">Idea 1 - Policy Sketches</h2> <p>Source: <d-cite key="andreas-icml17a"></d-cite></p> <p>When we talk about composing skills, a long line of work in RL has been on the idea of learning policies hierarchically (Hierarchical RL). For example, in the task of locomotion, controllers can learn individual control policies while a higher-level controller can learn a policy whose actions are to coordinate the lower-level controllers. This is similar to how we try to solve a problem using the dynamic programming method — by breaking it down into subproblems and then combining the solutions together.</p> <p>To demonstrate the approach followed in this paper, I will use the example provided in the paper shown in the figure below :</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/Policy-Sketches-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/Policy-Sketches-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/Policy-Sketches-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/Policy-Sketches.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In this simple grid world scenario. we have two tasks:</p> <ul> <li>$\tau_1$ requires an agent to make plans by first collecting wood and then using the collected wood on a workbench</li> <li>$\tau_2$ requires the agent to make sticks by first collecting wood and then taking the wood to a toolshed</li> </ul> <p>At the onset, we see that the first step in both of these tasks is to collect wood. Thus, if an agent learns a sub-policy to collect wood, it can reuse it for both tasks. Each subtask can be associated with a symbol $b$ and the associated policy by $\pi_b$. Now, given a set of learned symbols $b_1, b_2,\dots$ and the corresponding sub-policies $\pi_{b_1}, \pi_{b_1}, \dots$, a high-level task can be described by a sequence of symbols, which the authors call a sketch.</p> <p>The sketch is akin to a word using these symbols and thus, follows the basic rules of language. Given a sketch, a complex policy $\Pi_b$ can be executed by executing a sequence of policies. To make this technically feasible, the policy executions come with termination conditions that signify the duration for which a policy needs to be executed, something that is very standard in Policy-based.</p> <p>Thus, the authors recast the hierarchical problem as a problem of modular sketches using language symbols. Additionally, by being associated with symbols, the policies end up being more interpretable.</p> <p>Going a bit deeper into the technicalities for the interested folks, the authors model the multi-task setting by assuming the tasks to be specified by the reward and initial distribution $(R_\tau, \rho_\tau)$. AT any time step, a subpolicy selects either a low-level action $a \in \mathcal{A}$ or a special stop action indicating termination. They use Policy gradients for the optimization with the crucial factor of having an actor per symbol but one critic per task. Since actors can participate in multiple tasks, by constraining the critic to task, they are able to baseline policies using task-associated values. Finally, to tackle the issue of sparse rewards by using a curriculum learning approach, where the learner is initially presented with shorter sketches, which are progressively increased in length as the learner learns the sub-policies.</p> <h2 id="idea-2---reward-specification-via-grounded-natural-language">Idea 2 - Reward specification via grounded Natural Language</h2> <p>Source: <d-cite key="mahmoudieh-icml22a"></d-cite></p> <p>While approaches like policy sketches are very powerful in composing policies together using the symbolic capabilities and inherent compositional structure of language, they still require hand-designed rewards per task. This is something that is prevalent throughout the RL literature.</p> <p>The fundamental issue with this is that reward signals can be expensive to design in the real world and they usually require knowledge about the true state. On real-world tasks, however, RL agents usually have only access to pixel observations, that could be generated for latent states, for example. A common alternative to reward design is to hand-label goal images or collect demonstrations that can be used to teach an RL agent reward signals. This is a labor-intensive process as well.</p> <p>The authors of this paper take a different route by leveraging text descriptions. Specifically, language can be used to describe a task by providing the agent with descriptions of a goal and/or spatial relationships between the entities in the environment. When combined with the observation, this can be used to compute the proximity of an agent to the goal and thus, help guide the agent. The authors in this work specifically use language in the form of spatial relationships between entities. For example, a sentence like ‘A on the left of B’ would be interpreted as the x coordinate B being greater than that of A. By using multi-camera scenes, they are able to associate each description with a symbol by comparing the camera view that matches the condition.</p> <p>Once the labels have been generated, they train the reward model using a contrastive loss where the model essentially predicts which caption matches which image in a sampled batch of random images. Crucially, the aim is to maximize the cosine similarity between the image and text embeddings of the correct pairs and minimize the cosine similarity of the embeddings of the incorrect pairs.</p> <p>Once this has been achieved, the learned model can be used to provide rewards to RL policies. For this, they first learn several tasks using RL and the reward model. They create a large dataset of the rollouts of the learned policies and pair each trajectory with the goal text description of the tasks it was trained to learn. Finally, this data is used for supervised learning for predicting actions using text and images. The process has been visualized in the figure below:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/Grounding-Pic-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/Grounding-Pic-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/Grounding-Pic-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/Grounding-Pic.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="conclusion">Conclusion</h2> <p>In this post, we have seen two ways of using language for RL. There have been a lot of other ways recently in this direction. Some examples of these are</p> <ul> <li> <d-cite key="lampinen-icml22a"></d-cite> <p>augment policy networks with the auxiliary target of generating explanations and use this to learn the relational and causal structure of the world</p> </li> <li> <d-cite key="kumar-neurips22a"></d-cite> <p>use language to model compositional task distributions and induce human-centric priors into RL agents.</p> </li> </ul> <p>Given the growth of pre-trained language models, it is only a matter of time before we see many more innovative ideas come around in this field. Language, after all, is a powerful tool to incorporate structural biases into RL pipelines. Additionally, language opens up the possibility of easier interfaces between humans and RL agents, thus, allowing more human-in-the-loop methods to be applied to RL. Finally, the symbolic nature of natural language allows better interpretability in the learned policies, while potentially making them more explainable. Thus, I see this as a very promising direction of future research</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[It has been argued that language can be a very powerful way to compress information about the world. In fact, the learning of humans is significantly sped up around the time they start understanding and using language. A natural question, then, is whether the same can be argued for sequential decision-making systems that either learn to optimize a single or multiple, task. To this end, there has been a surge of works exploring the use of Language in Reinforcement Learning (RL) and Meta-RL. The goal of this blog post is to try and explain some of the recent works in this sub-field and help elucidate how language can help with incorporating structure about the environment to improve learning generalization in (Meta) RL]]></summary></entry><entry><title type="html">Transformers Learn Faster by Taking Notes on the Fly</title><link href="https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/taking_notes_on_the_fly/" rel="alternate" type="text/html" title="Transformers Learn Faster by Taking Notes on the Fly"/><published>2023-02-07T00:00:00+00:00</published><updated>2023-02-07T00:00:00+00:00</updated><id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/taking_notes_on_the_fly</id><content type="html" xml:base="https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/taking_notes_on_the_fly/"><![CDATA[<h2 id="introduction">Introduction</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/example-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/example-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/example-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/example.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 1 from the paper, which describes an example situation in which taking notes could be useful. Here COVID is a rare word. Therefore, the model is struggling with the completion task on the left because it has not possibly seen many sentences with the word COVID in it. Thus, we take notes of the contextual information of it as we see examples on it in the training set and we quickly learn to associate it with which words! </div> <p>Transformers, which were invented by Google in 2017 <d-cite key="vaswani2017attention"></d-cite>, have become the go-to architecture for various tasks in many domains, such as natural language processing and computer vision <d-cite key="dosovitskiy2020image"></d-cite>, <d-cite key="devlin2018bert"></d-cite>, <d-cite key="radford2018improving"></d-cite>, <d-cite key="brown2020language"></d-cite>. The success of transformers are mainly because they have two amazing properties:</p> <ol> <li> <p>They are phenomenal in grasping the context of words within the bodies of text that they belong to.</p> </li> <li> <p>They do not process the input sequences in order. Thus, their operations can easily be parallelized.</p> </li> </ol> <p>Equipped with these powerful features, transformers have excelled in unsupervised pre-training tasks, which is the driving force of several state-of-the-art models, such as BERT and GPT-3. In unsupervised pre-training, a large and diverse dataset is used to train the (baseline) model. If someone wishes to fine-tune the base model for a specific task, they can do so by training it with a relatively smaller, task-specific dataset.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/unsupervised-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/unsupervised-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/unsupervised-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/unsupervised.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> During unsupervised pre-training, the model is trained on a large unlabeled dataset. Then, it becomes a powerful baseline model that can be fine-tuned to work with various tasks. </div> <p>Generalization can be achieved with a sufficiently large model that is trained on sufficiently diverse and large data <d-cite key="radford2019language"></d-cite> <d-cite key="tirumala2022memorization"></d-cite>. However, pre-training large models is very time-consuming and costly in terms of environmental impacts and monetary resources <d-cite key="strubell2019energy"></d-cite> <d-cite key="chen2021bert2bert"></d-cite>. Thus, reducing the pre-training time and cost for transformer-based models is an imminent concern for machine learning practitioners. One area that has room for improvement is how quickly the model learns the embeddings of the rare words. It has been shown by many works that the embeddings of those words are noisy and not optimized <d-cite key="bahdanau2017learning"></d-cite>, <d-cite key="gong2018frage"></d-cite>, <d-cite key="khassanov2019constrained"></d-cite>, <d-cite key="schick2020s"></d-cite>. Furthermore, Wu et al. 2021 empirically observe that 20% of all sentences in the corpus contain a rare word and they propose a <em>“note-taking”</em> approach improves model’s ability to learn the embeddings of rare words <d-cite key="wu2021taking"></d-cite> . Impressively, they reduce the pre-training time of well-known large language models (LLMs), such as BERT, by 60%. The approach is called Taking Notes on the Fly (TNF) and we will dive deep into how it works in this blog post!</p> <h2 id="background">Background</h2> <h3 id="transformers">Transformers</h3> <p>Wu et al. <d-cite key="wu2021taking"></d-cite> extends the BERT model <d-cite key="devlin2018bert"></d-cite>, which is a transformer-based model, with an external memory. A transformer is composed of alternating multi-head attention and feed-forward layers. The initial input to the multi-head attention layer is the sum of word embeddings and positional embeddings. Each one-hot encoded token is multiplied with a weight matrix in order to obtain a real-valued non-sparse representation. The weight matrix is learned throughout the training. Because transformers do not process words in order, we also need to provide some information about the position of the token in a sentence. This is incorporated into the training by the “positional embedding (encoding)” \((PE)\) vector, composed of sine and cosine pairs.</p> \[PE_{\text{pos},2i} = sin(pos / 10000^{2i/d_{embed}} )\] \[PE_{\text{pos},2i+1} = cos(pos / 10000^{2i/d_{embed}} ),\] <p>where \(pos\) is the position of the token in the sentence, \(d_{embed}\) is the embedding dimension of the model, and \(i\) refers to the dimension in the \(PE\) vector. Note that the positional embeddings do not depend on the meaning of the words, but only the position of them!</p> <p>Self attention mechanism allows the model to relate words in a sentence through a set of learnable query \((Q)\), key \((K)\) and value \((V)\) vectors. The output of the attention function calculates a compatibility score for each pair of words in the sentence. Mathematically, self attention can be expressed as</p> \[\text{self-attention} (Q,K,V) = softmax(QK^T / \sqrt{(d_k)}),\] <p>where \(d_k\) is the dimension of hidden representations. In order to improve the representational power of the model, <d-cite key="vaswani2017attention"></d-cite> proposed a multi-head attention mechanism. In particular, the \(self-attention\) function is calculated several times independently, results are concatenated, and linearly projected into the desired dimension.</p> <p>BERT is a masked language model which uses the transformer architecture. During training time, 15% of the words in the sentence are masked or replaced with a random word. The model learns to predict the words that are masked.</p> <h3 id="word-distribution-in-texts">Word Distribution in Texts</h3> <p>The distribution of the words in a natural language corpora follow Zipf’s law <d-cite key="zipf1932selected"></d-cite>, that is, the frequency \(n^{th}\) most frequent word is proportiional to \(1/n^\alpha, \: where \:\: \alpha \sim 1\).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/zipf_law-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/zipf_law-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/zipf_law-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/zipf_law.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The frequencies of 50 most common words in Brown Corpus<d-footnote>Details can be found <a href="http://korpus.uib.no/icame/manuals/BROWN/INDEX.HTM"> in the link.</a></d-footnote>. Green is the word counts estimated by Zipf's law, blue is the actual count. Image is taken from <d-cite key="zipf"></d-cite>. </div> <p>In other words, number of popular words are much less than of rare words, yet their frequency is much larger. This harms pretraining of LLMs because of the sparse and inaccurate optimization of neural networks, rare words are much likely to generate noisy and low-quality embeddings <d-cite key="gao2019representation"></d-cite>.</p> <h2 id="related-work">Related Work</h2> <p>Pre-training of LLMs has become a burden in terms of training time and power consumption. Still, it is essential for almost every downstream task in NLP. This computational cost is addressed by several studies in terms of altering the model or utilizing the weight distribution of neural networks’ layers. Particularly, <d-cite key="clark2020electra"></d-cite> added a discriminator to predict if each word in the sentence that is completed by the generator is correct or not. Another important work arised after the observation that the attention distributions of top and bottom layers are quite similar. <d-cite key="gong2019efficient"></d-cite> proposed an iterative algorithm that doubles the number of layers after each training episode.</p> <p>The efficiency of pretraining LLMs has shown to be incresed, still the heavy-tailed distribution of words in natual language corpora is an obstacle in further development <d-cite key="strubell2019energy"></d-cite>. Note taking approach positively impacts learning performance in humans <d-cite key="makany2009optimising"></d-cite>. This idea is inspired studies in terms of contributing to training efficiency <d-cite key="wu2021taking"></d-cite> and increasing performance in downstream tasks <d-cite key="feng2022memory"></d-cite>, <d-cite key="fevry2020entities"></d-cite>, <d-cite key="guu2020retrieval"></d-cite>, <d-cite key="khandelwal2019generalization"></d-cite>.</p> <p>It is shown that the frequency of words affect the embeddings. Additionally, most of the rare words’ embeddings are close to each other in embedding space indepent from its semantic information while the neighbors of frequent words are the ones that have similar meaning <d-cite key="gong2018frage"></d-cite>. Initial studies mainly used subword information to encode semantic information, this approach is shown to be valuable for morphologically rich languages <d-cite key="pmlr-v32-santos14"></d-cite>, <d-cite key="kim2016character"></d-cite>, <d-cite key="el2019parsimonious"></d-cite>. Recently, this problem is also adressed by using adverserial training where a discriminator classifies each word as ‘frequent’ or ‘rare’ allowing semantic information to be encoded <d-cite key="gong2018frage"></d-cite>.</p> <h2 id="methodology">Methodology</h2> <p>Because learning the embeddings of rare words is arduous, it takes a lot of training epochs for the model to make up for the resulting loss in quality. Thus, the authors propose keeping a third type of embedding (besides the word embeddings and positional embeddings), which is designed to retain additional information about the rare words. This embedding type can be considered as <em>taking notes</em> on the contextual information of these rare words as the training progresses, is also called the note dictionary, and is updated as the training progresses.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/overview-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/overview-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/overview-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/overview.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 2 from the paper, which gives an overview of the note taking process. The note embeddings are randomly initialized and the other two embeddings are computed. Then, their sum is given to the transformer encoder as input. For every rare word encountered when going through the training data, its contextual information is calculated and the corresponding note embeddings are updated accordingly. This process goes on as the data is being fed to the transformer. </div> <p>At this point, we assume that the text has already been pre-processed using Byte Pair Encoding (BPE<d-footnote>A very nice <a href="https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10"> blog post</a> about BPE. Reading it is highly encouraged as it also provides visuals on BPE.</d-footnote>), which is a popular method that is used as a part of the text embedding process for NLP tasks <d-cite key="sennrich2015neural"></d-cite>. In BPE, each word is represented as a concatenation of sub-word units, which are selected according to how much each they unit occur in the given text. For example, if the sub-word <b>“pre”</b> occurs in the text frequently, it will be represented with a single character, such as <b>“X”</b> in this encoding. This way, the textual data is compressed and manageable. Also, because each sub-word unit gets their own embedding, we get a hybrid approach between word-level and character-level embeddings. Therefore, the embedding of each word might very well be made up of multiple consecutive tokens. With this information in mind, let us walk through the steps of note taking!</p> <p>The first three steps are about initializing the required variables and determining the hyper-parameters of the scheme.</p> <p>0a. Randomly initialize the note dictionary, \(NoteDict\).</p> <p>0b. Determine a window size (\(2k\) as denoted in the paper), which corresponds to the number of surrounding tokens whose embedding will be included in the note.</p> <p>0c. Determine a discount factor, \(\gamma\in (0,1)\). This will determine how much weight we give to each occurrence of the rare word and the corresponding contextual information.</p> <p>Now, note taking begins!</p> <p>1.For each word \(w\) in the training corpora, check if the word is a rare word or not. If it is rare, mark the index of the starting and ending sub-word tokens of the word with \(s\) and \(t\), respectively.</p> <p>2.Compute the output of the transformer encoder on the input embeddings (positional+token+note embeddings). The output will be composed of \(d\)-dimensional vector per token. Call the output of the transformer encoder on position \(j\), \(c_j\in \mathbb{R}^d\).</p> <p>3.Given a sequence of tokens \(x\) with word \(w\) in it, sum the \(d\)-dimensional input embedding vectors of all tokens located between indices \(s-k\) and \(t+k\) and divide this sum by \(2k+t-s\), namely, the number of tokens within that interval. The resulting vector is the note of \(w\) taken for sequence \(x\), \(Note(w,x)\). Mathematically, we have \(Note(w,x)=\dfrac{1}{2k+t-s}\sum_{j=s-k}^{t+k}c_j\).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/numberline-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/numberline-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/numberline-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/numberline.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> This figure demonstrates contextual embedding vectors at which locations will be selected and summed with an example. This line represents the indices of a sequence of length 11. Let us assume that the rare word is contained within tokens 4 to 6, and k=2, which makes the window size 2k=4. Thus, we sum the tokens at location 4, 5, 6, as well as 3, 4, (which are the two immediate left tokens) and 7,8 (which are the two immediate right tokens). Finally, we divide the each element of the resulting vector by 6, which is the total number of elements in the interval. </div> <p>4.To update the note embedding of w, NoteDict(w), take the exponential moving average of its previous value and Note(w,x) using the discount factor, namely, \(NoteDict(w)=(1-\gamma)NoteDict(w)+\gamma Note(w,x)\). This way, we can choose how much importance we assign to each occurrence of a rare word.</p> <p>This process repeats until all of the sentences are processed this way. Note that, this can be achieved on the fly, as the model processes each sentence. Now that we have our notes neatly stored in \(NoteDict\), let us incorporate them into the training process! We again take the exponential moving average of the sum of the positional and token embeddings (the embedding used in the original transformer paper) with the corresponding \(NoteDict\) value using another parameter called \(\lambda\in(0,1)\). In particular, for every word \(w\) that occurs in both \(NoteDict\) and sequence \(x\), each location corresponding to the word \(w\) and its surrounding \(2k\) tokens is set to the weighted of the sum of the positional and token embeddings with the corresponding NoteDict value. Any other location is set to the sum of the token embeddings and positional embeddings only. The resulting vector will be the input to our model for the next step. Mathematically, for location \(i\in[d]\), which corresponds to (one of the) tokens of word \(w\) in the sequence, we have \(\text{input}_i= \begin{cases} (1-\lambda)(\text{p_embed}_i+\text{t_embed}_i)+\lambda\text{NoteDict}(w), &amp; \text{w is a rare word} \\ \text{p_embed}_i+\text{t_embed}_i, &amp;\text{otherwise} \\ \end{cases}\) where \(\text{p_embed}\) is positional embeddings, \(\text{t_embed}\) is token embeddings and \(\lambda\) (set to 0.5) is the hyperparameter specifying the weight of the notes when computing the embeddings.</p> <h2 id="results">Results</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/graphs-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/graphs-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/graphs-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/graphs.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 3. from the paper, presenting the loss and GLUE scores of the models with and without taking notes, over many iterations. </div> <p>The experiments are conducted on BERT and ELECTRA models. The loss values of the pre-training runs with <em>note taking</em> descrease significantly faster than vanilla pre-training. Moreover, the models trained while taking notes achieve higher GLUE <d-cite key="wang2018glue"></d-cite> scores much faster. Additionally, they report that after one million iterations, the GLUE score of the models pre-trained with notes are superior to their counterparts trained without notes. Finally, they report that when it took one model with note taking to reach a certain GLUE score around 100.000 training iterations, it took the model around 400.000 training iterations to reach that same score without notes. That is a 60% improvement in training time to reach the same performance!</p> <h2 id="conclusion">Conclusion</h2> <p>The ever-increasing data sizes, enlarging models, and hardware resources are some of the major factors in the current success of LLMs. However, this also means immense power consumption and carbon emission. Because pre-training of LLMs is the most computationally intensive phase of a natural language task, efficient pre-training is the concern of this paper. Knowing that the heavy-tailed distribution of word frequencies in any natural language corpora may hinder the efficiency of pre-training, improving data utilization is crucial. Therefore, the authors propose a memory extension to the transformer architecture: “Taking Notes on the Fly”. TNF holds a dictionary where each key is a rare word. The values are the historical contextual information which is updated at each time the corresponding word is encountered. The dictionary is removed from the model during the inference phase. TNF reduces the training time by 60% without any reduction in the performance.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[If your unsupervised pre-training is taking forever and you need a lightweight solution that will accelerate it, taking notes might be the method you are looking for! This method takes notes of the contextual information of the rare words and incorporates this information as a part of their embeddings on the fly! The solution is lightweight in the sense that it does not increase the inference time and it does not require an additional pass during training. The experiments demonstrate that this method reduces the pre-training time of large language models by up to 60%.]]></summary></entry><entry><title type="html">Controllable Music Generation via MIDI-DDSP</title><link href="https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/controllable-music-generation-via-midi-ddsp/" rel="alternate" type="text/html" title="Controllable Music Generation via MIDI-DDSP"/><published>2023-02-02T00:00:00+00:00</published><updated>2023-02-02T00:00:00+00:00</updated><id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/controllable-music-generation-via-midi-ddsp</id><content type="html" xml:base="https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/controllable-music-generation-via-midi-ddsp/"><![CDATA[<h2 id="historical-background-and-motivation">Historical background and Motivation</h2> <p>Early years of synthesizer: Robert Moog, an American engineer developed a synthesizer in which created and shaped sound connected by patch cords where pitch was controlled via voltage. It was popularized in the late 1960s by rock and pop acts such as the Doors, the Grateful Dead, the Rolling Stones, and the Beatles.<d-cite key="wikipedia_2023"></d-cite> During the same time, American engineer Don Bul created the Buchla Modular Electronic Music System Buchla Modular Electronic Music System in which instead of a traditional keyboard he used touchplates where depending on finger position and force voltage was transmitted. However, Moog’s Synthesizer became more accessible and marketable to musicians during 1964 and the mid-1970s. The earliest versions of synthesizers could only produce a single note at a time. Tom Oberheim, an American engineer, developed some of the early commercial polyphonic synthesizers. The first fully programmable polyphonic synthesizer, - Prophet5 was released in 1978 which used microprocessors to store sounds in patch memory. This allowed synthesisers to go from producing uncertain sounds to “a conventional set of familiar sounds.” After introduction of MIDI in 1982, synthesizer market grew dramatically<d-cite key="vail2014synthesizer"></d-cite>.</p> <h4 id="notation-used">Notation used</h4> <p>DDSP - Differentiable Digital Signal Processing <br/></p> <h2 id="midi---ddsp">MIDI - DDSP</h2> <p>While generative models are function approximator and may assist the development of samples across many domains, this expressiveness comes at the expense of interaction, since users are often limited to black-box input-output mappings without access to the network’s internals.<d-cite key="wu2021midi"></d-cite>. This makes sense as we know that having access to the latent space can be very useful for the generative models. Diffusion models lack this!<br/> In computer vision and speech there has been development in methods where users are allowed to interact througout the hierarchy of system making it optimize for realism and control. Whereas in music synthesis methods still lack this interaction in hierarchy of music generation. Recent research states that one can either generate full-band audio or have control of pitch, dynamics and timbre but not both.<d-cite key="hawthorne2018enabling"></d-cite> <d-cite key="wu2021midi"></d-cite>. Authors of MIDI-DDSP<d-cite key="wu2021midi"></d-cite> take inspiration from process of creating music and propose a generative model of music generation organised in a hierarchy for more realism and control. As traditional synthesizer use MIDI standard audio files, MIDI - DDSP translates note timing, pitch, and expression data into granular control of DDSP synthesiser modules.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/MIDI_intro-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/MIDI_intro-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/MIDI_intro-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/MIDI_intro.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <em>Figure 1: Hierarchy in music synthesis <d-cite key="engel2020ddsp"></d-cite></em> </div> </div> <p>It has 3 level hierarchy:notes, performance, synthesis as shown in above figure <br/> Notes : Similar to how composer writes series of Notes <br/> Performance : Similar to how performer articulates these notes into dynamics, and expression in music. <br/> Synthesis : Similar to how the expression are then converted to audio by short-time pitch and timbre changes of the physical vibration. <br/></p> <p>MIDI-DDSP can be viewed similarly to a multi-level autoencoder. I has 3 separately trainable modules (DDSP Inference, Synthesis Generator, Expression Generator)<br/> <strong>DDSP Inference</strong> - The DDSP Inference module learns to make predictions about synthesis parameters from audio and then applies those learnings to resynthesized audio using an audio reconstruction loss.<br/> <strong>Synthesis Generator</strong> - The Synthesis Generator module makes predictions regarding synthesis parameters based on notes and the expression qualities associated with those notes. These predictions are then iterated through the use of a reconstruction loss and an adversarial loss. <br/> <strong>Expression Generator</strong> - The Expression Generator module uses autoregressive modelling to provide predictions about note expressions based on a given note sequence which is trained via teacher forcing. <br/></p> <h3 id="but-what-is-ddsp">BUT what is DDSP!</h3> <p><strong>Challenges of neural audio synthesis and how DDSP overcomes it</strong> <br/> As shown in below Figure 1(left) shows that strided convolution models generate waveform with overlapping frames and suffer from phase alignment problem. Here phase alignment comes from recording of the same source made with 2 or mics placed at different distance. This distance variation cause the sound to arrive at mics at slightly different times. Figure 1(center) shows spectral leakage which occurs when the Fourier basis frequencies do not completely match the audio, where sinusoids at several nearby frequencies and phases need to be blended to represent a single sinusoid. Although the three waveforms on the right side of Figure 1 appear to have the same sound (a relative phase offset of the harmonics), an autoregressive model would find them to have very different losses. This represent inefficiency of model such that waveform shape does not correspond to perception.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/challenges-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/challenges-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/challenges-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/challenges.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <em>Figure 3: Challenges in neural audio synthesis <d-cite key="engel2020ddsp"></d-cite></em> </div> </div> <p>DDSP model overcomes above challenge and gain an advantage from the inductive bias of using oscillators while preserving the expressive power of neural networks and end-to-end training.</p> <d-cite key="engel2020ddsp"></d-cite> <p><strong>Why making the synthesis differentiable is important?</strong> The harmonics-plus-noise model, a differentiable additive synthesis model, generates audio in the paper. A sinusoids-plus-noise model version. The harmonics-plus-noise model is a synthesiser, but it requires specifying each harmonic’s amplitude and the filter’s frequency response. The harmonics plus-noise synthesiser accurately recreates actual instrument sounds, but its complex synthesis settings prevent direct engagement.<d-cite key="masuda2021synthesizer"></d-cite></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/ddsp.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/ddsp.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/ddsp.gif-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/ddsp.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <em> Figure 4:Decomposition of a clip of solo violin.</em> </div> </div> <p>As seen in above animation, the signals for loudness and fundamental frequency are taken from the original audio. As a result of the impacts of the room acoustics, the loudness curve does not reveal clearly differentiated note segmentations. These conditioning signals are input into the DDSP autoencoder, which then makes predictions about amplitudes, harmonic distributions, and noise magnitudes. The entire resynthesis audio is produced by applying the extracted impulse response to the synthesiser audio.</p> <p>3 main design components: <strong>Expressive</strong> - due to more params in the synthesizer (also the ability to control the generation process) <strong>Interpretable</strong> - because of the harmonic oscillator assumption i.e. relying upon fundamental frequency and loudness <strong>Adaptable</strong> - interpolation between different instruments</p> <h2 id="midi-ddsp-architecturesummary">MIDI-DDSP architecture(summary)</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/hierachy_generation-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/hierachy_generation-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/hierachy_generation-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/hierachy_generation.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <em>Figure 2: MIDI-DDSP modules <d-cite key="engel2020ddsp"></d-cite></em> </div> </div> <p>This figure gives a high level view of the MIDI-DDSP modules. We also show that full-stack automatic music generation is possible when MIDI-DDSP is combined with a pretrained note generating model.</p> <h3 id="expression-generator">Expression Generator</h3> <p>Expression Generator is mainly an autoregressive RNN that is trained to predict “expression controls” from the note sequence. These are the synthesis parameters that will be used in the next network; the synthesis generator.</p> <p>So what are these Expression Controls that we speak of, you might ask! These controls also represents few of the choices that the performer makes while performing a composed track. Following is the list of controls applied:</p> <p><em>Volume</em>: Controls how loud a note is. <em>Volume fluctuation</em>: Controls how the loudness of a note changes over the note. <em>Volume peak position</em>: Controls the location of the peak volume during the course of a note. <em>Vibrato</em>: Controls the degree of a note’s vibrato, where Vibrato is a musical effect or a technique where a note changes pitch subtly and quickly <em>Brightness</em>: increases in value produce louder high-frequency harmonics, which in turn controls the timbre of the note. <em>Attack Noise</em>: Controls the amount of noise at the note’s beginning.</p> <h3 id="synthesis-generator">Synthesis Generator</h3> <p>Synthesis Generator again is an autoregressive RNN used to predict fundamental frequency, given a conditioning sequence from the previous module. It might now sound obvious that these params are in turn used in the next module, which is the DDSP Inference.</p> <h3 id="interaction-with-ddsp-interface">Interaction with DDSP interface</h3> <p>⇒ How is this different from DDSP? CNN is utilised on a logarithmic scale. Mel-spectrograms aid models in obtaining more data from audio input, enabling more precise estimation of synthesis parameter. A fully linked network is used on fundamental frequency and loudness in our DDSP inference module. In order to extract contextual information, the output is concatenated with the CNN output and sent to the bi-directional LSTM. To map the characteristics to the synthesis settings, another fully connected layer is utilised.</p> <h2 id="experiments">Experiments</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/Results1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/Results1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/Results1-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/Results1.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <em>Comparison of reconstruction audio accuracy<d-cite key="wu2021midi"></d-cite></em> </div> </div> <p>As Shown in above figure in left there is comparison of Mel spectrogram of synthesis results and on right it shows comparison of synthesis quality from listening test. From figure(right) it is seen that MIDI-DDSP inference is perceived as likely as ground truth compared to other methods such as MIDI2Params, Ableton and FluidSynth.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/Results-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/Results-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/Results-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/Results.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Above figure shows pearson correlation between the input control and the respective output quantity. It is seen that there is strong correlation between input control and note expression output.</p> <h2 id="future-direction">Future direction</h2> <p>Author mentions in paper that one promising direction for future research is to apply this method to polyphonic recordings by means of multi-instrument transcription and multi-pitch tracking. When making differentiable it goes through many combinations and the model explore all the different possibilities and backpropagate through the soft weighting of all those possible paths, which can very soon become intractable and Reinforcement Learning could possibly be used for this search(as it is a combinatorial problem-&gt;additive synthesis).</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Recently, there has been a lot of fascinating work focusing on making music generation for a larger and more general audience. However, these models could be of great help to the artists if they can intervene into the generation process at multiple levels in order to control what notes are played and how they are performed. We hence dive deeper into MIDI-DDSP that helps with high-fidelity generations using an interpretable hierarchy with several degrees of granularity.]]></summary></entry><entry><title type="html">Adaptive Reward Penalty in Safe Reinforcement Learning</title><link href="https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/" rel="alternate" type="text/html" title="Adaptive Reward Penalty in Safe Reinforcement Learning"/><published>2023-01-31T00:00:00+00:00</published><updated>2023-01-31T00:00:00+00:00</updated><id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning</id><content type="html" xml:base="https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/"><![CDATA[<h1 id="introduction-to-safe-reinforcement-learning">Introduction to Safe Reinforcement Learning</h1> <p>Safe RL can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or the deployment processes <d-cite key="garcia_comprehensive_2015"></d-cite>.</p> <center> <video autoplay="" muted="" loop="" controls="" src="https://jocelynshen.com/deep-learning-blog.github.io/assets/video/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/RL_boat_racing.mp4" style="width:500px" type="video/mp4"> </video> <figcaption> Open AIs CoastRunners agent from their blog post <a href="https://openai.com/blog/faulty-reward-functions">"Faulty Reward Functions in the Wild"</a> in Dec 2016.</figcaption> </center> <p>Defining a reward function is crucial in <a href="https://lilianweng.github.io/posts/2018-02-19-rl-overview/">Reinforcement Learning</a> for solving many problems of interest in AI. It is often based on the designers’ intuition of the goal of the system. In the above example of CoastRunners, the goal is to reach the finish line and collect points along the way. Whilst selecting the in-game score the player earned as a reflection of the informal goal of finishing the race is a reasonable reward function, it allows for dangerous and harmful behavior, as visible in the video above. The agent can drive off the track, crash into other boats, and catch fire and still win the game whilst achieving a score on average 20 percent higher than that achieved by human players.</p> <p>How can we prevent the agents from violating safety constraints (e.g., crashing into other boats)? Recent studies have started to address the problem of safe reinforcement learning from various perspectives, ICLR works including, but not limited to:</p> <ul> <li><a href="https://openreview.net/pdf?id=HJgEMpVFwB">Adversarial Policies: Attacking Deep Reinforcement Learning</a>, Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell, <strong>ICLR 2020</strong></li> <li><a href="https://arxiv.org/pdf/2201.09802.pdf">Constrained Policy Optimization via Bayesian World Models</a>, Yarden As, Ilnura Usmanova, Sebastian Curi and Andreas Krause, <strong>ICLR 2022</strong></li> <li><a href="https://openreview.net/pdf?id=TBIzh9b5eaz">Risk-averse Offline Reinforcement Learning</a>, Núria Armengol Urpí, Sebastian Curi, and Andreas Krause, <strong>ICLR 2021</strong></li> <li><a href="https://openreview.net/pdf?id=S1vuO-bCW">Leave No Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning</a>, Benjamin Eysenbach, Shixiang Gu, Julian Ibarz, and Sergey Levine, <strong>ICLR 2018</strong></li> <li><a href="https://openreview.net/pdf?id=iaO86DUuKi">Conservative Safety Critics for Exploration</a>, Homanga Bharadhwaj, Aviral Kumar, Nicholas Rhinehart, Sergey Levine, Florian Shkurti, and Animesh Garg, <strong>ICLR 2021</strong></li> <li><a href="https://openreview.net/pdf?id=TQt98Ya7UMP">Balancing Constraints and Rewards with Meta-gradient D4PG</a>, Dan A. Calian, Daniel J. Mankowitz, Tom Zahavy, Zhongwen Xu, Junhyuk Oh, Nir Levine, and Timothy Mann, <strong>ICLR 2021</strong></li> </ul> <p>We chose to illustrate the method of Reward Constrained Policy Optimization (RCPO) <d-cite key="Tessler2018RCPO"></d-cite> in this blog post because it is a simple yet effective method of introducing the ideas of safe RL. By providing a high-level constraint, the agent learns to respect it and achieve the perfect balance between meeting that constraint and maximizing the reward. Moreover, this removes the need to manually extend and tune the reward function since it is adaptively shaped during the learning!</p> <h1 id="a-formalism-for-safe-reinforcement-learning-constrained-mdps">A Formalism for Safe Reinforcement Learning: Constrained MDPs</h1> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/CMDP_Illustration-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/CMDP_Illustration-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/CMDP_Illustration-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/CMDP_Illustration.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Illustration of a Constrained Markov Decision Process (MDP) adapted from <a href="https://lilianweng.github.io/posts/2018-02-19-rl-overview">Lilian Weng</a>. <br/> Based on an observation (also called state) from the environment, the agent selects an action. This action is executed in an environment resulting in a new state and a reward that evaluates the action. Given the new state, the feedback loop repeats. </div> <p>In Reinforcement Learning, the world is modeled as a Markov Decision Process (MDP) and the goal is to select a policy \(\pi\) which maximizes an expected cumulative reward \(J^π\).</p> <p>\(J^π\) can be taken to be the infinite horizon discounted total return as</p> \[J^\pi = \mathbb{E}_{s\sim\mu} \left[ \sum_{t=0}^T \gamma^t r(s_t, a_t) \right]\] <p>where \(\gamma\) is the discount factor, and \(r(s_t,a_t)\) is the reward function.</p> <p>However, the agents must obey safety constraints in many real-world applications while achieving the goal. We can introduce a constraint objective analogous to the reward objective. This objective is typically defined as the expected constraint value over N time steps \(J^π_C = \mathbb{E}_{s\sim\mu} \left[ C(s) \right]\). The method of aggregating individual constraints over time can vary, e.g., using the average or the maximum constraint value over N time steps or even a discounted sum.</p> <p>In the example of the robot, the aim could be to prolong the motor life of the various robots while still enabling them to perform the task at hand. Thus we constrain the robot motors from using high torque values. Here, constraint C is defined as the average torque the agent has applied to each motor, and the penalty \(c(s, a)\) becomes the average amount of torque the agent decided to use at each time step.</p> <p>We limit the allowable amount of torque applied to \(\alpha\). <br/> The constrained MDP for our safe reinforcement learning problem is:</p> \[\max_{\pi \in \Pi} J^\pi_R \text{ s.t. } J^\pi_C \leq \alpha\] <h1 id="constrained-policy-optimization">Constrained Policy Optimization</h1> <p>Constrained objectives are often solved using the Lagrange relaxation technique. With parameterized approaches such as Neural Networks, the objective is then to find the networks parameters \(\theta\) that maximize \(J^\pi_R\) subject to the constraint \(J^\pi_C \leq \alpha\) given the Lagrangian multiplier \(\lambda\):</p> \[\min_{\lambda}\max_{\theta} [J^{\pi_\theta}_R - \lambda (J^{\pi_\theta}_C - \alpha)]\] <p>We now have our new global objective function that is subject to optimization!</p> <h3 id="what-exactly-does-the-lagrangian-do">What exactly does the Lagrangian do?</h3> <p>Intuitively, the Lagrangian multiplier \(\lambda\) determines how much weight is put onto the constraint. If \(\lambda\) is set to 0, the constraint is ignored, and the objective becomes the reward objective \(J^\pi_R\). If \(\lambda\) is set very high, the constraint is enforced very strictly, and the global objective function reduces to the constraint objective \(J^π_C\). Let’s look at a simple example to <strong>demonstrate the effect of the Lagrangian multiplier \(\lambda\)</strong>. We’ll use the simple CartPole Gym environment. The reward in this environment is +1 for every step the pole was kept upright.</p> <p>We can now add an example constraint to the environment. Let’s say we want to keep the cart in the left quarter of the x-axis. We, therefore, define the constraint value as the x-position of the cart and the upper bound \(\alpha\) as -2.</p> <p>Let’s see how with different lambda values, the constraint is enforced.</p> <center> <video autoplay="" muted="" loop="" controls="" src="https://jocelynshen.com/deep-learning-blog.github.io/assets/video/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/cart_pole_lambda.mp4" style="width:500px" type="video/mp4"> </video> <figcaption> The green area represents the "safe zone", where the x-position is smaller than -2, and the red area is the "unsafe zone". <br/> The lower the lambda, the more the constraint is ignored. The higher the lambda, the more the constraint is enforced, and the main reward objective is ignored. At λ = 1,000,000 the cart shoots to the right to tilt the pole to the left but does so ignoring the following balancing act, which is observable at λ ∈ {10, 100}.</figcaption> </center> <p>Tuning the \(\lambda\) through <a href="https://gibberblot.github.io/rl-notes/single-agent/reward-shaping.html">reward shaping</a> is no easy feat. The Lagrangian is a scaling factor, i.e., if the constraint values are inherently larger than the reward values, we will need a substantially lower \(\lambda\) than when the constraint values are significantly smaller than the possible reward values. That means that the range of good lambda values is large and differs with every environment.</p> <h3 id="how-can-we-learn-an-optimal-lagrangian">How can we learn an optimal Lagrangian?</h3> <p>Luckily, it is possible to <strong>view the Lagrangian as a learnable parameter</strong> and update it through gradient descent since the globally constrained optimization objective \(J^{\pi_{\theta}}\) is differentiable. In short, we can simply use the derivative of the objective function w.r.t \(\lambda\) and update the Lagrangian.</p> \[\frac{\partial J^{\pi_{\theta}}}{\partial \lambda} = -(J^{\pi_{\theta}}_C - \alpha)\] \[\lambda \gets max(\lambda - lr_{\lambda}(-(\mathbb{E}^{\pi_\theta}_{s\sim\mu} \left[C\right] - \alpha)), 0)\] <p>Hereby \(lr_{\lambda}\) is the learning rate for the Lagrangian multiplier. The max function ensures that the Lagrangian multiplier is always positive.</p> <h1 id="reward-constrained-policy-optimization">Reward Constrained Policy Optimization</h1> <p>Actor-Critic based approaches such as <a href="https://spinningup.openai.com/en/latest/algorithms/ppo.html">PPO</a> <d-cite key="Schulman2017PPO"></d-cite> have empirically been shown to compete at the top of a plethora of quality benchmarks. In this class of algorithms, the actor learns a policy \(\pi\), whereas the critic learns the value function using temporal difference learning. Intuitively, using the critic reduced the variance and enabled training using a finite number of samples.</p> <h3 id="how-to-integrate-the-constraint-into-the-actor-critic-approach">How to integrate the constraint into the Actor-Critic approach?</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/Algorithm-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/Algorithm-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/Algorithm-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/Algorithm.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>If we look at the RCPO algorithm illustrated above, we can see that implementing the constraint into the Actor-Critic approach is done in a few lines of code. First, we need to collect the constraint during the policy rollout. Then we can integrate the constraint values (the guiding penalty) into the reward during the computation of the policy and value gradients, as demonstrated in line 7. <br/> This is done by formulating the constraint as the infinite horizon discounted total cost, similar to the usual returns of an MDP.</p> \[J^\pi_{C_\gamma}(s) \hat{=} \mathbb{E}^{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t c(s_t, a_t) | s_0 = s \right]\] <p>Now we can simply include the guiding penalty to the reward function via the Lagrange multiplier to arrive at the penalized reward function:</p> \[\hat{r} = r(s,a) - \lambda c(s,a)\] <p>Finally, we can compute the gradient of the Lagrangian in line 11 and update \(\lambda\) in line 14 as discussed in the previous section and repeat the whole process for \(K\) times.</p> <h1 id="implementation">Implementation</h1> <p>To facilitate reproducibility, we integrated RCPO into the stable-baselines3<d-cite key="stable-baselines3"></d-cite> PPO implementation.</p> <h3 id="integrating-the-guiding-penalty">Integrating the guiding penalty</h3> <p>For the computation of returns with PPO, we use the Temporal Difference Error (TD estimate) and the Generalized Advantage Estimation (GAE) advantage. <br/> To integrate the constraint into the reward function, we need to add the Lagrangian-scaled constraint value to the reward, as discussed in the RCPO section. This is done when computing the TD error estimate.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_returns_and_advantages</span><span class="p">(</span>
    <span class="n">self</span><span class="p">,</span> <span class="n">last_values</span><span class="p">:</span> <span class="n">th</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dones</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>
<span class="p">)</span>
    <span class="c1"># ...
</span>    <span class="n">delta</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rewards</span><span class="p">[</span><span class="n">step</span><span class="p">]</span>
        <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">constraint_lambda</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">constraints</span><span class="p">[</span><span class="n">step</span><span class="p">]</span>
        <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">next_values</span> <span class="o">*</span> <span class="n">next_non_terminal</span>
        <span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="n">step</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="c1"># ...
</span></code></pre></div></div> <blockquote> <p>The discussed integration of the constraint into the reward function is implemented into the computation of the advantages and returns. When the lambda parameter is set to 0, the constraint is ignored and the reward function is the same as in the original PPO implementation.</p> </blockquote> <p>Additionally, it was necessary to extend the rollout buffer to collect the constraint values at each time step. To receive the constraint values, we customized the gym environments to return those in the info dictionary.</p> <h3 id="updating-the-lagrangian-multiplier">Updating the Lagrangian multiplier</h3> <p>Due to the fact that PPO (1) collects multiple episodes until the rollout buffers are full and (2) supports vectorized environments, the logic for collecting and aggregating the constraint values across the episodes and parallel environments is a bit more complex. <br/> Nevertheless, we have chosen the aggregation method to be the average over all time steps in one complete episode and across all those episodes themselves, i.e., episodes that have reached a terminal state.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># lambda &lt;- lambda - lr_lambda * -(C - alpha) = lambda + lr_lambda * (C - alpha)
</span><span class="n">d_constraint_lambda</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">C</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">constraint_lambda</span>
<span class="n">self</span><span class="p">.</span><span class="n">rollout_buffer</span><span class="p">.</span><span class="n">constraint_lambda</span> <span class="o">+=</span> <span class="p">(</span>
    <span class="n">self</span><span class="p">.</span><span class="n">lr_constraint_lambda</span> <span class="o">*</span> <span class="n">d_constraint_lambda</span>
<span class="p">)</span>
<span class="n">self</span><span class="p">.</span><span class="n">rollout_buffer</span><span class="p">.</span><span class="n">constraint_lambda</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span>
  <span class="n">self</span><span class="p">.</span><span class="n">rollout_buffer</span><span class="p">.</span><span class="n">constraint_lambda</span><span class="p">,</span> <span class="mi">0</span>
<span class="p">)</span>
</code></pre></div></div> <blockquote> <p>After aggregating the constraint values across the episodes and parallel environments into self.C, the Lagrangian is updated using gradient descent. The max function is used to ensure that the Lagrangian multiplier is always positive.</p> </blockquote> <h1 id="experiments">Experiments</h1> <p>As a proof-of-the-principle experiment, we reproduced the HalfCheetah task in <a href="https://gymnasium.farama.org/environments/mujoco/">OpenAI MuJoCo Gym</a> from Tessler C. et al.<d-cite key="Tessler2018RCPO"></d-cite>.</p> <p>The results of the experiments are shown in the following figures. We kept (almost) all hyperparameters the same as in the original paper and let the agents train for \(1,000,000\) time steps.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/experiments_results_smooth_constraints-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/experiments_results_smooth_constraints-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/experiments_results_smooth_constraints-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/experiments_results_smooth_constraints.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Rewards and average torque of the experiments on the HalfCheetah environment. The x-axis represents the time steps and the maximum torque constraint is illustrated by the dashed line. </div> <p>The results demonstrate that the RCPPO trained an agent that successfully walked forward while respecting the safety constraint. We achieved comparable results to the original experiments in the paper. <br/> Interestingly, low \(\lambda\) values seem to be less stable than higher \(\lambda\) values. The guiding penalty appears to enforce the constraint and improve the learning process overall. They limit the amount of torque the agent is allowed to apply, hinder the exploration of unsafe and poor-performing local minima and guide the policy to a safe and more optimal solution. <br/> Nevertheless, the poor performance of the unconstrained agents may be due to the neural network architecture being relatively small (i.e., 2 layers of 64 hidden units).</p> <h3 id="qualitative-observations">Qualitative observations</h3> <p>Finally ,let’s see how our HalfCheetah agents walk under the To do so, we have recorded videos of the agents walking forward with different \(\lambda\) values. The results can be seen below.</p> <center> <video autoplay="" muted="" loop="" controls="" src="https://jocelynshen.com/deep-learning-blog.github.io/assets/video/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/HalfCheetah_Experiments.mp4" style="width:500px" type="video/mp4"> </video> <figcaption>Visualization of the HalfCheetah agents learned through RCPPO and with different selected Lagrangian multipliers.</figcaption> </center> <p>We can again observe that the lower the lambda is, the more the constraint is ignored and the higher the lambda, the more the constraint is enforced and the main reward objective is ignored.<br/> At λ ∈ {10, 100}, the robot applies 0 torque to ultimately oblige to the constraint ignoring the main reward objective to walk forward, which is observable at λ ∈ {RCPPO, 0, 0.00001}. With λ ∈ {0, 0.00001} the robot can walk forward, but it is visible that it moves its legs much quicker and more aggressively than the RCPPO agent. Furthermore, the RCPPO agent walks perfectly, whilst the other (moving) agents tumble over their own hecktick steps.</p> <h1 id="discussion">Discussion</h1> <h3 id="theoretical-assumptions-vs-empirical-results">Theoretical assumptions vs. empirical results</h3> <p>We had to select higher values for the Lagrangian multiplier than what were used in the original paper. In the paper, a \(\lambda\) value of 0.1 is already very high as it leads to a reward of \(-0.4\) and torque of \(0.1387\), whereas in our case a \(\lambda\) value of \(1.0\) leads to a reward of about \(1 500\) with an average torque of \(0.39\). <br/> This affected the reward shaping process but also meant we had to increase the Lagrangian’s respective learning rate when training it as a parameter to grow quicker. As a result, \(lr_{\lambda}\) becomes larger than \(lr_{\pi}\), which <strong>ignores one of the assumptions made in the paper</strong>, yet leads to coherent results.</p> <p>A possible reason for the slower and weaker impact of the constraint could be attributed to the clipping of the trust region. This technique ensures that the policy does not change too much between updates and prevents it from landing in a bad local minimum that it can not escape. This is done by clipping the policy update to a specific range. Therefore, even with “high” values of lambda w.r.t. the original paper, the policy will not change significantly to conform to the constraint.</p> <p>Not only did we have to select a higher learning rate for the Lagrangian, but we also did not include different learning rates for the policy and the value function, <strong>ignoring the three times scales approach</strong> proposed in the original paper. Additionally, in the original paper the RCPPO algorithm updated their networks (actor and critic) after each episode. In our implementation, we need to fill the rollout buffer with potentially multiple episodes, thus reducing the frequency of network parameters and Lagrangian updates. Nevertheless, the PPO algorithm implements a parameter update loop of n epochs after each rollout, which to a degree counteracts the discussed lower update frequency of all parameters.</p> <h3 id="conclusion">Conclusion</h3> <p>The results of the experiments show that the RCPO approach can learn a policy that can optimize the main reward objective while respecting the constraint.</p> <p>Safe Reinforcement Learning is a critical area of research in the field of artificial intelligence, as it has the potential to shape the future of autonomous systems in a multitude of domains, ranging from robotics to finance. <br/> The more complex systems become, the more difficult it is to ensure safety requirements, especially through simple reward shaping. An approach such as RCPO can ensure that the safety constraints are respected while enforcing them by only providing the constraint itself.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[In this blog, we dive into the ICLR 2019 paper Reward Constrained Policy Optimization (RCPO) by Tessler et al. and highlight the importance of adaptive reward shaping in safe reinforcement learning. We reproduce the paper's experimental results by implementing RCPO into Proximal Policy Optimization (PPO). This blog aims to provide researchers and practitioners with (1) a better understanding of safe reinforcement learning in terms of constrained optimization and (2) how penalized reward functions can be effectively used to train a robust policy.]]></summary></entry><entry><title type="html">Underfitting and Regularization: Finding the Right Balance</title><link href="https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/Underfitting-and-Regularization-Finding-the-Right-Balance/" rel="alternate" type="text/html" title="Underfitting and Regularization: Finding the Right Balance"/><published>2023-01-15T00:00:00+00:00</published><updated>2023-01-15T00:00:00+00:00</updated><id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/Underfitting-and-Regularization-Finding-the-Right-Balance</id><content type="html" xml:base="https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/Underfitting-and-Regularization-Finding-the-Right-Balance/"><![CDATA[<h2 id="goal-of-this-blog-post">Goal of this blog post</h2> <p>Network Augmentation aka NetAug <d-cite key="DBLP:conf/iclr/CaiG0022"></d-cite> caters to training small neural network architectures like MobileNetV2-Tiny for the best top-k percent accuracy. The paper argues that training small neural networks technically differs from that of large neural networks because the former is prone to underfitting. NetAug is contrary to traditional methods like dropout<d-cite key="10.5555/2627435.2670313"></d-cite>, network pruning<d-cite key="9043731"></d-cite>, quantization<d-cite key="jacob2018quantization"></d-cite>, data augmentation<d-cite key="https://doi.org/10.48550/arxiv.1712.04621"></d-cite> and other regularization techniques<d-cite key="10.1007/s10462-019-09784-7"></d-cite> NetAug can be viewed as reversed form of dropout, as we enlarge the target model during the training phase instead of shrinking it. In this blog post, we identify some pitfalls with NetAug and propose potential workarounds.</p> <h2 id="background">Background</h2> <p>NetAug solely focuses on improving the performance of the tiny neural networks during inference, whilst optimizing their memory footprint to deploy them on edge devices. Tiny neural networks are usually inclined to underfit. Hence, the traditional training paradigms will not work for these small models because they fundamentally tackle the problem of overfitting and not overcome the underfitting issue. Several techniques in the recent time like data augmentation, pruning, dropout, knowledge distillation have been proposed to improve the generalizability of neural networks.</p> <ol> <li> <dl> <dt><strong>Knowledge Distillation</strong><d-cite key="hinton2015distilling"></d-cite></dt> <dd>It is quite difficult to deploy and maintain an ensemble. However, previous research has shown that it is possible to learn a single cumbersome model that has the same performance as an ensemble. In most knowledge distillation methods there exist a large teacher model that transfers its knowledge as a learned mapping via training to a small student model with the teacher models output. There exists several techniques like self-distillation in which the teacher model trains itself continuously. Convectional KD methods try to optimise the objective function such that the loss function penalizes the difference between the student and teacher model.</dd> </dl> </li> <li> <dl> <dt><strong>Regularization</strong><d-cite key="10.1007/s10462-019-09784-7"></d-cite></dt> <dd>Regularization is used to prevent overfitting of any ML model by reducing the variance, penalizing the model coefficients and complexity of the model. Regularization techniques are mainly composed of data augmentation one of the most simplest and conveninet ways to expand the size of the dataset such that it prevents overfitting issues that occur with a relatively small dataset. Dropout is also popularly applied while training models, in which at every iteration incoming and outgoing connections between certain nodes are randomly dropped based on a particular probability and the remaining neural network is trained normally.</dd> </dl> </li> <li> <dl> <dt><strong>Tiny Deep learning</strong><d-cite key="lin2020mcunet"></d-cite>,<d-cite key="lin2021mcunetv2"></d-cite>,<d-cite key="lin2022ondevice"></d-cite></dt> <dd>Several challenges are paved while transitioning from conventional high end ML systems to low level clients, maintaining the accuracy of learning models, provide train-to-deploy facility in resource economical tiny edge devices, optimizing processing capacity. This method includes AutoML procedures for designing automatic techniques for architecturing apt neural netowrks for a given target hardware platform includes customised fast trained models,auto channel pruning methods and auto mixed precision quantization. The other approaches like AutoAugment methods automatically searches for improvised data augmentation within the network to prevent overfitting. There exists network slimming methods to reduce model size, decrease the run-time memory footprint and computing resource.</dd> </dl> </li> <li> <dl> <dt><strong>Network Augmentation</strong><d-cite key="DBLP:conf/iclr/CaiG0022"></d-cite></dt> <dd>This method was proposed to solve the problem of underfitting in tiny neural networks. This is done by augmenting the given model (referred to as base model) into a larger model and encourage it to work as a sub-model to get extra supervision in additoin to functioning independently. This will help in increasing the representation power of the base model because of the gradient flow from the larger model and it can be viewed equivalently as “<strong>reverse-dropout</strong>”.</dd> </dl> </li> <li> <dl> <dt><strong>Neural Architechture Search (NAS)</strong><d-cite key="https://doi.org/10.48550/arxiv.1808.05377"></d-cite></dt> <dd>This method was proposed to solve the problem of architecture optimization and weight optimization based on the underlying training data. NAS usually involves defining an architectural search space and then searching for the best architecture based on the performance on the validation set. Recent approaches include weight sharing, nested optimization and joint optimization during training. However, there are drawbacks in using these approaches because they are computationally expensive and suffer from coupling between architecture parameters and model weights. This will degrade the performance of the inherited weights.</dd> </dl> </li> </ol> <h2 id="formulation-of-netaug">Formulation of NetAug</h2> <p>The end goal of any ML model is to be able to minimize the loss function with the help of gradient descent. Since tiny neural network have a very small capacity the gradient descent is likely to get stuck in local minima. Instead of traditional regularization techniques which add noise to data and model, NetAug proposes a way to increase the capacity of the tiny model without changing its architecture for efficient deployment and inference on edge devices. This is done by augmenting the tiny model (referred to as base model) into a larger model and jointly training both the base model independently and also the augmented model so that the base model benefits from the extra supervision it receives from the augmented model. However, during inference only the base model is used.</p> <p>To speed-up the training, a single largest augmented model is constructed by augmenting the width of the each layer of the base model using an augmentation factor \(r\). After building the largest augmented model, we construct other augmented models by selecting a subset of channels from the largest augmented model. NetAug proposes a hyper-parameter \(s\), named diversity factor, to control the number of augmented model configurations. We set the augmented widths to be linearly spaced between \(w\) and \(r \times w\). For instance, with \(r = 3\) and \(s = 2\), the possible widths would be \([w, 2w, 3w]\).</p> <h2 id="pitfalls-in-netaug">Pitfalls in NetAug</h2> <ol> <li> <dl> <dt><strong id="training_aug_models">Training the Generated Augmented Models</strong></dt> <dd>NetAug randomly samples sub-models from the largest model by augmenting the width instead of depth, its highly important to ensure we speed up the training time and reduce the number of traning iterations for these generated sub-models thereby enhancing the training convergence and reaching optimization faster. We theoretically aim at aiding this by introducing a re-parametrisation technique during training that involves sharing and unsharing of weights to attain convergence much faster. </dd> </dl> </li> <li> <dl> <dt><strong id="naive_loss">Naive Loss Function</strong></dt> <dd>NetAug computes loss in a very trivial form, i.e, by simply performing a weighted sum over the loss of the base model with that from the respective sampled augmented models. However, it was mentioned in the paper that sampling more than one sub-models from the largest augmented model in each training step is resulting in degradation of the base model’s performance. This can be attributed to the fact that simple weighted sum of losses from the base supervision and auxiliary supervision is causing the auxiliary supervision to shadow the base model. We propose different mixing strategies to circumvent this problem. </dd> </dl> </li> <li> <dl> <dt><strong id="generating_aug_model">Generating the Largest Augmented Model</strong></dt> <dd>For a particular network, rather than tuning for just a single network hyperparameter (i.e., network, depth, width etc.), what if we instead tune all the closely relevant network hyperparameters for every augmented sub-model? To advocate this it’s sensible to compare the entire distribution of hyperparameter across the model. This can be tackled using NAS to find the best largest augmented model and then use it for auxiliary supervision of the base model.</dd> </dl> </li> </ol> <h2 id="introducing-netaug-with-relation-knowledge-distribution-rkd">Introducing NetAug with Relation Knowledge Distribution (RKD)</h2> <p>Knowledge distillation in learned models is constituted of:</p> <ol> <li> <dl> <dt><strong>Individual Knowledge Distillation</strong></dt> <dd>Outputs of individual samples represented by the teacher and student are matched.</dd> </dl> </li> <li> <dl> <dt><strong>Relational Knowledge Distillation</strong></dt> <dd>Relation among examples represented by the teacher and student are matched. RKD is a generalization of convectional knowledge distillation that combines with NetAug to boost the performance due to its complementarity with conventional KD, that aims at transferring structural knowledge using mutual relations of data examples in the teacher’s output presentation rather than individual output themselves. Contrary to conventional approaches called as Individual KD (IKD) that transfers individual outputs of the teacher model \(f_T(\cdot)\) to the student model \(f_S(\cdot)\) point-wise, RKD transfers relations of the outputs structure-wise and computes a relational potential \(\psi\) for every \(n\)-tuple of data instance and transfers the relevant information through the potential from the teacher to the student models. In addition to knowledge represented in the output layers and the intermediate layers of a neural network, knowledge that captures the relationship between feature maps are also used to train a student model.</dd> </dl> </li> </ol> <p>We specifically aim at training the teacher and student model in a online setting, in this online type of distillation training method both the teacher and the student model are trained together simultaneously.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/rkd-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/rkd-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/rkd-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/rkd.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Relational Knowledge distillation can be expressed as</strong> -<br/> Given a teacher model \(\:T\:\) and a student model \(S\), we denote \(f_T(\cdot)\) and \(f_S(\cdot)\) as the functions of the teacher and the student, respectively, and \(\psi\) as a function extracting the relation, we have</p> \[\begin{equation} \mathcal{L}_{\text{RKD}} = \sum \limits_{\{x_1, \ldots, x_n\} \in \chi^N} l \big(\psi(t_1,\cdots,t_n), \: \psi(s_1, \cdots, s_n)\big) \end{equation}\] <p>where \(\mathcal{L}_{\text{RKD}}\) is the loss function, \(t_i = f_T(x_i)\) and \(s_i = f_S(x_i)\) and \(x_i \in \chi\) denotes the input data.</p> <h3 id="loss-functions-in-rkd">Loss Functions in RKD</h3> <ol> <li><strong>Distance-wise distillation loss (pair)</strong></li> </ol> <div class="small_img row mt-1"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-D-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-D-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-D-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-D.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This method is known as RKD-D. It transfers relative distance between points on embedding space. Mathematically, \(\begin{equation}\psi_{D}(t_i, t_j) = \frac{1}{\mu}\big\| t_i - t_j\big\|_2\end{equation}\) where \(\psi_d(\cdot, \cdot)\) denotes distance wise potential function \(\begin{equation}\mu = \frac{1}{|\chi^2|}\sum\limits_{(x_i, x_j) \in \chi^2} \big\| t_i - t_j\big\|_2\end{equation}\) \(\begin{equation}\boxed{\mathcal{L}_{\text{RKD-D}} = \sum \limits_{(x_i, x_j) \in \chi^2} l_\delta \big(\psi_D(t_i, t_j), \psi_D(s_i, s_j)\big)}\end{equation}\) where \(l_{\delta}\) denotes the Huber Los</p> \[\begin{equation}l_\delta(x, y) = \begin{cases} \frac{1}{2} (x-y)^2\:\:\:\: \text{for } |x-y| \leq 1 \\ |x - y| - \frac{1}{2}\:\:\: \text{otherwise.} \end{cases}\end{equation}\] <ol> <li><strong>Angle-wise distillation loss (triplet)</strong></li> </ol> <div class="small_img row mt-1"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-A-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-A-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-A-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-A.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This method is known as RKD-A. RKD-A transfers angle formed by three points on embedding space. Mathematically, \(\begin{equation}\psi_{A}(t_i, t_j, t_k) = \cos \angle t_it_jt_k = \langle \boldsymbol{e}^{ij}, \boldsymbol{e}{jk}\rangle\end{equation}\) where \(\psi_A(\cdot, \cdot, \cdot)\) denotes angle wise potential function \(\begin{equation}\boldsymbol{e}^{ij} = \frac{t_i - t_j}{\big\|t_i - t_j\big\|_2}, \: \boldsymbol{e}^{jk} = \frac{t_k - t_j}{\big\|t_k - t_j\big\|_2}\end{equation}\)</p> \[\begin{equation}\boxed{\mathcal{L}_{\text{RKD-A}} = \sum\limits_{(x_i, x_j, x_k) \in \chi^3} l_\delta \big(\psi_A(t_i, t_j, t_k), \psi_A(s_i, s_j, s_k)\big)}\end{equation}\] <h3 id="combining-rkd-with-netaug">Combining RKD with NetAug</h3> <p>We propose the following loss function to solve <a href="#naive_loss">naive loss problem of NetAug</a></p> \[\begin{equation}\mathcal{L}_{\text{aug}} = \underbrace{\mathcal{L}(W_t)}_{\text{base supervision}} \:+\: \underbrace{\alpha_1 \mathcal{L}([W_t, W_1]) + \cdots + \alpha_i \mathcal{L}([W_t, W_i]) + \cdots}_{\text{auxiliary supervision, working as a sub-model of augmented models}} \:+\: \lambda_{\text{KD}}\,\underbrace{\mathbf{\mathcal{L}_{\text{RKD}}}}_{\text{relational knowledge distillation}}\label{eqn:loss_func}\end{equation}\] <p>where \([W_t, W_i]\) represents an augmented model where \([W_t]\) represents the tiny neural network and \([W_i]\) contains weight of the sub-model sampled from the largest augmented model, \(\alpha\) is scaling hyper-parameter for combining loss from different augmented models and finally \(\lambda_{\text{KD}}\) is a tunable hyperparameter to balance RKD and NetAug.</p> <h3 id="netaug-training-with-rkd">NetAug Training with RKD</h3> <p>In NetAug, they train only one model for every epoch, training all the augmented models all once is not only computationally expensive but also impacts the performance. The proportion of the base supervision will decrease when we sample more augmented networks, which will make the training process biased toward augmented networks and shadows the base model.</p> <p>To further enhance the auxiliary supervision, we propose to use RKD in an online setting i.e., the largest augmented model will act as a teacher and the base model will act as a student. Both the teacher and the student are trained simultaneously.</p> <p>We train the both the base model and the augmented model via gradient descent based on the loss function \(\eqref{eqn:loss_func}\). The gradient update for the base model is then given by</p> \[\begin{equation}W^{n+1}_t = W^n_t - \eta \bigg(\frac{\partial \mathcal{L}(W^n_t)}{\partial W^n_t} + \alpha \frac{\partial \mathcal{L}([W^n_t, W^n_i])}{\partial W^n_t} + \lambda \frac{\partial \mathcal{L}_{\text{RKD}}([W^n_t, W^n_l])}{\partial W^n_t}\bigg)\end{equation}\] <p>Similar update equations can be obtained for the largest augmented model and the sub-models as well.</p> <h2 id="fasten-auxillary-model-training">Fasten Auxillary Model Training</h2> <p>We propose this method to solve the problem <a href="#training_aug_models">training the generated augmented models</a>. Based on <d-cite key="yang2021speeding"></d-cite> we propose to speed up the training process for the augmented sub models such that it can attain faster convergence and reduce the number of training iterations thereby obtain a better performance. In this technique, in the early phase of training, the neural network is trained with weights shared across all the layers of the model, to learn the commonly shared component across weights of different layers, and towards the later phase of training we un-share weights and continue training until convergence. Weight sharing for initial training steps will contrain the model complexity effectively. It brings the weights closer to the optimal value, which provides a better initialization for subsequent training steps and improved model generalization.</p> <p><strong>Mathematical Formulation</strong></p> <p>Denote the neural model as consisting of \(L\) stacked structurally similar modules as \(\mathcal{M} = \{\mathcal{M}_i\}, \, i=1,\cdots, L\) and \(w = \{w_i\}, \, i=1,\cdots, L\) denote the corresponding weights. These weights are re-parametrized as</p> \[\begin{equation}w_i = \frac{1}{\sqrt{L}}w_0 + \tilde{w}_i, \:\:\:\: i=1,\cdots,L\end{equation}\] <p>Here \(w_0\) represents the shared weights across all modules and is referred to as <strong>stem-direction</strong> and \(\tilde{w}_i\) represents the unshared weights across all modules and is referred to as <strong>branch-directions</strong>.</p> <p><strong>Training Strategy</strong></p> <p>Denote \(T\) as the number of training steps, \(\eta\) as the step size and \(\alpha \in (0, 1)\) is a tunable hyper-paramter indicating the fraction of weight sharing steps. Then we train \(\mathcal{M}\) as follows:</p> <ul> <li><strong>Sharing weights in early stage:</strong> For the first \(\tau = \alpha \cdot T\) steps, we update the shared weights \(w_0\) alone with gradient \(g_0\)</li> <li><strong>Unsharing weights in later stage:</strong> For the next \(t \geq \alpha \cdot T\), we update only the unshared weights \(\tilde{w}_i\) with gradient \(\tilde{g}_i\)</li> </ul> <p>The effective gradient updates for \(w_i\) can be found using chain rule as follows:</p> \[\begin{equation}g_0 = \frac{\partial \mathcal{L}}{\partial w_0} = \sum\limits_{i=1}^L \frac{\partial \mathcal{L}}{w_i}\,\frac{\partial w_i}{\partial w_0} = \frac{1}{\sqrt{L}}\sum\limits_{i=1}^L g_i \end{equation}\] \[\begin{equation}\tilde{g}_i = \frac{\partial \mathcal{L}}{\partial \tilde{w}_i} = \frac{\partial \mathcal{L}}{\partial w_i}\,\frac{\partial w_i}{\partial \tilde{w}_i} = g_i\end{equation}\] <p>where \(g_i\) denotes the gradients of \(w_i\) and \(\mathcal{L}\) denotes the loss function.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/NetAug_SWE-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/NetAug_SWE-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/NetAug_SWE-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/NetAug_SWE.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="generating-largest-augmented-model-via-single-path-one-shot-nas">Generating largest augmented model via Single Path One-Shot NAS</h2> <p>We propose this method to solve the problem of <a href="#generating_aug_model">generating the largest augmented model</a>. In NetAug, the largest augment model is generated randomly just based on the hyperparameters \(r\) and \(s\). Single Path One-Shot NAS with uniform sampling <d-cite key="guo2020single"></d-cite> revists the pitfalls of weight coupling in previous weight sharing methods. The one-shot paradigm is made attractive for real world tasks and better generalization. It is hyperparameter free, single path strategy works well because it can decouple the weights of different operations. This implementation can be more efficient in multiple search space. Using this technique we generate largest optimized augmented model by</p> <ol> <li> <p><strong>Supernet weight Optimization</strong> : \(\begin{equation}W_{\mathcal{A}}=\mathop{\arg \min}_{W} \: \mathcal{L}_{\text{train}}\big(\mathcal{N}(\mathcal{A},W)\big)\end{equation}\)</p> <p>The \(\mathcal{A}\) is the architecture search space represented as a directed acyclic graph which is encoded as a supernet \(\mathcal{N}(\mathcal{A}, W)\). During an SGD step in the above equation, each edge in the supernet graph is randomly dropped, using a dropout rate parameter. In this way, the co-adaptation of the node weights is reduced during training making the supernet training easier.</p> </li> <li> <p><strong>Architecture Search Optimization</strong> : \(\begin{equation}a^* = \mathop{\arg \max}_{a \in \mathcal{A}} \text{ ACC}_{\text{val}}\bigg(\mathcal{N}\big(a,W_{\mathcal{A}}(a)\big)\bigg)\end{equation}\)</p> <p>During search, each sampled architecture a inherits its weights from \(W_{\mathcal{A}}\) as \(W_{\mathcal{A}}(a)\). The architecture weights are ready to use making the search very efficient and flexible. This type of sequential optimization works because, the accuracy of any architecture \(a\) on a validation set using inherited weight \(W_{\mathcal{A}}(a)\) (without extra fine tuning) is highly predictive for the accuracy of \(a\) that is fully trained. Hence we try to minimize the training loss even further for better performance, supernet weights \(W_{\mathcal{A}}\) such that all the architectures in the search space are optimized simultaneously.</p> </li> </ol> \[\begin{equation}W_{\mathcal{A}} = \mathop{\arg \min}_{W} \: \mathbb{E}_{a \sim \Gamma(\mathcal{A})}\bigg[\mathcal{L}_{\text{train}}\big(\mathcal{N}(a, W(a))\big)\bigg]\end{equation}\] <p>where \(\Gamma(\mathcal{A})\) is the prior distribution of \(a \in \mathcal{A}\). This stochastic training of the supernet is helpful in better generalization of the optimized model and is also computationally efficient. To overcome the problem of weight coupling, the supernet \(\mathcal{N}(\mathcal{A},W)\) is chosen such that each architecture is a single path so that this realization is hyperparameter free as compared to traditional NAS approaches. The distribution \(\Gamma(\mathcal{A})\) is fixed apriori as a uniform distribution during our training and is not a learnable parameter.</p> <h2 id="evaluation-and-inference">Evaluation and Inference</h2> <p>Evaluation metric is the core for building any accurate machine learning model. We propose to implement the evaluation metrics precision@\(k\) ,recall@\(k\) and f1-score@\(k\) for the augmented sub-models sampled from largest augmented model and for the base model itself, where \(k\) represents the top k accuracy on the test set. These metrics will assist in evaluating the training procedure better than vanilla accuracy because we are trying to tackle the problem of underfitting, and an underfitted model is more likely to make the same prediction for every input and presence of class imbalance will lead to erroneous results.</p> <h2 id="conclusion">Conclusion</h2> <p>The main conclusion of this blog post is to further refine tiny neural networks effectively without any loss in accuracy and prevent underfitting in them. The paper implements this in a unique way apart from the conventional techniques such as regularization, dropout and data augmentation. This blog adds to other techniques that are orthogonal to NetAug can be used combined with NetAug for improvised results.</p> ]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[In this blog post, we will go over the ICLR 2022 paper titled NETWORK AUGMENTATION FOR TINY DEEP LEARNING. This paper introduces a new training method for improving the performance of tiny neural networks. NetAug augments the network (reverse dropout), it puts the tiny model into larger models and encourages it to work as a sub-model of larger models to get extra supervision.]]></summary></entry><entry><title type="html">Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-agent Reinforcement Learning</title><link href="https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/riit/" rel="alternate" type="text/html" title="Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-agent Reinforcement Learning"/><published>2022-12-13T00:00:00+00:00</published><updated>2022-12-13T00:00:00+00:00</updated><id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/riit</id><content type="html" xml:base="https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/riit/"><![CDATA[<blockquote> <p>QMIX [<a href="#8">8</a>], a very classical multi-agent reinforcement learning (MARL) algorithm, is often considered to be a weak performance baseline due to its representation capability limitations. However, we found that by improving the implementation techniques of QMIX we can enable it to achieve state-of-the-art on the StarCraft Multi-Agent Challenge (SMAC) testbed [<a href="#10">10</a>]. Specifically, we evaluated the effect of optimizer, number of rollout processes, replay buffer size, eligibility traces, hidden size and the degree of annealed exploration on QMIX, and tried to explain their role in MARL. Furthermore, the key factor of the monotonicity constraint of QMIX was found in this post, we tried to explain its role and corroborated its superior performance by combining it with another actor-critic style algorithm. We have open-sourced the code at <a href="https://github.com/hijkzzz/pymarl2"> https://github.com/hijkzzz/pymarl2 </a> for researchers to evaluate the effects of these proposed techniques. We hope this research will advance the MARL community and contribute to the establishment of new baselines of QMIX.</p> </blockquote> <h2 id="background"><a name="Background">Background</a></h2> <h3 id="from-rl-to-marl"><a name="From_RL_to_MARL">From RL to MARL</a></h3> <p>Since AlphaZero beats humans at Go, RL has become a consistent hot spot in academia and industry. The agent of RL can obtain some rewards by interacting with the environment and taking actions to maximize these cumulative rewards. Actually, almost all the RL problems can be described as <strong>Markov Decision Processes</strong> as illustrated in Figure <a href="#mdp">1</a>.</p> <div id="mdp" class="img-height-200 img-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/mdp-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/mdp-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/mdp-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/mdp.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 1: The agent-environment interaction in a Markov decision process. (Image source: Sec. 3.1 Sutton &amp; Barto (2017) <a ref="#14">[14]</a>)). $R_t, S_t, A_t$ denote the reward, state and action at timestep $t$.</center> <p><br/></p> <p>Just as its name implies, MARL contains multiple agents trained by RL algorithms in the same environment. Many complex multi-agent systems such as robot swarm control, autonomous vehicle coordination, and sensor networks, can be modeled as MARL tasks. The interaction of these agents would make them work together to achieve a common goal.</p> <div style="display:flex; margin-bottom:-30px; margin-left :150px; margin-right :150px"> <div id="chase" class="img-height-100"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/chase.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/chase.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/chase.gif-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/chase.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="magent" class="img-height-100"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/magent.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/magent.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/magent.gif-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/magent.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></div> <div style="display:flex; margin-top:-30px; margin-left :50px; margin-right :50px"> <div id="hide" class="img-height-200"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/hide.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/hide.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/hide.gif-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/hide.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="smac" class="img-height-200"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/smac.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/smac.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/smac.gif-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/smac.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></div> <div style="margin-bottom: 20px"><center>Figure 2: Some multi-agent cooperative scenarios [from-left-to-right]. <a href="https://github.com/openai/multiagent-particle-envs"> <br/> (a) Chasing in Multi-Agent Particle Environment (Predator-Prey); </a> <a href="https://github.com/geek-ai/MAgent"> (b) MAgent Environment; </a> <a href="https://openai.com/blog/emergent-tool-use"> <br/> (c) Hide &amp; Seek; </a> <a href="https://github.com/oxwhirl/smac"> (d) StarCraft Multi-Agent Challenge. </a></center></div> <p>In this general setting, agents usually have a limited sight range to observe their surrounding environment. As shown in Figure <a href="#smac_obs">3</a>, the cyan border indicates the sight and shooting range of the agent, which means the agent could only obtain the information of terrain or other agents in that range. This restricted field of view may also result in the difficulty of agents to access to global state information, making its policy updates subject to bias and unsatisfactory performance. In general, these kinds of multi-agent scenarios can be modeled as <strong>Decentralized Partially Observable Markov Decision Processes</strong> (Dec-POMDP) [<a href="#6">6</a>].</p> <p>Even though many RL algorithms [<a href="#14">14</a>] and their variants have been successfully extended to the cooperative scenarios in MARL setting, few of their performance is satisfactory. One of the most troublesome issues is <em>Non-Stationarity</em>. Specifically, as a part of the environment, the changing policies of other agents during training would make the observation non-stationary from the perspective of any individual agent [<a href="#28">28</a>] and significantly slow down the policy optimization of MARL. This situation has forced researchers to seek a method that can exploit global information during training but does not destroy the ability of the agents to only use their respective observations during execution, to find a joint policy $\boldsymbol{\pi} = \langle \pi^{1},…,\pi^{n}\rangle$ to maximize global reward. Naturally, the simplicity and effectiveness of the <strong>Centralized Training with Decentralized Execution</strong> (CTDE) paradigm have attracted the attention of the community, and many MARL algorithms based on CTDE were proposed, making a remarkable contribution to MARL.</p> <p>In the rest of this section, we briefly introduce Dec-POMDP and CTDE to facilitate the understanding of the contents of MARL, the QMIX algorithm and the following text.</p> <div style="float:left; margin-left :150px; margin-right :150px;"><div name="smac_obs" class="img-height-100"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/smac_agent_obs-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/smac_agent_obs-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/smac_agent_obs-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/smac_agent_obs.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 3: The partial observation of agents<br/>(Image source: SMAC <a ref="#10">[10]</a>). </center><br/></div> <h3 id="decentralized-partially-observable-markov-decision-process"><a name="Decentralized_Partially_Observable_Markov_Decision_Process">Decentralized Partially Observable Markov Decision Process</a></h3> <p>A <strong>Decentralized Partially Observable Markov Decision Process</strong> (Dec-POMDP) model, as described in [<a href="#8">8</a>][<a href="#28">28</a>], is typically used to represent a full cooperative multi-agent task. The model consists of a tuple denoted by $G=(S, U, P, r, Z, O, n, \gamma)$, and involves $n$ agents, where $n$ is an integer between 1 and $n$, inclusive. The true state of the environment, denoted by $s \in S$, describes global information that is relevant to both agents and other auxiliary features. At each timestep $t$, a transition in the environment occurs via a joint action $\mathbf{u} \in \mathbf{U} \equiv U^{n}$, which is composed of an action $u^i \in U$, chosen by each agent. This transition is driven by the state transition function $P\left(s^{\prime} \mid s, \mathbf{u}\right): S \times \mathbf{U} \times S \rightarrow[0,1]$. Additionally, there is a shared global reward function, denoted by $r(s, \mathbf{u}): S \times \mathbf{U} \rightarrow \mathbf{R}$, which is optimized by the whole team. Finally, each agent has a partial observation described by $o^i \in O$, which is derived from the observation function $Z(o^i \mid s, u^i) : S \times U \rightarrow O$. All agents work cooperatively to maximize the shared global reward $R_{t}=\sum_{k=0}^{T} \gamma^{k} r_{t+k}$, which is described by the joint value function \(Q^{\boldsymbol{\pi}}\left(s_{t}, \mathbf{u}_{t}\right) = \mathbb{E}_{s_{t+1: \infty}, \mathbf{u}_{t+1: \infty}}\left[R_{t} \mid s_{t}, \mathbf{u}_{t}\right]\).</p> <h3 id="centralized-training-with-decentralized-execution-and-value-decomposition"><a name="Centralized_Training_with_Decentralized_Execution_and_Value_Decomposition">Centralized Training with Decentralized Execution and Value Decomposition</a></h3> <p>To better explore the factors affecting the QMIX algorithm, our focus lies in the <strong>Centralized Training with Decentralized Execution</strong> (CTDE) paradigm of MARL algorithms. These algorithms under this paradigm have access to the true state $s$ and the action-observation histories $\tau^{i}$ of all agents to centrally train policies, but each agent can only rely on its local observation $o^{i}$ for decision-making. Some value-based algorithms implemented under CTDE follow the Individual-Global-Max (<strong>IGM</strong>) principle [<a href="#11">11</a>], ensuring consistency between the joint action-value function $Q_{tot} \left(\boldsymbol{\tau}, \mathbf{u}\right)$ and individual agent-utilities $[Q_i\left(\tau^i, u^i\right)] _{i=1} ^{n}$:</p> \[\underset{\mathbf{u}}{\operatorname{argmax}}\ Q_{tot} \left(\boldsymbol{\tau}, \mathbf{u}\right) = (\underset{u^{1}}{\operatorname{argmax}}\ Q_{1} \left(\tau^{1}, u^{1}\right), \ldots, \underset{u^{n}}{\operatorname{argmax}}\ Q_{n} \left(\tau^{n} , u^{n}\right)). \tag{1} \label{eq1}\] <p>One of the most typical ways to efficiently train the joint value function \(Q_{tot} \left(\boldsymbol{\tau}, \mathbf{u}\right)\) is to decompose it into the utility functions \([Q_i\left(\tau^i, u^i\right)] _{i=1} ^{n}\) and maintain updating consistency between them via IGM. The simplest factorization structure, called <em>additivity</em>, has been proposed by VDN [<a href="#13">13</a>], which makes VDN simply factorize $Q_{tot}$ into a sum of per-agent utilities \(Q_{tot}^{\mathrm{VDN}} \left(\boldsymbol{\tau}, \boldsymbol{u}\right)=\sum_{i=1}^{n} Q_{i} \left(\tau^{i}, u^{i}\right)\). VDN’s simplicity and equal weighting of each utility in the joint value function makes it ineffective for cooperative tasks, which has motivated the QMIX structure and other more efficient decomposition approaches.<br/><br/></p> <h3 id="notations"><a name="Notations">Notations</a></h3> <p>In this subsection, we define the notations used in this post. Specifically, in traditional RL, time steps $t$ are usually represented in the update formula and the value function of RL is considered to be estimated by the pairwise variables at the current time step $t$ and the next time step $t+1$. Since the <em>ID</em> of the agent also needs to be represented in the MARL algorithm, it may cause ambiguity when expressed in the same formula as the time step $t$. For simplicity of expression, variables without $t$ are indicated to be implemented at the current time step, while variables at the next time step are indicated with an apostrophe in the upper right corner in the rest of the context, e.g., $s$ means the current state and $s^{\prime}$ indicates the next time step state, the same approach applies to actions $u$ and observations $o$. All the notations are listed in Table <a href="#table1">1</a>.</p> <p><a name="table1"> </a></p> <center> Table 1: All the notations used in this post. </center> <style type="text/css">.tg{border-collapse:collapse;border-spacing:0}.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial,sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal}.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial,sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal}.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}</style> <table class="tg"> <thead> <tr> <th class="tg-c3ow">Notation</th> <th class="tg-c3ow">Description</th> <th class="tg-c3ow">Notation</th> <th class="tg-c3ow">Description</th> </tr> </thead> <tbody> <tr> <td class="tg-c3ow">$s$</td> <td class="tg-c3ow">the current state (at time $t$)</td> <td class="tg-c3ow">$S$</td> <td class="tg-c3ow">the set of all states</td> </tr> <tr> <td class="tg-c3ow">$s^{\prime}$</td> <td class="tg-c3ow">the next state (at time $t+1$)</td> <td class="tg-c3ow">$U$</td> <td class="tg-c3ow">the set of all actions</td> </tr> <tr> <td class="tg-c3ow">$u^{i}$</td> <td class="tg-c3ow">the action of agent $i$</td> <td class="tg-c3ow">$N$</td> <td class="tg-c3ow">the set of all agents</td> </tr> <tr> <td class="tg-c3ow">$\mathbf{u}$</td> <td class="tg-c3ow">the joint actions (at time $t$)</td> <td class="tg-c3ow">$\tau^{i}$</td> <td class="tg-c3ow">the action-observation history of agent $i$</td> </tr> <tr> <td class="tg-c3ow">$o^{i}$</td> <td class="tg-c3ow">the observation of agent $i$</td> <td class="tg-c3ow">$${\tau}$$</td> <td class="tg-c3ow">the joint action-observation histories</td> </tr> <tr> <td class="tg-c3ow">$$o$$</td> <td class="tg-c3ow">the joint observation</td> <td class="tg-c3ow">$r(s, \mathbf{u})$</td> <td class="tg-c3ow">the joint reward supplied by environments</td> </tr> <tr> <td class="tg-c3ow">$Q_{i}(\tau^{i}, u^{i})$</td> <td class="tg-c3ow">the utility function of agent $i$</td> <td class="tg-c3ow">$\gamma$</td> <td class="tg-c3ow">the discount factor</td> </tr> <tr> <td class="tg-c3ow">$Q_{tot}({\tau}, \mathbf{u})$</td> <td class="tg-c3ow">the joint value function </td> <td class="tg-c3ow">$P(s^{\prime} \mid s, \mathbf{u})$</td> <td class="tg-c3ow">the transition function</td> </tr> <tr> <td class="tg-c3ow">$Z(o^{i} \mid s, u^{i})$</td> <td class="tg-c3ow">the observation function</td> <td class="tg-c3ow">$\epsilon$</td> <td class="tg-c3ow">action selection probability of $\epsilon$-greedy</td> </tr> <tr> <td class="tg-c3ow">$N$</td> <td class="tg-c3ow">the set of all agents with $n$ agents</td> <td class="tg-c3ow">$$\theta$$</td> <td class="tg-c3ow">the set of parameters of agents network, with $[\theta^{i}]_{i=1}^{n}$</td> </tr> <tr> <td class="tg-c3ow">$b$</td> <td class="tg-c3ow">sampled batch size for training</td> <td class="tg-c3ow">$\phi$</td> <td class="tg-c3ow">the parameter of mixing network</td> </tr> <tr> <td class="tg-c3ow">$TS$</td> <td class="tg-c3ow">the $T$otal rollout $S$amples</td> <td class="tg-c3ow">$PP$</td> <td class="tg-c3ow">the number of rollout $P$rocesses in $P$arallel</td> </tr> <tr> <td class="tg-c3ow">$SE$</td> <td class="tg-c3ow">the number of $S$amples in each <br/> $E$pisode</td> <td class="tg-c3ow">$PI$</td> <td class="tg-c3ow">the $P$olicy $I$teration number</td> </tr> </tbody> </table> <h2 id="qmix-and-monotonicity-constraint"><a name="QMIX_and_Monotonicity_Constraint">QMIX and Monotonicity Constraint</a></h2> <p>To deal with the relationship between the individual agent and the cooperative group, QMIX [<a href="#8">8</a>] learns a joint action-value function $Q_{tot}$ and factorizes the joint policy into the individual policy of each agent. In other words, as illustrated in Figure <a href="#frame">4</a>, QMIX integrates all the individual $Q_{i}$ with a mixing network to obtain a centralized value function $Q_{tot}$, which can be more appropriately updated by the global reward.</p> <div id="frame" class="img-height-310 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/qmix_frame-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/qmix_frame-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/qmix_frame-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/qmix_frame.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 4: Framework of QMIX. (Image source: QMIX <a ref="#8">[8]</a>). On the left is Mixing Network (A Hypernetwork), and on the right is the Agent network.</center> <p><br/></p> <p>Still, it also can be represented in Eq.(\ref{eq2})</p> \[Q_{tot}(s, \boldsymbol{u} ; \boldsymbol{\theta}, \phi) = g_{\phi}\left(s, Q_{1}\left(\tau^{1}, u^{1} ; \theta^{1}\right), \ldots, Q_{n}\left(\tau^{n}, u^{n} ; \theta^{n}\right)\right);\] \[with \quad \frac{\partial Q_{tot}(s, \boldsymbol{u} ; \boldsymbol{\theta}, \phi)}{\partial Q_{i}\left(\tau^{i}, u^{i}; \theta^{i}\right)} \geq 0, \quad \forall i \in N. \tag{2} \label{eq2}\] <p>where $\theta^i$ is the parameter of the agent network $i$, $u^{i}$ denotes the action of agent $i$, and $\phi$ is the trainable parameter of the mixing network. The the mixing network $g_{\phi}(\cdot)$ is responsible to factorize $Q_{tot}$ to each utility $Q_{i}$. The <em>Monotonicity Constraint</em> is also implemented in the mixing network $g_{\phi}(\cdot)$, which inputs the global state $s$ and outputs <em>non-negative</em> wights through a <em>hyper-network</em> as illustrated in the left part of Figure <a href="#frame">4</a>, which will result in \(\frac{\partial Q_{tot}(s, \boldsymbol{u} ; \boldsymbol{\theta}, \phi)}{\partial Q_{i}\left(\tau^{i}, u^{i}; \theta^{i}\right)} \geq 0\). This delicate design ensures consistency between joint actions and the individual actions of each agent, then guarantees the Individual-Global-Max (IGM) principle. Benefiting from the monotonicity constraint in Eq. (\ref{eq2}), maximizing joint $Q_{tot}$ is precisely the equivalent of maximizing individual $Q_i$, which would also allow the optimal individual action to maintain consistency with optimal joint action. Furthermore, QMIX learns the centralized value function $Q_{tot}$ by sampling a multitude of transitions from the replay buffer and minimizing the mean squared temporal-difference (TD) error loss:</p> \[\mathcal{L}(\theta)= \frac{1}{2} \sum_{i=1}^{b}\left[\left(y_{i}^{}-Q_{tot}(s, u ; \theta, \phi)\right)^{2}\right] \tag{3} \label{eq3}\] <p>where the TD target value \(y=r+\gamma \underset{u^{\prime}}{\operatorname{max}} Q_{tot}(s^{\prime},u^{\prime};\theta^{-},\phi^{-})\), and $\theta^{-}, \phi^{-}$ are the target network parameters copied periodically from the current network and kept constant for a number of iterations. $b$ is the sampled training batch size. Due to the strong constraints in Eq.(\ref{eq2}), QMIX is still criticized for the insufficient expressive capacity of the joint value function [<a href="#3">3</a>].</p> <h2 id="extension-to-qmix"><a name="Extension_to_QMIX">Extension to QMIX</a></h2> <h3 id="experimental-design"><a name="Experimental_Design">Experimental Design</a></h3> <p>To facilitate the study of proper techniques affecting the training effectiveness and sample efficiency of QMIX, we perform a set of experiments designed to provide insight into some methods that have been proven effective in single-agent RL but may be ambiguous in MARL. In particular, we investigate the effects of <strong>Adam optimizer with parallel rollout process; the incremental replay buffer size; the number of parallel rollout processes; $\epsilon$-exploration steps; the implementation of $Q(\lambda)$ in centralized value function; the hidden size of the recurrent network of agents</strong>. And we also dive into the <strong>role of monotonicity constraints in QMIX</strong>. For all experiments, we generally implement PyMARL [<a href="#10">10</a>] framework to implement QMIX. To ensure fairness we run independent 3 to 6 experimental trials for each evaluation, each with a random seed. Unless otherwise mentioned, we use default settings as in PyMARL whenever possible, while incorporating the techniques of interest. To prevent the training process of the algorithm from crashing by chance, we remove the highest and lowest scores when counting the calculated returns and win rates for the test episode. All the results are plotted with the median and shaded the interval, and the final scores were <strong><em>not</em></strong> smoothed for the sake of image aesthetics, and we did so to verify exactly what direct effect the proposed techniques could have on QMIX.</p> <p><strong>StarCraft Multi-Agent Challenge (SMAC)</strong> As a commonly used testing environment, SMAC [<a href="#10">10</a>] sets an example to offer a great opportunity to tackle the cooperative control problems in the multi-agent domain. We focus on the micromanagement challenge in SMAC, which means each agent is controlled by an independent agency that conditions on a limited observation area, and these groups of units are trained to conquer the enemy consisting of built-in AI. According to the quantity and type of enemy, all testing scenarios could be divided into <em>Easy, Hard</em>, and <em>Super-Hard</em> levels. Since QMIX can effectively solve the <em>Easy</em> tasks, we pay attention to some <em>Hard</em> and <em>Super-Hard</em> scenarios that QMIX failed to win, especially in <em>Corridor, 3s5z_vs_3s6z</em>, and <em>6h_vs_8z</em>.</p> <p><strong>Predator-Prey (PP)</strong> is representative of another classical problem called <em>relative overgeneralization</em> [<a href="#16">16</a>] . The cooperating predators are trained to chase a faster running prey, and hope to capture this escaping robot with the fewest steps possible. We leverage Predator-Prey-2 (a variant of Predator-Prey) proposed in FACMAC [<a href="#29">29</a>], whose policy of prey is replaced with a hard-coded heuristic policy. The heuristic policy asks the prey to move to the farthest sampled position to the closest predator. If one of the cooperative agents collides with the prey, a team reward of +10 is emitted; otherwise, no reward is given. In the original simple tag environment, each agent can observe the relative positions of the other two agents, the relative position and velocity of the prey, and the relative positions of the landmarks. This means each agent’s private observation provides an almost complete representation of the true state of the environment.</p> <p>To introduce partial observability to the environment, the view radius is added to the agent, which restricts the agents from receiving information about other entities (including all landmarks, the other two agents, and the prey) that are out of range. Specifically, we set the view radius such that the agents can only observe other agents roughly 60% of the time. These environments require greater cooperation between agents.</p> <h3 id="optimizer"><a name="Optimizer">Optimizer</a></h3> <p>As an important part of training neural networks, the selection of an optimizer is very important since it could seriously affect the training effect of the reinforcement learning agent. Without a further illustration, QMIX uses RMSProp [<a href="#21">21</a>] to optimize the neural networks of agents as they prove stable in SMAC. While Adam [<a href="#1">1</a>] is famous for the fast convergence benefiting from the momentum in training, which seems to be the first choice for AI researchers. We reckon that the momentum property in Adam would have some advantages in learning the sampled data which is generated by agents interacting with the environment as in MARL. And then, on the other hand, QMIX is criticized for performing sub-optimally and sampling inefficiency when equipped with the A2C framework, which is implemented to promote the training efficiency of the RL algorithm. VMIX [<a href="#12">12</a>] argues this limitation is brought about by the value-based inherent Q function, so they extend QMIX to the actor-critic style algorithm to take advantage of the A2C framework. This controversy attracts our attention to evaluate the performance of QMIX using Adam, as well as the parallel sampling paradigm.</p> <div id="optimizer" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/optimizer.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/optimizer.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/optimizer.svg-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/optimizer.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 5: The performance of QMIX optimized by Adam and RMSProp.</center> <p><br/></p> <p><strong>Results</strong> As shown in Figure <a href="#optimizer">5</a>, we run the Adam-supported QMIX with <strong>8 rollout processes</strong>. Different from what was described in VMIX, the performance and efficiency of QMIX could be greatly improved by Adam. We speculate the reason is the momentum property in Adam could fastly fit the newly sampled data from the parallel rollout processes and then enhance the performance, while RMSProp failed.</p> <h3 id="rollout-process-number"><a name="Rollout_Process_Number">Rollout Process Number</a></h3> <p>Naturally, we come to focus on the benefits of parallel data sampling in QMIX. A2C [<a href="#5">5</a>] provides an excellent example to reduce training time and improve the training efficiency in single-agent RL. As we implement the algorithms under the paradigm of A2C, there is usually a defined total number of samples and an unspecified number of rollout processes. The total number of samples $TS$ can be calculated as $TS = SE \cdot PP \cdot PI$, where $TS$ is the total sum of sampled data, $SE$ denotes the number of samples in each episode, $PP$ and $PI$ denote the number of rollout processes in parallel and the policy iteration number, respectively. This section aims to perform analysis and spur discussion on the impact of the parallel rollout process on the final performance of QMIX.</p> <div id="process_number" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/process_number.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/process_number.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/process_number.svg-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/process_number.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 6: The performance of different rollout process numbers of QMIX. When given the total number of samples, the performance of fewer processes achieves better performance.</center> <p><br/></p> <p><strong>Results</strong> Still, we use Adam-supported QMIX to evaluate the effect of the number of the rollout process. Since we could choose the <em>Parallel</em> model to sample the interacting data of the agent with the environment in PyMARL, we can theoretically get more <strong>on-policy</strong> data which is close to the updating policy in training. Figure <a href="#process_number">6</a> shows that when $TS$ and $PP$ is given, the performance enhancement of QMIX is not consistent with the increase in rollout process number. The intuitive explanation is when we set the fewer rollout processes, the greater the quantity of policy would iterate [<a href="#14">14</a>]. Besides, too fast updated data in parallel may cause the factitious unstable training in policy updating, i.e., it is difficult for agents to learn effective information from rapidly sampled data from the replay buffer. The more times policies are iterated, the more information the agents would learn which lead to an increase in performance. However, it also causes longer training time and loss of stability. We suggest trying the fewer rollout process in the beginning and then balancing between training time and performance.</p> <h3 id="replay-buffer-size"><a name="Replay_Buffer_Size">Replay Buffer Size</a></h3> <p>Replay buffer plays an important role in improving sample efficiency in off-policy single-agent RL. Its capacity would greatly affect the performance and stability of algorithms. Researchers usually set a very large capacity of replay buffer in Deep Q-network (DQN) [<a href="#4">4</a>] to stabilize the training. Some research on the effect of replay buffer in single-agent RL has already been carried out in [<a href="#22">22</a>] , which poses the distribution of sampled training data should be close as possible to the agents’ policies to be updated. Actually, there are two factors affected when we change the capacity of the replay buffer: (1) the replay capacity (total number of transitions/episodes stored in the buffer); and (2) the replay ratio (the number of gradient updates per environment transition/episode) of old policies. When we increase the capacity of the replay buffer, the aged experiences of old policies would grow as the replay ratio is fixed. Then the distribution of outdated experiences would also be much different from the updating policy, which would bring additional difficulty to the training agents. From the results in [<a href="#22">22</a>], there seems to be an optimal range of choices between replay buffer size and replay ratio of experiences in RL, where we would like to know whether it is consistent with the results in MARL.</p> <div id="replay_buffer" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/buffer_size.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/buffer_size.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/buffer_size.svg-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/buffer_size.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 7: Setting the replay buffer size to 5000 episodes allows for QMIX’s learning to be stable.</center> <p><br/></p> <p><strong>Results</strong> The results seem not to be consistent with that in single-agent RL. Figure <a href="#replay_buffer">7</a> shows the large replay buffer size of QMIX would cause instability during training. When we increase the buffer size from the default setting in PyMARL, the performance would almost continuously declines. We speculate the reason is the fast-changing distribution of experiences in a larger buffer would make it more difficult to fit sampled data due to the enormous joint action space. Since the samples become obsolete more quickly, these aged policies would also be more different from the updating policy, which brings additional difficulty. On the other hand, we find the same performance decline when we squeeze the buffer. We reckon that a small buffer would accelerate the updating speed of sampling data in a disguised way, which makes it tough to fit the data and learn a good policy. We believe that researchers should be cautious to increase the buffer size in other multi-agent applications.</p> <h3 id="eligibility-traces"><a name="Eligibility_Traces">Eligibility Traces</a></h3> <p>The well-known trade-off between bias and variance of bootstrapping paradigm is a classic research topic in RL. Since we implement the Centralized Value Function (CVF) to alleviate the <em>Non-Stationarity</em> multi-agent settings, the estimated accuracy of CVF is critical to MARL and then guides the policies of agents to update. Eligibility traces such as TD($\lambda$)[<a href="#14">14</a>], Peng’s Q($\lambda$)[<a href="#2">2</a>], and TB($\lambda$)[<a href="#7">7</a>] achieve a balance between return-based algorithms (where return refers to the sum of discounted rewards $\sum_{k} \gamma^{k} r_{t+k}$) and bootstrap algorithms (where return refers $r_t + V(s_{t+1})$), then speed up the convergence of agents’ policies. As a pioneer, SMIX [<a href="#20">20</a>] equipped QMIX with the SARSA($\lambda$) to estimate the accurate CVF and get decent performance. As another example of eligibility trace in Q-learning, we study the estimation of CVF using Peng’s Q$(\lambda)$ for QMIX.</p> <div id="qlambda1" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/td_lambda.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/td_lambda.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/td_lambda.svg-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/td_lambda.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 8: Q(λ) significantly improves the performance of QMIX, but large values of λ lead to instability in the algorithm.</center> <p><br/></p> <p><strong>Results</strong> As the same in single-agent RL, the Q-networks without sufficient training usually have a large bias in bootstrapping returns. Figure <a href="#qlambda1">8</a> shows that, with the help of Q$(\lambda)$, the performance of QMIX has generally improved across all scenarios. It means the more accurate estimate of CVF would still provide a better direction of policy updating for each agent. However, the value of $\lambda$ in Peng’s Q$(\lambda)$ is not so radical as in single-agent RL, which would lead to failed convergence due to the large variance. We recommend a small $\lambda$, such as $0.5$, when using $Q(\lambda)$ in MARL.</p> <h3 id="hidden-size"><a name="Hidden_Size">Hidden Size</a></h3> <p>Searching for an optimal scale and architecture of neural networks is a very tough problem in the field of machine learning. Researchers typically use empirically small networks to train the agents in deep reinforcement learning. Since the role of neural networks is to extract the features of input states and actions, the size of the neural network would also have a great impact on the performance of MARL algorithms. The study in [<a href="#23">23</a>] has revealed that networks with a complex structure like ResNet[<a href="#25">25</a>] and DenseNet[<a href="#26">26</a>] can extract more useful information for training, while Ba [<a href="#24">24</a>] poses that the width of neural networks is probably more important than its depth. The subsequent study on QMIX [<a href="#19">19</a>] makes preliminary research on the depth of neural networks, which showed a limited improvement in performance. Though, there is little research on the width of neural networks in MARL. Instead of searching for an optimal network architecture here, we just want to make a pilot study on the effect of the hidden size of network width in QMIX.</p> <div id="hiddensize" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/hidden_size.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/hidden_size.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/hidden_size.svg-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/hidden_size.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 9: Impact of the hidden size of network in QMIX.</center> <p><br/></p> <p><strong>Results</strong> The study in [<a href="#24">24</a>] illustrates the ability of infinity-width networks to fit any complex function, which would theoretically provide the performance gain from increasing network width. As shown in Figure <a href="#hiddensize">9</a>, the final performance or the efficiency of policy training would have varying degrees of improvement when we increase the hidden size of the network from 64 to 256 in QMIX, where <strong>QMIX-ALL-Hidden refers to the size of the network including Recurrent Neural Network (RNN) and mixing part, while QMIX-RNN-Hidden just refers to RNN</strong>. Also, the results reveal the spectacular effect of increasing the network width of RNN, which would allow for about a 20% increase in the Super-Hard scenarios <em>3s5z_vs_3s6z</em>. While the performance improvement is limited in enlarging the mixing network. We speculate that more units in the network are needed to represent the complex temporal context information in RNN, which is not included in the mixing network. We advise researchers to appropriately increase the network width of RNN to achieve better performance.</p> <h3 id="exploration-steps"><a name="Exploration_Steps">Exploration Steps</a></h3> <p>Exploration and exploitation are other classic trade-offs in reinforcement learning. Agents need some directed mechanisms to explore the states that may be of higher value or inexperienced. The most versatile method of exploration in RL is $\epsilon$-greedy action, which makes the agent select random actions with probability $\epsilon$, or select the greedy action with $1 - \epsilon$. The value of $\epsilon$ would drop-down with training and then stays at a small constant. The annealing period of $\epsilon$-greedy determines how fast the drop down will be. This exploration mechanism is usually implemented for each agent to select their action, which has been criticized by MAVEN [<a href="#3">3</a>] for lacking joint exploratory policy over an entire episode. However, we can still get more exploration when $\epsilon$ drops slower, then we evaluate the performance of the annealing period of $\epsilon$-greedy in some Super-Hard scenarios in SMAC.</p> <div id="exploration" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/exploration.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/exploration.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/exploration.svg-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/exploration.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 10: Experinments for the impact of ε anneal period.</center> <p><br/></p> <p><strong>Results</strong> Apparently, appropriately increasing the annealing period of $\epsilon$-greedy from 100K steps to 500K would get explicit performance gain in those hard exploration scenarios, where QMIX failed with the default setting. However, as shown in Figure <a href="#exploration">10</a>, too large steps like 1000K would also bring additional exploration noise even making the training collapse. The results above confirm the $\epsilon$-greedy mechanism is still the proper and simplest choice in MARL but should be elaboratively tuned for different tasks.</p> <h3 id="integrating-the-techniques"><a name="Integrating_the_Techniques">Integrating the Techniques</a></h3> <p>These techniques mentioned above indeed impact QMIX in hard cooperative scenarios of SMAC, which really catches our attention to exhaust the extreme performance of QMIX. We combine these techniques and finetune all the hyperparameters in QMIX for each scenario of SMAC. As shown in Table <a href="#table2">2</a>, the Finetuned-QMIX would almost conquer all the scenarios in SMAC and exceed the effect of the original QMIX by a large margin in some Hard and Super-Hard scenarios.</p> <p><a name="table2"> </a></p> <center> Table 2: Best median test win rate of Finetuned-QMIX and QMIX (batch size=128) </center> <center> in all testing scenarios. </center> <table style="text-align: center; width: 600px; margin: 0 auto; margin-bottom:20px; margin-top:20px"> <thead> <tr> <td><b>Senarios</b></td> <td><b>Difficulty</b></td> <td><b>QMIX</b></td> <td><b>Finetuned-QMIX</b></td> </tr> </thead> <tbody> <tr> <td>10m_vs_11m</td> <td>Easy</td> <td>98%</td> <td><b>100%</b></td> </tr> <tr> <td>8m_vs_9m</td> <td>Hard</td> <td>84%</td> <td><b>100%</b></td> </tr> <tr> <td>5m_vs_6m</td> <td>Hard</td> <td>84%</td> <td><b>90%</b></td> </tr> <tr> <td>3s_vs_5z</td> <td>Hard</td> <td>96%</td> <td><b>100%</b></td> </tr> <tr> <td>bane_vs_bane</td> <td>Hard</td> <td><b>100%</b></td> <td><b>100%</b></td> </tr> <tr> <td>2c_vs_64zg</td> <td>Hard</td> <td><b>100%</b></td> <td><b>100%</b></td> </tr> <tr> <td>corridor</td> <td>Super hard</td> <td>0%</td> <td><b>100%</b></td> </tr> <tr> <td>MMM2</td> <td>Super hard</td> <td>98%</td> <td><b>100%</b></td> </tr> <tr> <td>3s5z_vs_3s6z</td> <td>Super hard</td> <td>3%</td> <td><b>93% (Hidden Size = 256)</b></td> </tr> <tr> <td>27m_vs_3s6z</td> <td>Super hard</td> <td>56%</td> <td><b>100%</b></td> </tr> <tr> <td>6h_vs_8z</td> <td>Super hard</td> <td>0%</td> <td><b>93% (λ = 0.3)</b></td> </tr> </tbody> </table> <h2 id="role-of-monotonicity-constraint"><a name="Role_of_Monotonicity_Constraint">Role of Monotonicity Constraint</a></h2> <h3 id="amazing-performance-in-policy-based-methods"><a name="Amazing_Performance_in_Policy-Based_Methods">Amazing Performance in Policy-Based Methods</a></h3> <div id="qmix_sy" class="img-height-180 image-center img-margin-left-30"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/riit.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/riit.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/riit.svg-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/riit.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 11: Architecture for AC-MIX: <b>|·|</b> denotes <b>absolute value operation</b>, implementing the monotonicity constraint of QMIX. <b>W</b> denotes the non-negative mixing weights. Agent $i$ denotes the agent's network, which can be trained end-to-end by maximizing the $Q_{tot}$.</center> <p><br/></p> <p>The novelty of QMIX is the IGM consistency between $\text{argmax} Q_{tot}$ and $\text{argmax} \sum_{i}^{n} Q_{i}$, which is implemented in the mixing network. <strong>We still expect to study the role of <em>monotonicity constraint</em> in MARL</strong>. Therefore, we propose an actor-critic style algorithm called Actor-Critic-Mixer (AC-MIX), which has a similar architecture to QMIX. As illustrated in Figure <a href="#qmix_sy">11</a>, we use the monotonic mixing network as a centralized critic, which integrates $Q_{i}$ of each agent, to optimize the decentralized policy networks $π^i_{θ_i}$ in an end-to-end pattern. We still add the Adaptive Entropy $\mathcal{H}(\cdot)$ <a href="#18">[18]</a> of each agent in the optimization object of Eq.(\ref{eq4}) to get more exploration, and the detail of the algorithm will be described in Appendix <a href="#A">A</a>.</p> \[\max _{\theta} \mathbb{E}_{t, s_{t}, \tau_{t}^{1}, \ldots, \tau_{t}^{n}}\left[Q_{\theta_{c}}^{\pi}\left(s_{t}, \pi_{\theta_{1}}^{1}\left(\cdot \mid \tau_{t}^{1}\right), \ldots, \pi_{\theta_{n}}^{n}\left(\cdot \mid \tau_{t}^{n}\right)\right) + \mathbb{E}_{i}\left[\mathcal{H}\left(\pi_{\theta_{i}}^{i}\left(\cdot \mid \tau_{t}^{i}\right)\right)\right]\right] \tag{4} \label{eq4}\] <div id="riit_abla" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/monotonicity_riit.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/monotonicity_riit.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/monotonicity_riit.svg-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/monotonicity_riit.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 12: Comparing AC-MIX w./ and w./o. monotonicity constraint</center> <center>(remove absolute value operation) on SMAC and Predator-Prey-2</center> <p><br/></p> <p>As the monotonicity constraint on the critic of AC-MIX is theoretically no longer required as the critic is not used for greedy action selection. We can evaluate the effects of the monotonicity constraint by removing the absolute value operation in the mixing network. The results in Figure <a href="#riit_abla">12</a> demonstrate the <em>monotonicity constraint</em> significantly improves the performance of AC-MIX. Then to explore the generality of <em>monotonicity constraints</em> in the parallel sampling framework of MARL, we extend the above experiments to VMIX [<a href="#12">12</a>] . VMIX adds the monotonicity constraint to the value network of A2C, and learns the policy of each agent by advantage-based policy gradient [<a href="#14">14</a>] as illustrated in Figure <a href="#vmix_net">13</a>. Still, the result from Figure <a href="#vmix_abla">14</a> shows that the monotonicity constraint improves the sample efficiency in value networks.</p> <div id="vmix_net" class="img-height-180 image-center img-margin-left-60"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/vmix.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/vmix.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/vmix.svg-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/vmix.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 13. Architecture for VMIX: |·| denotes absolute value operation</center> <p><br/></p> <div id="vmix_abla" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/monotonicity_vmix.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/monotonicity_vmix.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/monotonicity_vmix.svg-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/monotonicity_vmix.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 14: Comparing VMIX w./ and w./o. monotonicity constraint</center> <center>(remove absolute value operation) on SMAC</center> <p><br/></p> <h3 id="what-is-under-the-hood"><a name="What_is_Under_the_Hood">What is Under the Hood?</a></h3> <p>Observed from the results of previous experiments, <strong>the <em>monotonicity constraints</em> in the mixing network indeed improve performance and sample efficiency of training</strong>, but on the flip side of the coin, QMIX is still criticized for the insufficient expressive capacity of the centralized critic [<a href="#3">3</a>], which may cause poor performance. The abnormal question naturally occurred to us: <em>Why the performance of AC-MIX would be better than AC-MIX-nonmonotonic which aims to relax the monotonicity constraint of mixing network</em>?</p> <p>To answer this question we first need to reexamine the <strong>IGM</strong> principle. Since in QMIX, $Q_{tot}$ is decomposed by the mixing network into the sum of the weighted $[Q_i] _{i=1}^{n}$, as shown in Figure <a href="#frame">4</a>, where the weights and bias of mixing network are generated by the <em>Hypernetwork</em>, then the monotonicity in QMIX can be defined simplistically as a constraint on the relationship between \(Q_{tot}\) and each \(Q_{i}\) :</p> \[Q_{tot} = \sum_{i=1}^{N}w_{i}(s_{t}) \cdot Q_{i} + b(s_{t}), \\ w_{i} = \frac{\partial Q_{tot}}{\partial Q_{i}} \geq 0, \forall i \in N. \tag{5} \label{5}\] <p>From the sufficient condition above, the weight $w_{i}$ in <em>Mixing Network</em> would be forced to be greater or equal to zero $w_{i} \geq 0$. To put it another way, it makes the parameter space smaller for searching $w_{i}$ weights to decompose $Q_{tot}$. As illustrated in the schematic diagram <a href="#diagram">15</a>, assume there is only 1 agent in the environment, the parameter searching space will be directly halved and the optimal $w_{1}$ will be found in the region where $w \geq 0$, i.e., the green region. Similarly, when the number of agents is 2 or 3, its parameter searching space for $w_i$ will be restricted to the first quadrant, and the same can be recursively extended to the case of high-dimensional parameter space. <strong>In other words, the search area of exhausting the whole joint state-action space would also be decreased exponentially by $(\frac{1}{2})^{N}$ ($N$ denotes the number of $w_{i}$, as well as the number of agents).</strong> Then the optimal solution in the original domain cannot be expressed correctly in the restricted region. Since the essence of learning in MARL is to search for the optimal joint-policy parameterized by weights and bias of agents and mixing network, QMIX could find a satisfying policy more quickly in these <strong>reduced</strong> parameter spaces.</p> <div id="diagram" style="display:flex; margin:20px 0; gap:5px"> <div id="1_agent" class="img-height-100"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/1_agent.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/1_agent.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/1_agent.svg-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/1_agent.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="2_agent" class="img-height-100"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/2_agent.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/2_agent.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/2_agent.svg-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/2_agent.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="3_agent" class="img-height-100"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/3_agent.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/3_agent.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/3_agent.svg-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/3_agent.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></div> <div style="margin-bottom: 20px"><center>Figure 15: the weight parameter space diagram of different number of agents in QMIX [from-left-to-right]. (a) weight parameter space of only 1 agent; (b) weight parameter space of 2 agents; (c) weight parameter space of 3 agents.</center></div> <p>As a side effect, the global optimum may not be in the parameter space that QMIX needs to search at all due to the monotonicity of the mixing network. One effective way is to estimate the $Q_{tot}$ as accurately as possible in the hope that it could find the global optimum, this probably explains why $Q(\lambda)$ in the previous section could result in such a performance improvement in SMAC. On the other hand, we could delicately design the reward function to be approximately monotonic when we use QMIX to solve cooperative multi-agent tasks. Then adapting the algorithm to the test environment is not a good idea, after all, we still need to figure out how to use QMIX more effectively or develop other more efficient algorithms.</p> <h2 id="conclusion"><a name="Conclusion">Conclusion</a></h2> <p>In this post, we revisited the performance of the QMIX as a baseline algorithm in the SMAC environment. We found that the application of hyperparameters and other RL techniques have a great impact on the effectiveness of QMIX. We evaluated the effect of optimizer, number of rollout processes, replay buffer size, eligibility traces, hidden size and the degree of annealed exploration on QMIX, and tried to explain their role in MARL. Furthermore, we dived into the monotonicity in QMIX, and found the absolute operation in mixing network would decrease the parameter searching space of the joint state-action area exponentially by $(\frac{1}{2})^{N}$, which would make QMIX find the satisfying policy more quickly but with the drawback of inaccurate evaluated joint value function of optimal policy. We hope that our findings will stimulate some inspiration for the value decomposition method in MARL and provoke the community to think about the performance of QMIX as a new benchmark.</p> <h2 id="authorship-credit-attribution-and-acknowledgement"><a name="Authorship,_Credit_Attribution_and_Acknowledgement ">Authorship, Credit Attribution and Acknowledgement</a></h2> <p>Jian Hu was responsible for the key ideas, open source code and all experiments, as well as the first draft of the paper.</p> <p>Siying Wang was responsible for the writing of the blog.</p> <p>Siyang Jiang participated in writing the first draft of the paper.</p> <p>Weixun Wang provided feedback on revisions.</p> <p>Siyang Jiang was supported by the fund which aims to improve scientific research capability of key construction disciplines in Guangdong province “Light-weight federal learning paradigm and its application” (No:2022ZDJS058) and Foundation for Distinguished Young Talents in Higher Education of Guangdong, China. (NO. 2022KQNCX084)</p> <h2 id="appendix"><a name="Appendix">Appendix</a></h2> <h3 id="a-pseudo-code-of-ac-mix-">A Pseudo-code of AC-MIX<a id="A"> </a></h3> <p>In this subsection, we show the pseudo-code for the training procedure of AC-MIX. (1) Training the critic network with offline samples and 1-step TD error loss improves the sample efficiency for critic networks; (2) We find that policy networks are sensitive to old sample reuse. Training policy networks end-to-end and critic with TD($\lambda$) and online samples improve the learning stability of AC-MIX.</p> <div id="algorithm_riit" class="img-height-600 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/algorithm_riit.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/algorithm_riit.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/algorithm_riit.svg-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-13-riit/algorithm_riit.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h3 id="b-hyperparameters">B HYPERPARAMETERS</h3> <p>In this subsection, we present our hyperparameters tuning process. We get the optimal hyperparameters for each algorithm by grid search, shown in Table <a href="#t3">3</a>.</p> <center> Table 3: Hyperparameters Search on SMAC. The bold type indicates the </center> <center> selected hyperparameters. </center> <table style="text-align: center; width: 700px; margin: 0 auto; margin-bottom:20px; margin-top:20px;"><a name="t3"> </a> <thead> <tr> <td><b>Tricks</b></td> <td><b>QMIX</b></td> <td><b>AC-MIX</b></td> </tr> </thead> <tbody> <tr> <td>Optimizer</td> <td><b>Adam</b>,RMSProp</td> <td><b>Adam</b>,RMSProp</td> </tr> <tr> <td>Learning Rates</td> <td>0.0005, <b>0.001</b></td> <td>0.0005, <b>0.001</b></td> </tr> <tr> <td>Batch Size (episodes)</td> <td>32, 64, <b>128</b></td> <td>32, <b>64</b> </td> </tr> <tr> <td>Replay Buffer Size</td> <td><b>5000</b>, 10000, 20000</td> <td>2000, <b>5000</b>, 10000</td> </tr> <tr> <td>Q(λ)/TD(λ)</td> <td>0, 0.3, <b>0.6</b>, 0.9</td> <td>0.3, <b>0.6</b>, 0.8</td> </tr> <tr> <td>Entropy/Adaptive Entropy</td> <td>-</td> <td>0.005, 0.01, <b>0.03</b>, 0.06</td> </tr> <tr> <td>ε Anneal Steps</td> <td>50K, <b>100K, 500K</b>, 1000K</td> <td>-</td> </tr> </tbody> </table> <p><br/></p> <p><strong>Rollout Processes Number</strong>. For SMAC, 8 rollout processes for parallel sampling are used to obtain as many samples as possible from the environments at a high rate. And 4 rollout processes are used for Predator-Prey-2.</p> <p><strong>Other Settings</strong>. We set all discount factors $\gamma$ = 0.99. We update the target network every 200 episodes.</p> <h2 id="reference"><a name="Reference">Reference</a></h2> <p><a name="1" href="https://arxiv.org/abs/1412.6980">[1] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR 2015, San Diego, CA, USA, May 7-9, 2015, 2015. </a></p> <p><a name="2" href="https://arxiv.org/abs/2103.00107">[2] Tadashi Kozuno, Yunhao Tang, Mark Rowland, Remi Munos, Steven Kapturowski, Will Dabney, Michal Valko, and David Abel. Revisiting peng’s q (λ) for modern reinforcement learning. arXiv preprint arXiv:2103.00107, 2021. </a></p> <p><a name="3" href="https://arxiv.org/abs/1910.07483">[3] Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. MAVEN: multi-agent variational exploration. In NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,pp. 7611–7622, 2019. </a></p> <p><a name="4" href="https://arxiv.org/abs/1312.5602">[4] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, DaanWierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.</a></p> <p><a name="5" href="http://proceedings.mlr.press/v48/mniha16.html">[5] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In ICML 2016, New York City, NY, USA, June 19-24, 2016, pp. 1928–1937, 2016.</a></p> <p><a name="6" href="https://www.comp.nus.edu.sg/~leews/publications/rss09.pdf">[6] Sylvie CW Ong, Shao Wei Png, David Hsu, and Wee Sun Lee. Pomdps for robotic tasks with mixed observability. 5:4, 2009. </a></p> <p><a name="7" href="https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1079&amp;context=cs_faculty_pubs">[7] Doina Precup, Richard S. Sutton, and Satinder P. Singh. Eligibility traces for off-policy policy evaluation. In ICML 2000, Stanford University, Stanford, CA, USA, June 29 - July 2, 2000, pp.759–766. Morgan Kaufmann, 2000. </a></p> <p><a name="8" href="http://proceedings.mlr.press/v80/rashid18a.html">[8] Tabish Rashid, Mikayel Samvelyan, Christian Schr ̈oder de Witt, Gregory Farquhar, Jakob N.Foerster, and Shimon Whiteson. QMIX: monotonic value function factorization for deep multi-agent reinforcement learning. In ICML 2018, Stockholmsmassan, Stockholm, Sweden, July10-15, 2018, pp. 4292–4301, 2018. </a></p> <p><a name="9" href="https://ui.adsabs.harvard.edu/abs/2020arXiv200610800R/abstract">[9] Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted QMIX: Expand-ing Monotonic Value Function Factorisation. arXiv preprint arXiv:2006.10800, 2020. </a></p> <p><a name="10" href="https://arxiv.org/abs/1902.04043">[10] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, NantasNardelli, Tim G. J. Rudner, Chia-Man Hung, Philip H. S. Torr, Jakob Foerster, and ShimonWhiteson. The StarCraft Multi-Agent Challenge.arXiv preprint arXiv:1902.04043, 2019. </a></p> <p><a name="11" href="http://proceedings.mlr.press/v97/son19a.html">[11] Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Hostallero, and Yung Yi. QTRAN: learning to factorize with transformation for cooperative multi-agent reinforcement learning. In ICML 2019, 9-15 June 2019, Long Beach, California, USA, pp. 5887–5896, 2019. </a></p> <p><a name="12" href="https://www.aaai.org/AAAI21Papers/AAAI-2412.SuJ.pdf">[12] Jianyu Su, Stephen Adams, and Peter A. Beling. Value-Decomposition Multi-Agent Actor-Critics. arXiv:2007.12306, 2020. </a></p> <p><a name="13" href="https://arxiv.org/abs/1706.05296">[13] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi,Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Grae-pel. Value-Decomposition Networks For Cooperative Multi-Agent Learning. arXiv preprint arXiv:1706.05296, 2017. </a></p> <p><a name="14" href="https://go.gale.com/ps/i.do?id=GALE%7CA61573878&amp;sid=googleScholar&amp;v=2.1&amp;it=r&amp;linkaccess=abs&amp;issn=07384602&amp;p=AONE&amp;sw=w">[14] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018. </a></p> <p><a name="15" href="https://arxiv.org/abs/2008.01062">[15] Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. QPLEX: Duplex Dueling Multi-Agent Q-Learning. arXiv:2008.01062, 2020. </a></p> <p><a name="16" href="https://www.aaai.org/ocs/index.php/SSS/SSS18/paper/viewPaper/17508">[16] Ermo Wei, Drew Wicke, David Freelan, and Sean Luke. Multiagent Soft Q-Learning. arXivpreprint arXiv:1804.09817, 2018. </a></p> <p><a name="17" href="https://arxiv.org/abs/2002.03939">[17] Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and HongyaoTang. Qatten: A General Framework for Cooperative Multiagent Reinforcement Learning. arXiv preprint arXiv:2002.03939, 2020. </a></p> <p><a name="18" href="https://arxiv.org/abs/2010.09776">[18] Ming Zhou, Jun Luo, and Julian Villella et al. Smarts: Scalable multi-agent reinforcement learning training school for autonomous driving, 2020. </a></p> <p><a name="19" href="https://www.jmlr.org/papers/volume21/20-081/20-081.pdf">[19] Rashid T, Samvelyan M, Schroeder de Witt C, et al. Monotonic value function factorisation for deep multi-agent reinforcement learning. Journal of Machine Learning Research, 2020, 21.</a></p> <p><a name="20" href="https://ojs.aaai.org/index.php/AAAI/article/view/6223">[20] Wen C, Yao X, Wang Y, et al. Smix (λ): Enhancing centralized value functions for cooperative multi-agent reinforcement learning. Proceedings of the AAAI Conference on Artificial Intelligence. 2020, 34(05): 7301-7308. </a></p> <p><a name="21" href="http://www.cs.toronto.edu/~hinton/coursera/lecture6/lec6.pdf">[21] Hinton G, Srivastava N, Swersky K. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent. Cited on, 2012, 14(8): 2.</a></p> <p><a name="22" href="http://proceedings.mlr.press/v119/fedus20a.html">[22] Fedus W, Ramachandran P, Agarwal R, et al. Revisiting fundamentals of experience replay. International Conference on Machine Learning. PMLR, 2020: 3061-3071. </a></p> <p><a name="23" href="http://proceedings.mlr.press/v119/ota20a.html">[23] Ota K, Oiki T, Jha D, et al. Can Increasing Input Dimensionality Improve Deep Reinforcement Learning?. International Conference on Machine Learning. PMLR, 2020: 7424-7433.</a></p> <p><a name="24" href="https://arxiv.org/abs/1312.6184">[24] Ba L J, Caruana R. Do deep nets really need to be deep?. arXiv preprint arXiv:1312.6184, 2013.</a></p> <p><a name="25" href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">[25] He K, Zhang X, Ren S, et al. Deep residual learning for image recognition. Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.</a></p> <p><a name="26" href="https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.html">[26] Huang G, Liu Z, Van Der Maaten L, et al. Densely connected convolutional networks. Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 4700-4708.</a></p> <p><a name="27" href="http://proceedings.mlr.press/v48/wangf16.html">[27] Wang Z, Schaul T, Hessel M, et al. Dueling network architectures for deep reinforcement learning. International conference on machine learning. PMLR, 2016: 1995-2003.</a></p> <p><a name="28" href="https://www.ccs.neu.edu/home/camato/publications/OliehoekAmato16book.pdf">[28] Oliehoek, Frans A., and Christopher Amato. A concise introduction to decentralized POMDPs. Springer, 2016.</a></p> <p><a name="29" href="https://proceedings.neurips.cc/paper_files/paper/2021/file/65b9eea6e1cc6bb9f0cd2a47751a186f-Paper.pdf">[29] Peng, Bei, et al. Facmac: Factored multi-agent centralised policy gradients. Advances in Neural Information Processing Systems 34 (2021): 12208-12221.</a></p>]]></content><author><name>Jian Hu</name></author><category term="multi-agent"/><category term="reinforcement-learning"/><category term="experimental techniques"/><category term="monotonicity"/><summary type="html"><![CDATA[QMIX [8], a very classical multi-agent reinforcement learning (MARL) algorithm, is often considered to be a weak performance baseline due to its representation capability limitations. However, we found that by improving the implementation techniques of QMIX we can enable it to achieve state-of-the-art on the StarCraft Multi-Agent Challenge (SMAC) testbed [10]. Specifically, we evaluated the effect of optimizer, number of rollout processes, replay buffer size, eligibility traces, hidden size and the degree of annealed exploration on QMIX, and tried to explain their role in MARL. Furthermore, the key factor of the monotonicity constraint of QMIX was found in this post, we tried to explain its role and corroborated its superior performance by combining it with another actor-critic style algorithm. We have open-sourced the code at https://github.com/hijkzzz/pymarl2 for researchers to evaluate the effects of these proposed techniques. We hope this research will advance the MARL community and contribute to the establishment of new baselines of QMIX.]]></summary></entry><entry><title type="html">Decay No More</title><link href="https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/adamw/" rel="alternate" type="text/html" title="Decay No More"/><published>2022-12-01T00:00:00+00:00</published><updated>2022-12-01T00:00:00+00:00</updated><id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/adamw</id><content type="html" xml:base="https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/adamw/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Weight decay is a regularization technique in machine learning which scales down the weights in every step. It dates back at least to the 1990’s and the work of Krogh and Hertz <d-cite key="Krogh1991"></d-cite> and Bos and Chug <d-cite key="Bos1996"></d-cite>.</p> <p>In <code class="language-plaintext highlighter-rouge">Pytorch</code>, weight decay is one simple line which typically is found somewhere in the <code class="language-plaintext highlighter-rouge">step</code>-method:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="sh">'</span><span class="s">params</span><span class="sh">'</span><span class="p">]:</span>
  <span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">add_</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">decay</span><span class="p">)</span></code></pre></figure> <p>Subtracting a multiple of the weight can be seen as taking a step into the negative gradient direction of the squared norm of the weight. This relates weight decay to \(\ell_2\)-regularization (see also the <a href="#appendix">Appendix</a> with an excerpt of the original work by Krogh and Hertz <d-cite key="Krogh1991"></d-cite>).</p> <p>The exact mechanism of weight decay is still puzzling the machine learning community:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">The story of weight decay in pictures:<br/><br/>weight decay ...<br/>1) improves data efficiency by &gt; 50%<br/>2) is frequently found in the best hyperparam configs<br/>3) is among the most important hparams to tune<br/>4) is also tricky to tune <a href="https://t.co/PjWpk3pJxz">pic.twitter.com/PjWpk3pJxz</a></p>&mdash; Sebastian Raschka (@rasbt) <a href="https://twitter.com/rasbt/status/1614327550058328064?ref_src=twsrc%5Etfw">January 14, 2023</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">There is a gaping hole in the literature regarding the purpose of weight decay in deep learning. Nobody knows what weight decay does! AFAIK, the last comprehensive look at weight decay was this 2019 paper <a href="https://t.co/7WDBZojsm0">https://t.co/7WDBZojsm0</a>, which argued that weight decay <a href="https://t.co/qUpCbfhFRf">https://t.co/qUpCbfhFRf</a></p>&mdash; Jeremy Cohen (@deepcohen) <a href="https://twitter.com/deepcohen/status/1617274166570528769?ref_src=twsrc%5Etfw">January 22, 2023</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>The paper by Zhang et al. <d-cite key="Zhang2019"></d-cite> - which is the one mentioned in the second tweet - gives a comprehensive overview of weight decay and its effect on generalization, in particular in the interplay with Batch Normalization <code class="language-plaintext highlighter-rouge">(BN)</code> <d-cite key="Ioffe2015"></d-cite>. Batch Normalization describes a module of a network that normalizes the output of the previous layer to have zero mean and variance of one (or a variant of this with learnable mean and variance). We will not go into the details here but refer to <a href="https://iclr-blog-track.github.io/2022/03/25/unnormalized-resnets/">this blog post</a> <d-cite key="pieterjan2022normalizationisdead"></d-cite> for the interested reader.</p> <p>We want to summarize two findings of <d-cite key="Zhang2019"></d-cite>:</p> <ul> <li>On the one hand, weight decay has (in theory) no effect on layers with <code class="language-plaintext highlighter-rouge">(BN)</code>. This is simply due to the fact that <code class="language-plaintext highlighter-rouge">(BN)</code> makes the output invariant to a rescaling of the weights.</li> </ul> <blockquote> Weight decay is widely used in networks with Batch Normalization (Ioffe &amp; Szegedy, 2015). In principle, weight decay regularization should have no effect in this case, since one can scale the weights by a small factor without changing the network’s predictions. Hence, it does not meaningfully constrain the network’s capacity. —Zhang et al., 2019 </blockquote> <ul> <li>However, te experiments of the paper show that weight decay on layers with <code class="language-plaintext highlighter-rouge">(BN)</code> can nevertheless improve accuracy. The authors argue that this is due to an effectively larger learning rate.</li> </ul> <p>This blog post will summarize the development of weight decay specifically for <span style="font-family:monospace">Adam</span>. We try to shed some light on the following questions:</p> <ol> <li>What is the difference between <span style="font-family:monospace">Adam</span> and its weight decay version <span style="font-family:monospace">AdamW</span>? Does the existing literature give a clear answer to the question when (and why) <span style="font-family:monospace">AdamW</span> performs better?</li> <li>Is the weight decay mechanism of <span style="font-family:monospace">AdamW</span> just <em>one more trick</em> or can we actually motivate it from an optimization perspective?</li> <li>The last section is somewhat explorational: could we come up with different formulas for a weight decay version of <span style="font-family:monospace">Adam</span>? By doing so, we will see that <span style="font-family:monospace">AdamW</span> already combines several advantages for practical use.</li> </ol> <h3 id="notation">Notation</h3> <p>We denote by \(\alpha &gt; 0\) the initial learning rate. We use \(\eta_t &gt; 0\) for a learning rate schedule multiplier. By this, the effective learning rate in iteration \(t\) is \(\alpha \eta_t\). We use \(\lambda &gt; 0\) for the weight decay parameter.</p> <h2 id="adam">Adam</h2> <p><span style="font-family:monospace">Adam</span> uses an exponentially moving average (EMA) of stochastic gradients, typically denoted by \(m_t\), and of the elementwise squared gradients, denoted by \(v_t\).</p> <p>We denote with \(\hat m_t\) and \(\hat v_t\) the EMA estimates with bias correction (see <d-cite key="Kingma2015"></d-cite>), this means</p> \[\hat m_t = \frac{m_t}{1-\beta_1^t}, \quad \hat v_t = \frac{v_t}{1-\beta_2^t}\] <p>where \(\beta_1, \beta_2 \in [0,1)\). The update formula of <span style="font-family:monospace">Adam</span> is given by</p> \[w_t = w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.\] <p>How would <span style="font-family:monospace">Adam</span> handle regularization? The first approach to this was to simply add the regularization term \(\frac{\lambda}{2}\|w\|^2\) on top of the loss, do backpropagation and then compute the <span style="font-family:monospace">Adam</span> step as outlined above. This is usually referred to as <span style="font-family:monospace">AdamL2</span>. However, Loshchilov and Hutter <d-cite key="Loshchilov2019"></d-cite> showed that this can be suboptimal and one major contribution to alleviate this was the development of <span style="font-family:monospace">AdamW</span>.</p> <h2 id="adamw">AdamW</h2> <p>For training with \(\ell_2\)-regularization, Loshchilov and Hutter proposed <span style="font-family:monospace">AdamW</span> in 2019 <d-cite key="Loshchilov2019"></d-cite> as an alternative to <span style="font-family:monospace">AdamL2</span>. In the paper, the update formula is given as</p> \[\tag{AdamW} w_t = (1-\eta_t \lambda)w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.\] <p>While for <span style="font-family:monospace">Adam</span> several results for convex and nonconvex problems are established <d-cite key="Defossez2022, Reddi2018"></d-cite>, theoretical guarantees for <span style="font-family:monospace">AdamW</span> have been explored - to the best of our knowledge - only very recently <d-cite key="Anonymous2023"></d-cite>. Despite this, the method has enjoyed considerable practical success: for instance, <span style="font-family:monospace">AdamW</span> is implemented in the machine learning libraries Tensorflow <d-cite key="Abadi2015"></d-cite> and Pytorch <d-cite key="Paszke2019"></d-cite>. Another example is the <code class="language-plaintext highlighter-rouge">fairseq</code> library, developped by Facebook Research, which implements many SeqToSeq models. In their codebase, when <span style="font-family:monospace">Adam</span> is specified with weight decay, <span style="font-family:monospace">AdamW</span> is used by default (see <a href="https://github.com/facebookresearch/fairseq/blob/main/fairseq/optim/adam.py">here</a>).</p> <p>We summarize the empirical findings of <d-cite key="Loshchilov2019"></d-cite> as follows:</p> <ul> <li> <p><span style="font-family:monospace">AdamW</span> improves generalization as compared to <span style="font-family:monospace">AdamL2</span> for image classification tasks. In the paper, the authors use a ResNet model <d-cite key="He2016"></d-cite> for the CIFAR10 and Imagenet32 dataset.</p> </li> <li> <p>Another advantage of <span style="font-family:monospace">AdamW</span> is stated in the abstract of <d-cite key="Loshchilov2019"></d-cite>:</p> </li> </ul> <blockquote> We provide empirical evidence that our proposed modification decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam [...]. —Loshchilov and Hutter, 2019 </blockquote> <p>What the authors mean by <em>decoupling</em> is that if we plot the test accuracy as a heatmap of learning rate and weight decay, the areas with high accuracy are more rectangular; the best learing rate is not too sensitive to the choice of weight decay. We illustrate this conceptually in the plot below which is inspired by Figure 2 in <d-cite key="Loshchilov2019"></d-cite>. The advantage of a decoupled method is that if one of the two hyperparameters is changed, the optimal value for the other one might still be identical and does not need to be retuned - this could reduce a 2D grid search to two 1D line searches.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/heatmap-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/heatmap-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/heatmap-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/heatmap.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Fig. 1: Heatmap of the test accuracy (bright = good accuracy) depending on learning rate and weight decay parameter choice. </div> <p>When revisiting the literature on <span style="font-family:monospace">AdamW</span> we made an interesting practical observation: the <a href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html">Pytorch implementation</a> of <span style="font-family:monospace">AdamW</span> is actually slightly different to the algorithm proposed in the paper. In Pytorch, the following is implemented:</p> \[w_t = (1-\eta_t \alpha \lambda)w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.\] <p>The difference is that the decay factor in the code is \(1-\eta_t \alpha \lambda\) instead of \(1-\eta_t \lambda\) in the paper. Clearly, this is equivalent as we can simply reparametrize the weight decay factor \(\lambda\) to make up for this. However, as the default learning rate \(\alpha=0.001\) is rather small, this means that practicioners might need to choose rather high values of \(\lambda\) in order to get sufficiently strong decay. Moreover, this leaves a certain ambiguity when tuned values for \(\lambda\) are reported in the literature.</p> <h2 id="follow-up-work">Follow-up work</h2> <p>In a recent article, Zhuang et al. revisit the <span style="font-family:monospace">AdamW</span> method and try to explain its practical success <d-cite key="Zhuang2022"></d-cite>. One of their central arguments is that <span style="font-family:monospace">AdamW</span> is approximately equal to <span style="font-family:monospace">Adam</span> with a proximal update for \(\ell_2\)-regularization.</p> <p>Before explaining this in detail, we first want to summarize the empirical findings of <d-cite key="Zhuang2022"></d-cite>:</p> <ul> <li>When <code class="language-plaintext highlighter-rouge">(BN)</code> is <em>deactivated</em>, <span style="font-family:monospace">AdamW</span> achieves better generalization compared to <span style="font-family:monospace">AdamL2</span> for image classification with a standard ResNet architecture <d-cite key="He2016"></d-cite>.</li> <li>When <code class="language-plaintext highlighter-rouge">(BN)</code> is <em>activated</em>, the test accuracy of <span style="font-family:monospace">AdamW</span> and <span style="font-family:monospace">AdamL2</span> are on par. Moreover, the best accuracy is achieved for no weight decay, i.e. \(\lambda=0\).</li> </ul> <p>The second result is somewhat stunning as it seems to contradict the results in <d-cite key="Loshchilov2019"></d-cite>, which had shown that <span style="font-family:monospace">AdamW</span> generalizes better than <span style="font-family:monospace">AdamL2</span>.<d-footnote>It seems like the AdamW-paper also used (BN) in their experiments, see https://github.com/loshchil/AdamW-and-SGDW.</d-footnote></p> <p>Comparing the details of the experimental setups, we presume the following explanations for this:</p> <ul> <li> <p>The model that is trained in <d-cite key="Loshchilov2019"></d-cite> is slightly different as it uses a Shake-Shake-Image ResNet <d-cite key="He2016, Gastaldi2017"></d-cite>.</p> </li> <li> <p>From Figure 4 in <d-cite key="Loshchilov2019"></d-cite>, one can observe that the improvement in accuracy for the CIFAR-10 dataset becomes noticeable very late in the training (see also Section 4.3 in <d-cite key="Loshchilov2019"></d-cite>). Thus, depending on the number of epochs after which training is stopped, one can reach different conclusions.</p> </li> </ul> <h2 id="proxadam">ProxAdam</h2> <p>The paper by Zhuang et al. <d-cite key="Zhuang2022"></d-cite> does not only compare <span style="font-family:monospace">AdamL2</span> to <span style="font-family:monospace">AdamW</span> experimentally, but it also provides a mathematical motivation for weight decay. In order to understand this, we first need to introduce the <strong>proximal operator</strong>, a central concept of convex analysis.</p> <h3 id="a-short-introduction-to-proximal-operators">A short introduction to proximal operators</h3> <p>Proximal algorithms have been studied for decades in the context of (non-smooth) optimization, way before machine learning was a thing. The groundwork of this field has been laid by R. Tyrrell Rockafellar from the 1970’s onwards <d-cite key="Rockafellar1976,Rockafellar1998"></d-cite>. If \(\varphi: \mathbb{R}^n \to \mathbb{R}\) is convex then the proximal operator is defined as</p> \[\mathrm{prox}_\varphi(x) := \mathrm{argmin}_{z \in \mathbb{R}^n} \varphi(z) + \frac12 \|z-x\|^2.\] <p>For many classical regularization functions (e.g. the \(\ell_1\)-norm), the proximal operator can be computed in closed form. This makes it a key ingredient of optimization algorithms for regularized problems. Assume that we want to minimize the sum of a differentiable loss \(f\) and a convex regularizer \(\varphi\), i.e.</p> \[\min_{w \in \mathbb{R}^n} f(w) + \varphi(w).\] <p>The proximal gradient method in this setting has the update formula</p> \[w_{t} = \mathrm{prox}_{\alpha \varphi}\big(w_{t-1}- \alpha \nabla f(w_{t-1})\big),\] <p>where \(\alpha&gt;0\) is a step size (<em>aka</em> learning rate). An equivalent way of writing this (which will become useful later on) is<d-footnote>This can be proven using the definition of the proximal operator and completing the square.</d-footnote></p> \[\tag{1} w_{t} = \mathrm{argmin}_y \langle y-w_{t-1}, \nabla f(w_{t-1})\rangle + \varphi(y) + \frac{1}{2\alpha}\|y-w_{t-1}\|^2.\] <h3 id="weight-decay-as-a-proximal-operator">Weight decay as a proximal operator</h3> <p>For \(\ell_2\)-regularization \(\varphi(w) = \frac{\lambda}{2}\|w\|^2\), the proximal operator at \(w\) is given by \(\frac{1}{1+\lambda}w = (1-\frac{\lambda}{1+\lambda})w\). Based on this, the authors of <d-cite key="Zhuang2022"></d-cite> propose a proximal version of <span style="font-family:monospace">Adam</span> called <span style="font-family:monospace">ProxAdam</span>. It is given by</p> \[\tag{ProxAdam} w_t = \big(1- \frac{\lambda\eta_t}{1+\lambda\eta_t} \big)w_{t-1} - \frac{\eta_t \alpha}{1+\lambda\eta_t} \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.\] <p>Knowing this, we can now understand why <span style="font-family:monospace">AdamW</span> is approximately a proximal version of <span style="font-family:monospace">Adam</span>. Using the first-order Taylor-approximation \(\frac{ax}{1+bx}\approx ax\) for small \(x\), applied to the coefficients in front of \(w_{t-1}\) and \(\frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}\) gives the formula</p> \[w_t = (1-\eta_t \lambda)w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}\] <p>which is equal to <span style="font-family:monospace">AdamW</span>. The argument we just presented is exactly how <d-cite key="Zhuang2022"></d-cite> concludes that <span style="font-family:monospace">AdamW</span> \(\approx\) <span style="font-family:monospace">ProxAdam</span>.</p> <h3 id="changing-the-norm">Changing the norm</h3> <p>There is one more way of interpreting proximal methods. Let us begin with a simple example: Define the diagonal matrix \(D_t := \mathrm{Diag}(\epsilon + \sqrt{\hat v_t})\). Then, the <span style="font-family:monospace">Adam</span> update can be equivalently written<d-footnote>This can be proven by first-order optimality and solving for $w_t$. We will do a similar calculation further below.</d-footnote> as</p> \[w_t = \mathrm{argmin}_y \langle y-w_{t-1}, \hat m_t \rangle + \frac{1}{2\eta_t\alpha}\|y-w_{t-1}\|_{D_t}^2.\] <p>In other words, <span style="font-family:monospace">Adam</span> takes a proximal step of a linear function, but with the adaptive norm \(D_t\). This change in norm is what makes <span style="font-family:monospace">Adam</span> different from <span style="font-family:monospace">SGD</span> with (heavy-ball) momentum.</p> <p>The update formula of <span style="font-family:monospace">ProxAdam</span> can also be written as a proximal method:</p> \[\tag{P1} w_t = \mathrm{argmin}_y \langle y-w_{t-1}, \hat m_t \rangle + \frac{\lambda}{2\alpha}\|y\|_{D_t}^2 + \frac{1}{2 \eta_t \alpha}\|y-w_{t-1}\|_{D_t}^2.\] <p>In fact, the first-order optimality conditions of (P1) are</p> \[0 = \hat m_t + \frac{\lambda}{\alpha} D_t w_t + \frac{1}{\eta_t \alpha}D_t (w_t-w_{t-1}).\] <p>Solving for \(w_t\) (and doing simple algebra) gives</p> \[\tag{2} w_t = (1+\lambda \eta_t)^{-1}\big[w_{t-1} - \eta_t \alpha D_t^{-1} \hat m_t\big]\] <p>which is equal to <span style="font-family:monospace">ProxAdam</span>.</p> <p>What is slightly surprising here is the term \(\alpha^{-1}\|y\|_{D_t}^2\) in (P1) - we might have expected the regularization term to be used with the standard \(\ell_2\)-norm. This leads us to our final section.</p> <h2 id="adamw-is-scale-free"><span style="font-family:monospace">AdamW</span> is scale-free</h2> <p>As an alternative to (P1), we could replace \(\alpha^{-1}\|y\|_{D_t}^2\) by \(\|y\|^2\) and update</p> \[w_t = \mathrm{argmin}_y \langle y-w_{t-1}, \hat m_t \rangle + \frac{\lambda}{2}\|y\|^2 + \frac{1}{2\eta_t\alpha}\|y-w_{t-1}\|_{D_t}^2.\] <p>Again, setting the gradient of the objective to zero and solving for \(w_t\) we get</p> \[w_t = \big(\mathrm{Id} + \eta_t \lambda \alpha D_t^{-1}\big)^{-1} \big[w_{t-1} - \eta_t\alpha D_t^{-1} \hat m_t \big].\] <p>Comparing this to (2) we see that the second factor is the same, but the decay factor now also depends on \(D_t\) and \(\alpha\). Let us call this method <span style="font-family:monospace">AdamP</span>.</p> <p>Now the natural question is whether <span style="font-family:monospace">AdamP</span> or <span style="font-family:monospace">ProxAdam</span> (or <span style="font-family:monospace">AdamW</span> as its approximation) would be superior. One answer to this is that we would prefer a <em>scale-free</em> algorithm: with this we mean that if the loss function would be multiplied by a positive constant, we could still run the method with exactly the same parameters and obtain the same result. <span style="font-family:monospace">Adam</span> for example is scale-free and in <d-cite key="Zhuang2022"></d-cite> it is explained that <span style="font-family:monospace">ProxAdam</span>/<span style="font-family:monospace">AdamW</span> are, too. The reason for this is the following: looking at (P1) we see that if the loss is scaled by \(c&gt;0\), then \(\hat m_t\) and \(D_t\) are scaled by \(c\) (if we neglect the \(\epsilon\) in \(D_t\)). Hence, the objective in (P1) is multiplied by \(c\) which implies that <span style="font-family:monospace">ProxAdam</span> for \(\epsilon=0\) is invariant to scaling for the same values of \(\lambda,\alpha,\eta_t\). Now, for (P2) the story is different, as here the second term \(\frac{\lambda}{2}\|y\|^2\) is not scaled by \(c\), but the other terms are. We would need to rescale \(\lambda\) by \(c\) to obtain the identical update. As a consequence, <span style="font-family:monospace">AdamP</span> would <strong>not be scale-free</strong> and this makes it less attractive as a method. We should point out that scale-freeness is rather a practical advantage that requires less tuning when changing the model or dataset - it does not imply that the test accuracy would be different when both methods are tuned.</p> <p>To verify this, we ran a simple experiment on a ResNet20 for CIFAR10 with <code class="language-plaintext highlighter-rouge">(BN)</code> deactivated. For <span style="font-family:monospace">AdamW</span> (the <code class="language-plaintext highlighter-rouge">Pytorch</code> version) and <span style="font-family:monospace">AdamP</span> we tested the learning rates <code class="language-plaintext highlighter-rouge">[1e-3,1e-2,1e-1]</code> and weight decay <code class="language-plaintext highlighter-rouge">[1e-5,1e-4,1e-3,1e-2]</code>. From the plots below, we can see that both methods approximately achieve the same accuracy for the best configurations<d-footnote>The best configurations all have learning rate 1e-3.</d-footnote>. The only difference - in this very simple example - is that <span style="font-family:monospace">AdamP</span> seems to arrive at a model with smaller norm for the configurations with high accuracy (see right plot). Hence, its regularization seems to be stronger.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/resnet20val_score-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/resnet20val_score-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/resnet20val_score-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/resnet20val_score.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/resnet20model_norm-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/resnet20model_norm-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/resnet20model_norm-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/resnet20model_norm.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For the sake of completeness, we also add a <code class="language-plaintext highlighter-rouge">Pytorch</code> implementation of <span style="font-family:monospace">AdamP</span> in the <a href="#appendix">Appendix</a>.</p> <h2 id="summary">Summary</h2> <ul> <li> <p>Weight decay can be seen as a proximal way of handling \(\ell_2\)-regularization. Therefore, it is not a different <em>type</em> of regularization itself but rather a different <em>treatment</em> of regularization in the optimization method. As a consequence, <span style="font-family:monospace">AdamW</span> is an (almost) proximal version of <span style="font-family:monospace">Adam</span>.</p> </li> <li> <p>Whether or not weight decay brings advantages when used <em>together with</em> <code class="language-plaintext highlighter-rouge">(BN)</code> seems to depend on several factors of the model and experimental design. However, in all experiments we discussed here <span style="font-family:monospace">AdamW</span> performed better or at least on par to <span style="font-family:monospace">AdamL2</span>.</p> </li> <li> <p>The second conclusion suggests that proximal algorithms such as <span style="font-family:monospace">AdamW</span> seem to be favourable. Together with the scale-free property that we described in the final section, this makes <span style="font-family:monospace">AdamW</span> a robust method and explains its practical success.</p> </li> </ul> <p><a name="appendix"></a></p> <h2 id="appendix">Appendix</h2> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/krogh_snippet-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/krogh_snippet-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/krogh_snippet-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/krogh_snippet.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Fig. 2: Excerpt of the introduction in <d-cite key="Krogh1991"></d-cite>. </div> <p>Below you find a <code class="language-plaintext highlighter-rouge">Pytorch</code> implementation of <span style="font-family:monospace">AdamP</span>:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch.optim</span> <span class="kn">import</span> <span class="n">Optimizer</span>


<span class="k">class</span> <span class="nc">AdamP</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sa">r</span><span class="sh">"""</span><span class="s">
    Arguments:
        params (iterable): iterable of parameters to optimize or dicts defining
            parameter groups
        lr (float, optional): learning rate (default: 1e-3)
        betas (Tuple[float, float], optional): coefficients used for computing
            running averages of gradient and its square (default: (0.9, 0.999))
        eps (float, optional): term added to the denominator to improve
            numerical stability (default: 1e-8)
        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)
        
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span>
                 <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">lr</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Invalid learning rate: {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">eps</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Invalid epsilon value: {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">eps</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Invalid beta parameter at index 0: {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Invalid beta parameter at index 1: {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">weight_decay</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Invalid weight_decay value: {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">weight_decay</span><span class="p">))</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
                        <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">_init_lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>

        <span class="k">return</span>
   

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">closure</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Performs a single optimization step.

        Arguments:
            closure (callable, optional): A closure that reevaluates the model
                and returns the loss.
        </span><span class="sh">"""</span>
        
        <span class="k">if</span> <span class="n">closure</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">enable_grad</span><span class="p">():</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="nf">closure</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="sh">'</span><span class="s">params</span><span class="sh">'</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">grad</span>
                <span class="n">state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>

                <span class="c1"># State initialization
</span>                <span class="k">if</span> <span class="sh">'</span><span class="s">step</span><span class="sh">'</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
                    <span class="n">state</span><span class="p">[</span><span class="sh">'</span><span class="s">step</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="c1"># Exponential moving average of gradient values
</span>                    <span class="n">state</span><span class="p">[</span><span class="sh">'</span><span class="s">exp_avg</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>
                    <span class="c1"># Exponential moving average of squared gradient values
</span>                    <span class="n">state</span><span class="p">[</span><span class="sh">'</span><span class="s">exp_avg_sq</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>
                    
                <span class="n">exp_avg</span><span class="p">,</span> <span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="sh">'</span><span class="s">exp_avg</span><span class="sh">'</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="sh">'</span><span class="s">exp_avg_sq</span><span class="sh">'</span><span class="p">]</span>
                <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="sh">'</span><span class="s">betas</span><span class="sh">'</span><span class="p">]</span>

                <span class="n">state</span><span class="p">[</span><span class="sh">'</span><span class="s">step</span><span class="sh">'</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">bias_correction1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="o">**</span><span class="n">state</span><span class="p">[</span><span class="sh">'</span><span class="s">step</span><span class="sh">'</span><span class="p">]</span>
                <span class="n">bias_correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="o">**</span><span class="n">state</span><span class="p">[</span><span class="sh">'</span><span class="s">step</span><span class="sh">'</span><span class="p">]</span>

                
                <span class="c1"># Decay the first and second moment running average coefficient
</span>                <span class="n">exp_avg</span><span class="p">.</span><span class="nf">mul_</span><span class="p">(</span><span class="n">beta1</span><span class="p">).</span><span class="nf">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="n">beta1</span><span class="p">)</span>
                <span class="n">exp_avg_sq</span><span class="p">.</span><span class="nf">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">).</span><span class="nf">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="n">beta2</span><span class="p">)</span>
                <span class="n">D</span> <span class="o">=</span> <span class="p">(</span><span class="n">exp_avg_sq</span><span class="p">.</span><span class="nf">div</span><span class="p">(</span><span class="n">bias_correction2</span><span class="p">)).</span><span class="nf">sqrt</span><span class="p">().</span><span class="nf">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="sh">'</span><span class="s">eps</span><span class="sh">'</span><span class="p">])</span>

                <span class="n">lr</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="sh">'</span><span class="s">lr</span><span class="sh">'</span><span class="p">]</span>
                <span class="n">lmbda</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="sh">'</span><span class="s">weight_decay</span><span class="sh">'</span><span class="p">]</span>

                <span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">addcdiv_</span><span class="p">(</span><span class="n">exp_avg</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="n">lr</span><span class="o">/</span><span class="n">bias_correction1</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">lmbda</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">div_</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">lr</span><span class="o">*</span><span class="n">lmbda</span><span class="o">/</span><span class="n">D</span><span class="p">)</span> <span class="c1"># adaptive weight decay
</span>
            

        <span class="k">return</span> <span class="n">loss</span></code></pre></figure>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Weight decay is among the most important tuning parameters to reach high accuracy for large-scale machine learning models. In this blog post, we revisit AdamW, the weight decay version of Adam, summarizing empirical findings as well as theoretical motivations from an optimization perspective.]]></summary></entry><entry><title type="html">Autoregressive Renaissance in Neural PDE Solvers</title><link href="https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/autoregressive-neural-pde-solver/" rel="alternate" type="text/html" title="Autoregressive Renaissance in Neural PDE Solvers"/><published>2022-12-01T00:00:00+00:00</published><updated>2022-12-01T00:00:00+00:00</updated><id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/autoregressive-neural-pde-solver</id><content type="html" xml:base="https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/autoregressive-neural-pde-solver/"><![CDATA[<h2 id="introduction">Introduction</h2> <blockquote> Improving PDE solvers has trickle down benefits to a vast range of other fields. </blockquote> <p>Partial differential equations (PDEs) play a crucial role in modeling complex systems and understanding how they change over time and in space.</p> <p>They are used across physics and engineering, modeling a wide range of physical phenomena like heat transfer, sound waves, electromagnetism, and fluid dynamics, but they can also be used in finance to model the behavior of financial markets, in biology to model the spread of diseases, and in computer vision to model the processing of images.</p> <p>They are particularly interesting in deep learning!</p> <ol> <li><span style="color:#9444e2;">Neural networks can be used to solve complex PDEs.</span></li> <li><span style="color:#9444e2;">Embedding knowledge of a PDE into a neural network can help it generalize better and/or use less data</span></li> <li><span style="color:#9444e2;">PDEs can help explain and/or interpret neural networks.</span></li> </ol> <p>Despite their long history, dating back to equations first formalized by Euler over 250 years ago, finding numerical solutions to PDEs continues to be a challenging problem.</p> <p>The recent advances in machine learning and artificial intelligence have opened up new possibilities for solving PDEs in a more efficient and accurate manner. These developments have the potential to revolutionize many fields, leading to a better understanding of complex systems and the ability to make more informed predictions about their behavior.</p> <p>The background and problem set up precedes a brief look into classical and neural solvers, and finally discusses the message passing neural PDE solver (MP-PDE) introduced by Brandstetter et al. <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>.</p> <h2 id="background">Background</h2> <h3 id="lets-brush-up-on-the-basics">Let's brush up on the basics…</h3> <p><em>The notation and definitions provided match those in the paper for consistency, unless otherwise specified.</em></p> <div> <p> Ordinary differential equations (ODEs) describe how a function changes with respect to a <span style="color:#9444e2">single independent variable</span> and its derivatives. In contrast, PDEs are mathematical equations that describe the behavior of a dependent variable as it changes with respect to <span style="color:#9444e2">multiple independent variables</span> and their derivatives. </p> <p> Formally, for one time dimension and possibly multiple spatial dimensions denoted by \(\textbf{x}=[x_{1},x_{2},x_{3},\text{...}]^{\top} \in \mathbb{X}\), a general (temporal) PDE may be written as </p> <p> $$\partial_{t}\textbf{u}= F\left(t, \textbf{x}, \textbf{u},\partial_{\textbf{x}}\textbf{u},\partial_{\textbf{xx}}\textbf{u},\text{...}\right) \qquad (t,\mathbf{x}) \in [0,T] \times \mathbb{X}$$ </p> <ul> <li>Initial condition: \(\mathbf{u}(0,\mathbf{x})=\mathbf{u}^{0}(\mathbf{x})\) for \(\mathbf{x} \in \mathbb{X}\)</li> <li>Boundary conditions: \(B[ \mathbf{u}](t,x)=0\) for \((t,\mathbf{x}) \in [0,T] \times \partial \mathbb{X}\)</li> </ul> </div> <div class="fake-img l-gutter"> <p> Many equations are solutions to such PDEs alone. For example, the wave equation is given by \(\partial_{tt}u = \partial_{xx}u\). You will find that any function in the form \(u(x,t)=F(x-ct)+\) \(G(x+ct)\) is a potential solution. Initial conditions are used to specify how a PDE "starts" in time, and boundary conditions determine the value of the solution at the boundaries of the region where the PDE is defined. </p> </div> <details><summary>Types of boundary conditions</summary> Dirichlet boundary conditions prescribe a fixed value of the solution at a particular point on the boundary of the domain. Neumann boundary conditions, on the other hand, prescribe the rate of change of the solution at a particular point on the boundary. There are also mixed boundary conditions, which involve both Dirichlet and Neumann conditions, and Robin boundary conditions, which involve a linear combination of the solution and its derivatives at the boundary. </details> <p><br/></p> <div class="l-body-outset"> <iframe src="/deep-learning-blog.github.io/assets/html/2022-12-01-autoregressive-neural-pde-solver/slider.html" frameborder="0" scrolling="no" height="750px" width="100%"></iframe> </div> <div class="caption"> Example of the wave equation PDE \(\partial^{2}_{t}u = c^{2}\partial^{2}_ {\mathbf{x}}u\). Drag the slider to watch it evolve in time! </div> <p>The study of PDEs is in itself split into many broad fields. Briefly, these are two other important properties in addition to the initial and boundary conditions:</p> <details><summary>Linearity</summary> <ul> <li>Linear: the highest power of the unknown function appearing in the equation is one (i.e., a linear combination of the unknown function and its derivatives)</li> <li>Nonlinear: the highest power of the unknown function appearing in the equation is greater than one</li> </ul> </details> <p><br/></p> <details><summary>Homogeneity</summary> <ul> <li>Homogeneous: PDEs with no constant terms (i.e., the right-hand side is equal to zero)</li> <li>Inhomogeneous: PDEs with a non-zero constant term on the right-hand side</li> </ul> </details> <p><br/></p> <p>PDEs can be either linear or nonlinear, homogeneous or inhomogeneous, and can contain a combination of constant coefficients and variable coefficients. They can also involve a variety of boundary conditions, such as Dirichlet, Neumann, and Robin conditions, and can be solved using analytical, numerical, or semi-analytical methods <d-cite key="straussPartialDifferentialEquations2007"></d-cite>.</p> <hr style="width:40%"/> <p>Brandstetter et al. <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite> follow precedence set by Li et al. <d-cite key="liFourierNeuralOperator2021"></d-cite> and Bar-Sinai et al. <d-cite key="bar-sinaiLearningDatadrivenDiscretizations2019"></d-cite>to focus on <span style="color:#9444e2;">PDEs written in conservation form</span>:</p> <p style="text-align:center;"> \(\partial_{t} \mathbf{u} + \nabla \cdot \mathbf{J}(\mathbf{u}) = 0\) </p> <ul> <li><p>\(J\) is the flux, or the amount of some quantity that is flowing through a region at a given time</p> </li> <li><p>\(\nabla \cdot J\) is the divergence of the flux, or the amount of outflow of the flux at a given point</p> </li> </ul> <p>Additionally, they consider <span style="color:#9444e2;">Dirichlet and Neumann</span> boundary conditions.</p> <h3 id="solving-pdes-the-classical-way">Solving PDEs the classical way</h3> <p>A brief search in a library will find numerous books detailing how to solve various types of PDEs. </p> <details><summary>Analytical methods: an exact solution to a PDE can be found by mathematical means <d-cite key="straussPartialDifferentialEquations2007"></d-cite>.</summary><br/> <ul> <li>Separation of Variables<ul> <li>This method involves expressing the solution as the product of functions of each variable, and then solving each function individually. It is mainly used for linear PDEs that can be separated into two or more ordinary differential equations.</li> </ul> </li> <li>Green&#39;s Functions<ul> <li>This method involves expressing the solution in terms of a Green&#39;s function, which is a particular solution to a homogeneous equation with specified boundary conditions.</li> </ul> </li> </ul> </details> <p><br/></p> <details><summary>Semi-analytical methods: an analytical solution is combined with numerical approximations to find a solution <d-cite key="bartelsNumericalApproximationPartial"></d-cite>.</summary><br/> <ul> <li>Perturbation methods<ul> <li>This method is used when the solution to a PDE is close to a known solution or is a small deviation from a known solution. The solution is found by making a perturbation to the known solution and solving the resulting equation analytically.</li> </ul> </li> <li>Asymptotic methods<ul> <li>In this method, the solution is represented as a series of terms that are solved analytically. The solution is then approximated by taking the leading terms of the series.</li> </ul> </li> </ul> </details> <p><br/></p> <blockquote> Very few PDEs have analytical solutions, so numerical methods have been developed to approximate PDE solutions over a wider range of potential problems. </blockquote> <h4 id="numerical-methods">Numerical Methods</h4> <p>Often, approaches for temporal PDEs follow the <span style="color:#9444e2;">method of lines (<abbr title="method of lines">MOL</abbr>)</span>.</p> <p>Every point of the discretization is then thought of as a separate ODE evolving in time, enabling the use of ODE solvers such as Runge-Kutta methods.</p> <details><summary>1. Discretizing the problem</summary><br/> <p> In the most basic case (<span style="color:#9444e2;">a regular grid</span>), arbitrary spatial and temporal resolutions \(\mathbf{n_{x}}\) and \(n_{t}\) can be chosen and thus used to create a grid where \(\mathbf{n_{x}}\) is a vector containing a resolution for each spatial dimension. </p> <hr style="width:40%"/> <p> The domain may also be <span style="color:#9444e2;">irregularly sampled, resulting in a grid-free discretization</span>. This is often the case with real-world data that comes from scattered sensors, for example. </p> <p><abbr title="finite difference method">FDM</abbr> or any other time discretization technique can be used to discretize the time domain. </p> <p> One direction of ongoing research seeks to determine discretization methods which can result in more efficient numerical solvers (for example, take larger steps in flatter regions and smaller steps in rapidly changing regions). </p> </details> <p><br/></p> <details><summary>2. Estimating the spatial derivatives</summary><br/> <p> A popular choice when using a gridded discretization is the <span style="color:#9444e2;">finite difference method (FDM)</span>. Spatial derivative operators are replaced by a stencil which indicates how values at a finite set of neighboring grid points are combined to approximate the derivative at a given position. This stencil is based on the Taylor series expansion. </p> <p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/fdm_animation.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/fdm_animation.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/fdm_animation.gif-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/fdm_animation.gif" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </p> <div class="caption"> Credits: Augusto Peres, Inductiva <d-cite key="HeatHeatEquation"></d-cite>. </div> <hr style="width:40%"/> <p> The <span style="color:#9444e2;">finite volume method (FVM)</span> is another approach which works for irregular geometries. Rather than requiring a grid, the computation domain can be divided into discrete, non-overlapping control volumes used to compute the solution for that portion <d-cite key="bartelsNumericalApproximationPartial"></d-cite>. </p> <p> While this method <span style="color:#9444e2;">only works for conservation form equations</span>, it can handle complex problems with irregular geometries and fluxes that are difficult to handle with other numerical techniques such as the <abbr title="finite difference method">FDM</abbr>. </p> <hr style="width:40%"/> <p> In the <span style="color:#9444e2;">pseudospectral method (PSM)</span>, PDEs are solved pointwise in physical space by using basis functions to approximate the spatial derivatives <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. </p> <p> It is well-suited for solving problems with <span style="color:#9444e2;">smooth solutions and periodic boundary conditions</span>, but its performance drops for irregular or non-smooth solutions. </p> </details> <p><br/></p> <details><summary>3. Time updates</summary><br/> The resulting problem is a set of temporal ODEs which can be solved with classical ODE solvers such as any member of the Runge-Kutta method family. </details> <p><br/></p> <h4 id="limitations-of-classical-methods">Limitations of Classical Methods</h4> <p>The properties of a PDE, such as its order, linearity, homogeneity, and boundary conditions, determine its solution method. <span style="color:#9444e2;">Different methods have been developed based on the different properties and requirements of the problem at hand.</span> Brandstetter at al. categorizes these requirements into the following <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>:</p> <div> <table> <thead> <tr> <th>User</th> <th>Structural</th> <th>Implementational</th> </tr> </thead> <tbody> <tr> <td>Computation efficiency, computational cost, accuracy, guarantees (or uncertainty estimates), generalization across PDEs</td> <td>Spatial and temporal resolution, boundary conditions, domain sampling regularity, dimensionality</td> <td>Stability over long rollouts, preservation of invariants</td> </tr> </tbody> </table> <p> The countless combinations of requirements resulted in what Bartels defines as a <span style="color:#9444e2;">splitter field</span> <d-cite key="bartelsNumericalApproximationPartial"></d-cite>: a specialized classical solver is developed for each sub-problems, resulting in many specialized tools rather than a single one. </p> <p> These methods, while effective and mathematically proven, often come at high computation costs. Taking into account that PDEs often exhibit chaotic behaviour and are sensitive to any changes in their parameters, <span style="color:#ff4f4b;">re-running a solver every time a coefficient or boundary condition changes in a single PDE can be computationally expensive</span>. </p> </div> <div class="fake-img l-gutter"> <p> Courant–Friedrichs–Lewy (CFL) condition </p> <p> <it>The maximum time step size should be proportional to the minimum spatial grid size.</it> </p> <p> According to this condition, as the number of dimensions increases, the size of the temporal step must decrease and therefore numerical solvers become very slow for complex PDEs. </p> </div> <h3 id="neural-solvers">Neural Solvers</h3> <p> Neural solvers offer some very desirable properties that may serve to unify some of this splitter field. Neural networks can <span style="color:#9444e2;">learn and generalize to new contexts</span> such as different initial/boundary conditions, coefficients, or even different PDEs entirely <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. They can also circumvent the CFL condition, making them a promising avenue for solving highly complex PDEs such as those found in weather prediction. </p> <h4 id="neural-operator-methods">Neural operator methods</h4> <p>Neural operator methods <span style="color:#9444e2;">model the solution of a PDE as an operator that maps inputs to outputs</span>. The problem is set such that a neural operator \(\mathcal{M}\) satisfies \(\mathcal{M}(t,\mathbf{u}^{0}) = \mathbf{u}(t)\) where \(\mathbf{u}^{0}\) are the initial conditions <d-cite key="luDeepONetLearningNonlinear2021, brandstetterMessagePassingNeural2022a"></d-cite>.</p> <p> One of the current state-of-the-art models is the <span style="color:#9444e2;">Fourier Neural Operator (FNO)</span> <d-cite key="liFourierNeuralOperator2021"></d-cite>. It operates within Fourier space and takes advantage of the convolution theorem to place the integral kernel in Fourier space as a convolutional operator. </p> <div> <p> These global integral operators (implemented as Fourier space convolutional operators) are combined with local nonlinear activation functions, resulting in an architecture which is <span style="color:#9444e2;">highly expressive yet computationally efficient, as well as being resolution-invariant</span>. </p> <p> While the vanilla <abbr title="Fourier neural operator">FNO</abbr> required the input function to be defined on a grid due to its reliance on the FFT, further work developed mesh-independent variations as well <d-cite key="kovachkiNeuralOperatorLearning2022"></d-cite>. </p> </div> <div class="fake-img l-gutter"> <p> Convolution Theorem </p> <p> The Fourier transform of the convolution of two signals is equal to the pointwise product of their individual Fourier transforms </p> </div> <p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/FNO-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/FNO-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/FNO-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/FNO.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </p> <div class="caption"> <abbr title="Fourier neural operator">FNO</abbr> architecture. For more details, see <a href="https://zongyi-li.github.io/blog/2020/fourier-pde/">this blogpost</a>. Credits: Li et al. <d-cite key="liFourierNeuralOperator2021"></d-cite>. </div> <p> Neural operators are able to operate on multiple domains and can be completely data-driven. </p> <p> However, these models <span style="color:#ff4f4b;">do not tend to predict out-of-distribution \(t\)</span> and are therefore limited when dealing with temporal PDEs. Another major barrier is their relative <span style="color:#ff4f4b;">lack of interpretability and guarantees</span> compared to classical solvers. </p> <h4 id="autoregressive-methods">Autoregressive methods</h4> <p>While neural operator methods directly mapped inputs to outputs, <span style="color:#9444e2;">autoregressive methods take an iterative approach instead</span>. For example, iterating over time results in a problem such as \(\mathbf{u}(t+\Delta t) = \mathcal{A}(\Delta t, \mathbf{u}(t))\) where \(\mathcal{A}\) is some temporal update <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>.</p> <div class="l-body-outset"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/rnn-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/rnn-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/rnn-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/rnn.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/wavenet.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/wavenet.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/wavenet.gif-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/wavenet.gif" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Similarly to <abbr title="recurrent neural networks">RNN</abbr>s (left), autoregressive models take previous time steps to predict the next time step. However, autoregressive models (right) are entirely feed-forward and take the previous predictions as inputs rather than storing them in some hidden state. Credits: RNN diagram from Colah's Blog <d-cite key="UnderstandingLSTMNetworks"></d-cite>, WaveNet from Deepmind Blog <d-cite key="WaveNetGenerativeModel"></d-cite> </div> </div> <p>Three autoregressive works mentioned by Brandstetter et al. are hybrid methods which use neural networks to predict certain parameters for finite volume, multigrid, and iterative finite elements methods. <span style="color:#9444e2;">All three retain a (classical) computation grid which makes them somewhat interpretable</span> <d-cite key="bar-sinaiLearningDatadrivenDiscretizations2019, greenfeldLearningOptimizeMultigrid2019a, hsiehLearningNeuralPDE2019"></d-cite>.</p> <div class="fake-img l-gutter"> <p> Other autoregressive models include PixelCNN for images, WaveNet for audio, and the Transformer for text. </p> </div> <p>However, autoregressive models have not gained the acclaim seen by neural operators as a whole.</p> <p>This is on one hand due to their <span style="color:#ff4f4b;">limitations in generalization</span>. In Hsieh et al.’s case, an existing numerical method must be used to craft a complementary neural iterator <d-cite key="hsiehLearningNeuralPDE2019"></d-cite>.</p> <p>Another major concern is the <span style="color:#ff4f4b;">accumulation of error</span>, which is particularly detrimental for PDE problems that often exhibit chaotic behavior <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>.</p> <h2 id="message-passing-neural-pde-solver-mp-pde">Message Passing Neural PDE Solver (MP-PDE)</h2> <p>Brandstetter et al. propose a <span style="color:#9444e2;">fully neural PDE solver which capitalizes on neural message passing</span>. The overall architecture is laid out below, consisting of an <abbr title="multilayer perceptron">MLP</abbr> encoder, a <abbr title="graph neural network">GNN</abbr> processor, and a CNN decoder.</p> <div class="l-body-outset"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/MP-PDE-Solver-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/MP-PDE-Solver-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/MP-PDE-Solver-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/MP-PDE-Solver.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Overall MP-PDE architecture. Credits: Brandstetter et al. <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. </div> </div> <p>At its core, this model is autoregressive and thus faces the same challenge listed above. Two key contributions of this work are the <span style="color:#9444e2;">pushforward trick and temporal bundling which mitigate the potential butterfly effect of error accumulation</span><d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. The network itself, being fully neural, is capable of generalization across many changes as well.</p> <h3 id="the-pushforward-trick-and-temporal-bundling">The Pushforward Trick and Temporal Bundling</h3> <div class="l-body-outset"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/pushforward3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/pushforward3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/pushforward3-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/pushforward3.jpg" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Pushforward trick compared to one-step and unrolled training. Credits: Brandstetter et al. <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. </div> </div> <p>During testing, the model uses current time steps (first from data, then <span style="color:#9444e2;">from its own predictions</span>) to approximate the next time step.</p> <p>This results in a distribution shift problem because the inputs are no longer solely from ground truth data: <span style="color:#9444e2;">the distribution learned during training will always be an approximation of the true data distribution</span>. The model will appear to overfit to the one-step training distribution and perform poorly the further it continues to predict.</p> <p>An adversarial-style stability loss is added to the one-step loss so that the training distribution is brought closer to the test time distribution <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>:</p> <details><summary style="text-align:center;color:black;"> \(L_{\text{one-step}} =\) <span style="color:#23a15c;">\(\mathbb{E}_{k}\)</span> <span style="color:#928b54;">\(\mathbb{E}_{\mathbf{u^{k+1}|\mathbf{u^{k},\mathbf{u^{k} \sim p_{k}}}}}\)</span> \([\) <span style="color:#5588e0;">\(\mathcal{L}\)</span> \((\) <span style="color:#9444e2;">\(\mathcal{A}(\mathbf{u}^{k})\)</span> \(,\) <span style="color:#46b4af;">\(\mathbf{u}^{k+1}]\)</span> </summary> <p> The <span style="color:#5588e0;">loss function</span> is used to evaluate the difference between the <span style="color:#9444e2;">temporal update</span> and the <span style="color:#46b4af;">expected next state</span>, and the overall one-step loss is calculated as the expected value of this loss over <span style="color:#23a15c;">all time-steps</span> and <span style="color:#928b54;">all possible next states</span>. </p> </details> <p><br style="line-height:5px"/></p> <p style="text-align:center;"> \(L_{\text{stability}} = \mathbb{E}_{k}\mathbb{E}_{\mathbf{u^{k+1}|\mathbf{u^{k},\mathbf{u^{k} \sim p_{k}}}}}[\mathbb{E}_{\epsilon | \mathbf{u}^{k}} [\mathcal{L}(\mathcal{A}(\mathbf{u}^{k}+\) <span style="color:#faad18;">\(\epsilon\)</span> \()),\mathbf{u}^{k+1}]]\) </p> <p style="text-align:center;"> \(L_{\text{total}} = L_{\text{one-step}} + L_{\text{stability}}\) </p> <p> The stability loss is largely based off the one-step loss, but now assumes that the temporal update uses <span style="color:#faad18;">noisy data</span>. </p> <p> The pushforward trick lies in the choice of <span style="color:#faad18;">\(\epsilon\)</span> such that \(\mathbf{u}^{k}+\epsilon = \mathcal{A}(\mathbf{u}^{k-1})\), similar to the test time distribution. Practically, it is implemented to be <span style="color:#9444e2;">noise from the network itself</span> so that as the network improves, the loss decreases. </p> <p> Necessarily, the noise of the network must be known or calculated to implement this loss term. So, <span style="color:#9444e2;">the model is unrolled for 2 steps</span> but only backpropagated over the most recent unroll step, which already has the neural network noise <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. </p> <p> While the network could be unrolled during training, this not only slows the training down but also might result in the network learning shortcuts across unrolled steps. </p> <p><strong>Temporal bundling</strong></p> <div class="row mt-3"> <div class="col-8"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/NN-AR-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/NN-AR-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/NN-AR-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/NN-AR.jpg" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/temporalbundling-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/temporalbundling-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/temporalbundling-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/temporalbundling.jpg" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Temporal bundling compared to neural operators and autoregressive models. Credits: Brandstetter et al. <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. </div> <p>This trick complements the previous by <span style="color:#9444e2;">reducing the amount of times the test time distribution changes</span>. Rather than predicting a single value at a time, the MP-PDE predicts multiple time-steps at a time, as seen above <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>.</p> <h3 id="network-architecture">Network Architecture</h3> <p><abbr title="graph neural network">GNN</abbr>s have been used as PDE solvers in a variety of works <d-cite key="liNeuralOperatorGraph2020, eliasofPdegcnNovelArchitectures2021, iakovlevLearningContinuoustimePDEs2021"></d-cite>; however, in this implementation, <span style="color:#9444e2;">links can be drawn directly from the <abbr title="method of lines">MOL</abbr> to each component of the network architecture centering around the use of a message passing algorithm.</span></p> <table> <thead> <tr> <th>Classical Numerical Method</th> <th>MP-PDE Network Component</th> </tr> </thead> <tbody> <tr> <td>Partitioning the problem onto a grid</td> <td>Encoder <br/><em>Encodes a vector of solutions into node embeddings</em></td> </tr> <tr> <td>Estimating the spatial derivatives</td> <td>Processor <br/><em>Estimates spatial derivatives via message passing</em></td> </tr> <tr> <td>Time updates</td> <td>Decoder <br/><em>Combines some representation of spatial derivatives smoothed into a time update</em></td> </tr> </tbody> </table> <ol> <li>Encoder <p> The encoder is implemented as a two-layer <abbr title="multilayer perceptron">MLP</abbr> which computes an embedding for each node \(i\) to cast the data to a <span style="color:#9444e2;">non-regular integration grid</span>: </p> <details><summary style="text-align:center;color:black;">\(\mathbf{f}_{i}^{0} = \epsilon^{v}([\mathbf{u}_{i}^{k-K:k},\mathbf{x}_{i},t_{k},\theta_{PDE}])\) </summary> where \(\mathbf{u}_{i}^{k-K:k}\) is a vector of previous solutions (the length equaling the temporal bundle length), \(\mathbf{x}_{i}\) is the node's position, \(t_{k}\) is the current timestep, and \(\theta_{PDE}\) holds equation parameters. </details> </li> <li> Processor <p> The node embeddings from the encoder are then used in a message passing <abbr title="graph neural network">GNN</abbr>. <a id="spatialderivative" style="text-decoration:none;">The message passing algorithm, which approximates spatial derivatives, is run \(M\) steps using the following updates:</a> </p> <details><summary style="text-align:center;color:black;"> \(\text{edge } j \to i \text{ message:} \qquad \mathbf{m}_{ij}^{m} =\) <span style="color:#ae46b4;">\(\phi\)</span> \((\) <span style="color:#b4a546;">\(\mathbf{f}_{i}^{m}, \mathbf{f}_{j}^{m},\)</span> <span style="color:steelblue;">\(\mathbf{u}_{i}^{k-K:k}-\mathbf{u}_{j}^{k-K:k}\)</span>, <span style="color:#6546b4;">\(\mathbf{x}_{i}-\mathbf{x}_{j}\)</span>, <span style="color:#46b4af;">\(\theta_{PDE}\)</span> \())\) </summary> The <span style="color:#6546b4;">difference in spatial coordinates</span> helps enforce translational symmetry and, combined with the <span style="color:steelblue;">difference in node solutions</span>, relates the message passing to a local difference operator. The addition of the <span style="color:#46b4af;">PDE parameters</span> is motivated by considering what the MP-PDE should generalize over: by adding this information in multiple places, flexibility can potentially be learned since all this information (as well as the <span style="color:#b4a546;">node embeddings</span>) is fed through <span style="color:#ae46b4;">a two-layer <abbr title="multilayer perceptron">MLP</abbr></span>. In addition, the solution of a PDE at any timestep must respect the boundary condition (the same as in classical methods for BVPs), so adding the <span style="color:#46b4af;">PDE parameters</span> in the edge update provides knowledge of the boundary conditions to the neural solver. </details> <br/> <details><summary style="text-align:center;color:black;"> \(\text{node } i \text{ update:} \qquad\) <span style="color:#ff4f4b;">\(\mathbf{f}_{i}^{m+1}\)</span> \(=\) <span style="color:#928b54;">\(\psi\)</span> \((\) <span style="color:#5588e0;">\(\mathbf{f}^{m}_{i}\)</span>, <span style="color:#722e4e;">\(\sum_{j \in \mathcal{N}(i)} \mathbf{m}_{ij}^{m}\)</span>, <span style="color:#46b4af;">\(\theta_{PDE}\)</span> \()\) </summary> The <span style="color:#ff4f4b;">future node embedding</span> is updated using <span style="color:#5588e0;">the current node embedding</span>, <span style="color:#722e4e;">the aggregation of all received messages</span>, and (again) the <span style="color:#46b4af;">PDE parameters</span>. This information is also fed through <span style="color:#928b54;">a two-layer <abbr title="multilayer perceptron">MLP</abbr></span>. </details><br/> <p> Bar-Sinai et al. explores the relationship between <abbr title="finite difference method">FDM</abbr> and <abbr title="finite volume method">FVM</abbr> as used in the method of lines <d-cite key="bar-sinaiLearningDatadrivenDiscretizations2019"></d-cite>. In both methods, the \(n^{th}\) order derivative at a point \(x\) is approximated by </p> <p style="text-align:center;"> \(\partial^{(n)}_{x}u \approx \sum_{i} a^{(n)}_{i} u_{i}\) </p> <p> for some precomputed coefficients \(a^{(n)}_{i}\). <span style="color:#9444e2;">The right hand side parallels the message passing scheme</span>, which aggregates the local difference (<span style="color:steelblue;">\(\mathbf{u}_{i}^{k-K:k}-\mathbf{u}_{j}^{k-K:k}\)</span> in the edge update) and other (learned) embeddings over neighborhoods of nodes. </p> <p> This relationship gives an intuitive understanding of the message passing <abbr title="graph neural network">GNN</abbr>, which mimics <abbr title="finite difference method">FDM</abbr> for a single layer, <abbr title="finite volume method">FVM</abbr> for two layers, and <abbr title="Weighted Essentially Non-Oscillatory (5th order)">WENO5</abbr> for three layers <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. <abbr title="Weighted Essentially Non-Oscillatory (5th order)">WENO5</abbr> is a numerical interpolation scheme used to reconstruct the solution at cell interfaces in <abbr title="finite volume method">FVM</abbr>. </p> <p> While the interpretation is desirable, how far this holds in the actual function of the <abbr title="message passing graph neural network">MP-GNN</abbr> is harder to address. The concepts of the nodes as integration points and messages as local differences break down as the nodes and edges update. In addition, the furthest node that contributes a message from for any point is at \(n\) edges away for the \(n^{th}\) layer (or a specified limit). This results in a very coarse and potentially underinformed approximation for the first layer which is then propagated to the next layers. However, both the updates use two layer <abbr title="multilayer perceptron">MLP</abbr>s which (although abstracting away from their respective interpretations) may in effect learn optimal weightings to counterbalance this. </p> </li> <li> Decoder <p> The approximated spatial derivatives are then <span style="color:#9444e2;">combined and smoothed using a CNN</span> which outputs a bundle of next time steps (recall temporal bundling) \(\mathbf{d}_{i}\). The solution is then updated: </p> <p style="text-align:center;"> \(\mathbf{u}^{k+l}_{i} = u^{k}_{i} + (t_{k+l}-t_{k})\mathbf{d}^{l}_{i}\) </p> <p> Some precedence is seen, for example, in classical linear multistep methods which (though effective) face stability concerns. Since the CNN is adaptive, it appears that it avoids this issue <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. </p> </li> </ol> <h3 id="results">Results</h3> <details><summary>Quantitative measures: accumulated error, runtime</summary> <p> Accumulated error: \(\frac{1}{n_{x}} \sum_{x,t} MSE\) </p> <p> Runtime (s): Measured time taken to run for a given number of steps. </p> </details> <blockquote> As a general neural PDE solver, the <abbr title="message passing graph neural network">MP-GNN</abbr> surpasses even the current state-of-the-art <abbr title="Fourier neural operator">FNO</abbr>. </blockquote> <p>For example, after training a neural model and setting up an instance of <abbr title="method of lines">MOL</abbr>, this is a brief comparison of how they can generalize without re-training.</p> <table> <thead> <tr> <th>Generalization to...</th> <th><abbr title="message passing graph neural network">MP-GNN</abbr></th> <th><abbr title="Fourier neural operator">FNO</abbr></th> <th>Classical (<abbr title="method of lines">MOL</abbr>)</th> </tr> </thead> <tbody> <tr> <td>New PDEs</td> <td>Yes</td> <td>No</td> <td>No</td> </tr> <tr> <td>Different resolutions</td> <td>Yes</td> <td>Yes</td> <td>No (unless downsampling)</td> </tr> <tr> <td>Changes in PDE parameters</td> <td>Yes</td> <td>Yes</td> <td>Sometimes</td> </tr> <tr> <td>Non-regular grids</td> <td>Yes</td> <td>Some</td> <td>Yes (dependent on implementation)</td> </tr> <tr> <td>Higher dimensions</td> <td>Yes</td> <td>No</td> <td>No</td> </tr> </tbody> </table> <div class="l-body-outset"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/shock_formation-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/shock_formation-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/shock_formation-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/shock_formation.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Demonstration of shock formation using MP-PDE from different training data resolutions. Credits: Brandstetter et al. <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. </div> </div> <p>This experiment exemplifies the MP-PDE’s ability to model shocks (where both the <abbr title="finite difference method">FDM</abbr> and PSM methods fail) across multiple resolutions. Even at a fifth of the resolution of the ground truth, both the small and large shocks are captured well.</p> <div class="l-body-outset"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/2dshock-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/2dshock-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/2dshock-1400.webp"/> <img src="/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/2dshock.jpg" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Demonstration of shock formation using MP-PDE from different training data resolutions. Credits: Brandstetter et al. <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. </div> </div> <p>The same data is displayed in 2D to show the time evolution. After about 7.5s, the error accumulation is large enough to visibly diverge from the ground truth. The predictions become unreliable due to error accumulation.</p> <p>In practice, this survival time should be empirically found (as seen here) to determine for how long the solution is reliable. However, the ground truth would be needed for comparison, rendering this as another chicken-egg problem.</p> <table> <thead> <tr> <th colspan="2"></th> <th colspan="4" style="border-left:1px solid lightgrey;">Accumulated Error</th> <th colspan="2" style="border-left:1px solid lightgrey;">Runtime [s]</th> </tr> </thead> <tbody> <tr> <td colspan="2"> \(\quad (n_{t},n_{x})\) </td> <td style="border-left:1px solid lightgrey;">WENO5</td> <td>FNO-RNN</td> <td style="border-left:1px solid lightgrey;">FNO-PF</td> <td>MP-PDE</td> <td style="border-left:1px solid lightgrey;">WENO5</td> <td>MP-PDE</td> </tr> <tr> <td><b>E1</b></td> <td>(250,100)</td> <td style="border-left:1px solid lightgrey;">2.02</td> <td>11.93</td> <td style="border-left:1px solid lightgrey;">0.54</td> <td>1.55</td> <td style="border-left:1px solid lightgrey;">1.9</td> <td>0.09</td> </tr> <tr> <td><b>E1</b></td> <td>(250, 50)</td> <td style="border-left:1px solid lightgrey;">6.23</td> <td>29.98</td> <td style="border-left:1px solid lightgrey;">0.51</td> <td>1.67</td> <td style="border-left:1px solid lightgrey;">1.8</td> <td>0.08</td> </tr> <tr> <td><b>E1</b></td> <td>(250, 40)</td> <td style="border-left:1px solid lightgrey;">9.63</td> <td>10.44</td> <td style="border-left:1px solid lightgrey;">0.57</td> <td>1.47</td> <td style="border-left:1px solid lightgrey;">1.7</td> <td>0.08</td> </tr> <tr> <td><b>E2</b></td> <td>(250, 100)</td> <td style="border-left:1px solid lightgrey;">1.19</td> <td>17.09</td> <td style="border-left:1px solid lightgrey;">2.53</td> <td>1.58</td> <td style="border-left:1px solid lightgrey;">1.9</td> <td>0.09</td> </tr> <tr> <td><b>E2</b></td> <td>(250, 50)</td> <td style="border-left:1px solid lightgrey;">5.35</td> <td>3.57</td> <td style="border-left:1px solid lightgrey;">2.27</td> <td>1.63</td> <td style="border-left:1px solid lightgrey;">1.8</td> <td>0.09</td> </tr> <tr> <td><b>E2</b></td> <td>(250, 40)</td> <td style="border-left:1px solid lightgrey;">8.05</td> <td>3.26</td> <td style="border-left:1px solid lightgrey;">2.38</td> <td>1.45</td> <td style="border-left:1px solid lightgrey;">1.7</td> <td>0.08</td> </tr> <tr> <td><b>E3</b></td> <td>(250, 100)</td> <td style="border-left:1px solid lightgrey;">4.71</td> <td>10.16</td> <td style="border-left:1px solid lightgrey;">5.69</td> <td>4.26</td> <td style="border-left:1px solid lightgrey;">4.8</td> <td>0.09</td> </tr> <tr> <td><b>E3</b></td> <td>(250, 50)</td> <td style="border-left:1px solid lightgrey;">11.71</td> <td>14.49</td> <td style="border-left:1px solid lightgrey;">5.39</td> <td>3.74</td> <td style="border-left:1px solid lightgrey;">4.5</td> <td>0.09</td> </tr> <tr> <td><b>E3</b></td> <td>(250, 40)</td> <td style="border-left:1px solid lightgrey;">15.97</td> <td>20.90</td> <td style="border-left:1px solid lightgrey;">5.98</td> <td>3.70</td> <td style="border-left:1px solid lightgrey;">4.4</td> <td>0.09</td> </tr> </tbody> </table> <div class="caption"> Table of experiment results adapted from paper. Credits: Brandstetter et al. <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. </div> <details><summary>Abbreviations</summary> <table> <thead> <tr> <th>Shorthand</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td><strong>E1</strong></td> <td>Burgers&#39; equation without diffusion</td> </tr> <tr> <td><strong>E2</strong></td> <td>Burgers&#39; equation with variable diffusion</td> </tr> <tr> <td><strong>E3</strong></td> <td>Mixed equation, see below</td> </tr> <tr> <td>\(n_{t}\)</td> <td>Temporal resolution</td> </tr> <tr> <td>\(n_{x}\)</td> <td>Spatial resolution</td> </tr> <tr> <td>WENO5</td> <td>Weighted Essentially Non-Oscillatory (5th order)</td> </tr> <tr> <td><abbr title="Fourier neural operator">FNO</abbr>-<abbr title="recurrent neural networks">RNN</abbr></td> <td>Recurrent variation of <abbr title="Fourier neural operator">FNO</abbr> from original paper</td> </tr> <tr> <td><abbr title="Fourier neural operator">FNO</abbr>-PF</td> <td><abbr title="Fourier neural operator">FNO</abbr> with the pushforward trick added</td> </tr> <tr> <td>MP-PDE</td> <td>Message passing neural PDE solver</td> </tr> </tbody> </table> <p> The authors form a general PDE in the form </p> <p style="text-align:center;"> \([\partial_{t}u + \partial_{x}(\alpha u^{2} - \beta \partial_{x} u + \gamma \partial_{xx} u)](t,x) = \delta (t,x)\) </p> <p style="text-align:center;"> \(u(0,x) = \delta(0,x)\) </p> <p> such that \(\theta_{PDE} = (\alpha, \beta, \gamma)\) and different combinations of these result in the heat equation, Burgers' equation, and the KdV equation. \(\delta\) is a forcing term, allowing for greater variation in the equations being tested. </p> </details> <p>For this same experiment, the error and runtimes were recorded when solving using <abbr title="Weighted Essentially Non-Oscillatory (5th order)">WENO5</abbr>, the recurrent variant of the <abbr title="Fourier neural operator">FNO</abbr> (<abbr title="Fourier neural operator">FNO</abbr>-<abbr title="recurrent neural networks">RNN</abbr>), the <abbr title="Fourier neural operator">FNO</abbr> with the pushforward trick (<abbr title="Fourier neural operator">FNO</abbr>-PF), and the MP-PDE.</p> <blockquote> The pushforward trick is successful in mitigating error accumulation. </blockquote> <p>Comparing the accumulated errors of <abbr title="Fourier neural operator">FNO</abbr>-<abbr title="recurrent neural networks">RNN</abbr> and the <abbr title="Fourier neural operator">FNO</abbr>-PF across all experiments highlights the advantage of the pushforward trick. While the MP-PDE outperforms all other tested methods in the two generalization experiments <strong>E2</strong> and <strong>E3</strong>, the <abbr title="Fourier neural operator">FNO</abbr>-PF is most accurate for <strong>E1</strong>.</p> <p>When solving a single equation, the <abbr title="Fourier neural operator">FNO</abbr> likely performs better, though both <abbr title="Fourier neural operator">FNO</abbr>-PF and MP-PDE methods outperform <abbr title="Weighted Essentially Non-Oscillatory (5th order)">WENO5</abbr>.</p> <blockquote> Neural solvers are resolution-invariant. </blockquote> <p>As \(n_{x}\) is decreased, <abbr title="Weighted Essentially Non-Oscillatory (5th order)">WENO5</abbr> performs increasingly worse whereas all the neural solvers remain relatively stable.</p> <blockquote> Neural solver runtimes are constant to resolution. </blockquote> <p>Additionally, the runtimes of <abbr title="Weighted Essentially Non-Oscillatory (5th order)">WENO5</abbr> decrease (likely proportionally) since fewer steps require fewer calculations, but the MP-PDE runtimes again appear relatively stable.</p> <h2 id="conclusion">Conclusion</h2> <h4 id="future-directions">Future Directions</h4> <p>The authors conclude by discussing some future directions.</p> <p>For example, the MP-PDE can be modified for <span style="color:#9444e2;">PDE <em>retrieval</em> (which they call parameter optimization)</span>. There is some precedence for this: Cranmer et al. develop a method which fits a symbolic regression model (eg.: PySR, eureqa) to the learned internal functions of a GNN <d-cite key="cranmerDiscoveringSymbolicModels2020"></d-cite>. Alternatively, the MP-PDE’s capacity for generalization means that biasing the model with a prior to determine coefficients could be as simple as training on an example instance of the predicted equation, fitting this model on real world data (much like a finetuning process), and extracting the \(\theta_{PDE}\) parameters.</p> <p><span style="color:#9444e2;">Adaptive time stepping</span> is another avenue which could make the model more efficient and accurate by taking large steps over stable/predictable solution regions and smaller steps over changing/unpredictable solution regions. The choice of a CNN for the decoder works well over regular inputs and outputs, but other options like attention-based architectures could potentially weigh the outputted node embeddings such that the model might learn different time steps. Some care would have to be taken with temporal bundling in this case, since the resulting vectors would be potentially irregular in time.</p> <p>The one-step loss which is the basis of the <span style="color:#9444e2;">adversarial-style loss</span> is also used in reinforcement learning, which frequently uses deep autoregressive models. Other formulations which borrow from reinforcement learning (where distribution shifts are quite common) and other fields could prove successful as well. Transformer-based natural language processing are now capable of capturing extremely long sequence dependencies and generating coherent long-form text. Since <a href="https://graphdeeplearning.github.io/post/transformers-are-gnns/">Transformers are GNNs</a> which use attention to aggregate neighborhoods, this may be a viable avenue to explore.</p> <p><strong>One more potential direction is inspired by the recent GRAND paper.</strong></p> <p>Brandstetter et al. emphasizes the value of relationships to classical solvers - in fact, this is one of the key benefits of hybrid autoregressive models. However, modeling continuous functions as in neural operator models typically outperforms their competitors. Even the MP-PDE is fully neural, making it less explanable than the hybrid autoregressive models introduced earlier.</p> <p>The Graph Neural Diffusion (GRAND) model introduced by Chamberlain et al. demonstrates that <span style="color:#9444e2;"><abbr title="graph neural network">GNN</abbr> can be crafted using differential equations</span> (like diffusion processes) where, <a href="#spatialderivative">similarly to Brandstetter et al.</a>, the spatial derivative is analogous to the difference between node features <d-cite key="chamberlainGRANDGraphNeural2021a"></d-cite>. The layers are however analogous to the temporal change in a continuous-time differential equation, diverging from the MP-PDE intuition.</p> <p>Rather than “representationally [containing] some classical methods” <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>, GRAND provides a <span style="color:#9444e2;">mathematical framework</span> which not only offers explanability, but also a method to design new architectures with theoretical guarantees like stability or convergence <d-cite key="chamberlainGRANDGraphNeural2021a"></d-cite>.</p> <p>For example, standard <abbr title="message passing graph neural network">MP-GNN</abbr>s are shown to be equivalent to the explicit single-step Euler scheme; other classical solvers result in different flavours of message passing. Using GRAND to extend the MP-PDE would require rethinking the encoder and decoder, but the potential benefit could result in more reliability and therefore wider adoption of neural solvers for real world applications.</p> <h4 id="remarks">Remarks</h4> <p>In their paper “Message Passing Neural PDE Solver”, Brandstetter at al. present a well-motivated neural solver based on the principle of message passing. The key contributions are the end-to-end network capable of one-shot generalization, and the mitigation of error accumulation in autoregressive models via temporal bundling and the pushforward trick. Note that the latter are self-contained can be applied to other architectures (as in the FNO-PF), providing a valuable tool to improve autoregressive models.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Recent developments in the field of neural partial differential equation (PDE) solvers have placed a strong emphasis on neural operators. However, the paper Message Passing Neural PDE Solver by Brandstetter et al. published in ICLR 2022 revisits autoregressive models and designs a message passing graph neural network that is comparable with or outperforms both the state-of-the-art Fourier Neural Operator and traditional classical PDE solvers in its generalization capabilities and performance. This blog post delves into the key contributions of this work, exploring the strategies used to address the common problem of instability in autoregressive models and the design choices of the message passing graph neural network architecture.]]></summary></entry></feed>