<?xml version="1.0" encoding="utf-8"?> <feed xmlns="http://www.w3.org/2005/Atom"> <title>ICLR Blogposts 2023 (staging)</title> <link href="https://jocelynshen.com/deep-learning-blog.github.io/atom.xml" rel="self"/> <link href="https://jocelynshen.com/deep-learning-blog.github.io/"/> <updated>2023-10-12T18:58:58+00:00</updated> <id>https://jocelynshen.com</id> <author> <name></name> <email></email> </author> <entry> <title>Welcome to Jekyll!</title> <link href="https://jocelynshen.com/blog/2023/welcome-to-jekyll/"/> <updated>2023-10-12T18:14:35+00:00</updated> <id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/welcome-to-jekyll</id> <content type="html">&lt;p&gt;You’ll find this post in your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt; &lt;p&gt;Jekyll requires blog post files to be named according to the following format:&lt;/p&gt; &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YEAR-MONTH-DAY-title.MARKUP&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YEAR&lt;/code&gt; is a four-digit number, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MONTH&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DAY&lt;/code&gt; are both two-digit numbers, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MARKUP&lt;/code&gt; is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt; &lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt; &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;Tom&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints &apos;Hi, Tom&apos; to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt; &lt;p&gt;Check out the &lt;a href=&quot;https://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt; </content> </entry> <entry> <title>A Match Made in Drug Discovery - Marrying Geometric and Diffusion Models</title> <link href="https://jocelynshen.com/blog/2023/diffusion-is-all-you-need/"/> <updated>2023-02-17T00:00:00+00:00</updated> <id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/diffusion-is-all-you-need</id> <content type="html">&lt;h1 id=&quot;a-match-made-in-drug-discovery-marrying-geometric-and-diffusion-models&quot;&gt;A Match Made in Drug Discovery: Marrying Geometric and Diffusion Models&lt;/h1&gt; &lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;With the initial breakthrough of the Denoising Diffusion &lt;d-cite key=&quot;ho2020denoising&quot;&gt;&lt;/d-cite&gt;, diffusion models have evolved into a powerful tool for a wide range of applications in machine learning, including image generation and text generation. The rise and the recent release of Stable Diffusion &lt;d-cite key=&quot;rombach2022high&quot;&gt;&lt;/d-cite&gt; in August 2022 has shown superior performance in a wide range of practical applications - from artistic and creative projects to scientific and industrial ones and has recently paved its way into the natural sciences, which includes the field of drug discovery &lt;d-cite key=&quot;zhang2023sdegen,jing2022torsional&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Data-driven applications have increasingly been shown to accelerate solving diverse problems in the drug discovery pipeline &lt;d-cite key=&quot;CHEN20181241&quot;&gt;&lt;/d-cite&gt; – from the use of predictive analytical methods for target identification and lead optimization, to the analysis of large-scale biological data for drug repurposing and personalized medicine. To limit the scope of the blog post, we will only focus on the use of generative methods in &lt;strong&gt;molecular conformation generation&lt;/strong&gt;.&lt;/p&gt; &lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt; &lt;p&gt;Generating molecular conformations is a task fundamental to cheminformatics and drug discovery. The conformation of a molecule refers to the three-dimensional (3D) coordinates of all the atoms in a molecule in a 3D Euclidean space, which can be interconverted by rotations about formally single bonds &lt;d-cite key=&quot;nature_def&quot;&gt;&lt;/d-cite&gt;. It allows for the prediction of the physical and chemical properties and interactions of molecules based on their possible 3D conformations, as well as their biological activity. In drug discovery, it is integral to obtain all the possible conformations of molecules for various tasks, such as three-dimensional quantitative structure-activity relationships (3D-QSAR), pharmacophore searching, molecular docking and thermodynamic calculations. Deep generative modelling, which aims to learn complex data distributions, is a recent promising approach to tackle the conformation generation problem &lt;d-cite key=&quot;zhang2023sdegen&quot;&gt;&lt;/d-cite&gt;. When studying molecules it is important to understand them as three-dimensional structures formed by atoms bonded to each other. To encode the chemical properties, molecules could be represented as graphs where atoms (nodes) are connected by bonds (edges). Representing molecules as 3D molecular graphs captures the spatial arrangement of a molecule, which in turn determines its chemical property. A molecule could take up any conformation based on all possible permutations and combinations of spatial arrangements of atoms. However, some conformations may not occur physically, due to e.g steric hindrance, which arises when the spatial arrangement of atoms leads to unfavourable interactions/repulsive forces, leading to a higher energy state and less stable conformation.&lt;/p&gt; &lt;p&gt;Therefore, we are only interested in conformations that fall in stable low-energy minima, as these low-energy conformations are the ones that the molecule will most likely adopt under natural conditions and play a crucial role in determining the molecule’s behaviour and properties. By identifying and characterizing the low-energy conformations, researchers gain insights into their stability, reactivity and interactions with other molecules.&lt;/p&gt; &lt;h2 id=&quot;formulating-the-conformation-generation-problem&quot;&gt;Formulating the conformation generation problem&lt;/h2&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/formulation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/formulation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/formulation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/formulation.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;A formulation of the conformation generation problem, adapted from &lt;d-cite key=&quot;xu2022geodiff&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;We can formulate the problem of conformation generation as a conditional generative problem where we aim to generate stable conformations \(C\) from a molecule’s graph \(G\). For each \(G\) given its conformations \(C\) as i.i.d samples from an underlying Boltzmann distribution &lt;d-cite key=&quot;noe2019boltzmann&quot;&gt;&lt;/d-cite&gt;, our goal is to learn a generative model $$p_\theta(C&lt;/td&gt; &lt;td&gt;G)$$ to draw possible conformations from.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h3 id=&quot;roto-translation-equivariance&quot;&gt;Roto-translation equivariance&lt;/h3&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/roto-trans-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/roto-trans-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/roto-trans-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/roto-trans.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;A visualisation of roto-translation equivariance, adapted from &lt;d-cite key=&quot;xu2022geodiff&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;To generate stable molecular conformations, we need an algorithm that preserves roto-translation equivariance of the conformations that previous work has not focused on. To explain this property, let us delve into what equivariance is. A representation \(φ\) is equivariant with a transformation \(g\) of the input if the transformation can be transferred to the representation output. Invariance is a special case of equivariance obtained when the transformation is the identity map &lt;d-cite key=&quot;lenc2015understanding&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;In the context of molecular conformations, we have to achieve the special case of equivariance in terms of rotation and translation, namely, roto-translation equivariance of the conformations which ensures that however the molecule is rotated or translated, the estimated (conditional) likelihood should be unaffected. GeoDiff considers the SE(3) Lie group which can be used to represent rotation and translation in 3D space &lt;d-cite key=&quot;eade_2017&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h2 id=&quot;decomposing-geodiff&quot;&gt;Decomposing GeoDiff&lt;/h2&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/geodiff_main-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/geodiff_main-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/geodiff_main-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-diffusion-is-all-you-need/geodiff_main.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;The diffusion model of GeoDiff, adapted from &lt;d-cite key=&quot;xu2022geodiff&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Legend&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;\(C^{0}\) denotes the ground truth conformations&lt;/li&gt; &lt;li&gt;\(C^{t}\), where \(t = 1,···, T\) is the index for diffusion steps and \(C^{t}\), is the sequence of latent variables with the same dimension&lt;/li&gt; &lt;li&gt;\(q(C^{t} \mid C^{t-1})\) is the fixed posterior distribution&lt;/li&gt; &lt;li&gt;\(p_\theta(C^{t-1} \mid G, C^{t})\) are the Markov kernels through which the conformations are refined&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&quot;a-primer-on-diffusion-models&quot;&gt;A primer on Diffusion Models&lt;/h3&gt; &lt;p&gt;A diffusion probabilistic model &lt;d-cite key=&quot;sohl2015deep&quot;&gt;&lt;/d-cite&gt; can be described as a latent variable model with two processes: the forward and the reverse generative processes. Intuitively, the diffusion process progressively injects small noises into \(C^{0}\), while the generative process learns to revert the diffusion process by gradually eliminating the noise to recover the ground truth. Diffusion models are trained by adding noise to the input, which the model then learns how to remove.&lt;/p&gt; &lt;p&gt;In this blog post, we use the GeoDiff implementation of diffusion models to explain how the diffusion model works and how it is being used for the geometric representation of molecules. The implementation of the diffusion model in GeoDiff is inspired by the DDPM paper &lt;d-cite key=&quot;ho2020denoising&quot;&gt;&lt;/d-cite&gt;. To give a quick overview, the forward process \(q\) transforms the original input into complete noise over a certain number of timesteps and follows a normal distribution; the \(p_0\) involves denoising complete noise to the actual input using a neural network.&lt;/p&gt; &lt;h3 id=&quot;forward-process&quot;&gt;Forward process&lt;/h3&gt; &lt;p&gt;Let \(q(\mathbf{C}^0)\) be the real data distribution of molecular conformation. We can sample from this distribution to get a conformation, \(\mathbf{C}^0 \sim q(\mathbf{C}^0)\). We define the forward diffusion process which adds Gaussian noise at each time step \(t\), according to a known variance schedule \beta_t which can be linear, quadratic, cosine, etc. as follows:&lt;/p&gt; \[\begin{align}\tag{1} q\left(\mathcal{C}^{1: T} \mid \mathcal{C}^0\right)=\prod_{t=1}^T q\left(\mathcal{C}^t \mid \mathcal{C}^{t-1}\right) \end{align}\] &lt;p&gt;where \(\quad q\left(\mathcal{C}^t \mid \mathcal{C}^{t-1}\right)=\mathcal{N}\left(\mathcal{C}^t ; \sqrt{1-\beta_t} \mathcal{C}^{t-1}, \beta_t I\right)\)&lt;/p&gt; &lt;p&gt;Instead of having to compute \(q\left(\mathcal{C}^t \mid \mathcal{C}^{t-1}\right)\) at every timestep \(t\), we could compute at an arbitrary timestep in closed form:&lt;/p&gt; \[\begin{equation}\tag{2} q\left(\mathcal{C}^t \mid \mathcal{C}^0\right)=\mathcal{N}\left(\mathcal{C}^t ; \sqrt{\bar{\alpha}_t} \mathcal{C}^0,\left(1-\bar{\alpha}_t\right) I\right) \end{equation}\] &lt;p&gt;where \(\alpha_t=1-\beta_t\) and \(\bar{\alpha}_t=\prod_{s=1}^t \alpha_s\)&lt;/p&gt; &lt;p&gt;Thus with a sufficiently large number of timesteps, the forward process could convert \(\mathcal{C}^0\) to whitened isotropic Gaussian and so we could set \(p\left(\mathcal{C}^T\right)\) as a standard Gaussian distribution.&lt;/p&gt; &lt;h3 id=&quot;reverse-process&quot;&gt;Reverse Process&lt;/h3&gt; &lt;p&gt;The reverse process involved recovering the original conformation \(\mathcal{C}^0\) from the white noise \(\mathcal{C}^T\) . We need the conditional distribution \(p_\theta\left(\mathcal{C}^{t-1} \mid \mathcal{G}, \mathcal{C}^t\right)\) to sample some random Gaussian noise \(\mathcal{C}^t\), and “denoise” gradually to end up with a sample from the real distribution \(\mathcal{C}^0\).&lt;/p&gt; &lt;p&gt;However, the conditional distribution of \(p_\theta\left(\mathcal{C}^{t-1} \mid \mathcal{G}, \mathcal{C}^t\right)\) is intractable as it requires knowing the distribution of all possible conformations in order to calculate this conditional probability. Hence, a neural network could be used to learn this conditional probability distribution, let’s call it \(p_\theta\), with \(\theta\) being the parameters of the neural network, updated by gradient descent. Thus, we formulate the reverse process as a conditional Markov chain with learnable transitions:&lt;/p&gt; \[\begin{align*}\tag{3} p_\theta\left(\mathcal{C}^{0: T-1} \mid \mathcal{G}, \mathcal{C}^T\right)=\prod_{t=1}^T p_\theta\left(\mathcal{C}^{t-1} \mid \mathcal{G}, \mathcal{C}^t\right) \end{align*}\] &lt;p&gt;where \(\quad p_\theta\left(\mathcal{C}^{t-1} \mid \mathcal{G}, \mathcal{C}^t\right)=\mathcal{N}\left(\mathcal{C}^{t-1} ; \mu_\theta\left(\mathcal{G}, \mathcal{C}^t, t\right), \sigma_t^2 I\right)\)&lt;/p&gt; &lt;p&gt;Hence, the neural network in the reverse process needs to learn/represent the mean and variance. However, just like the DDPM paper, GeoDiff also lets the variance be user-defined and fixed, and \(\mu_\theta\) is the neural network that estimates means.&lt;/p&gt; &lt;p&gt;GeoDiff uses a parametrisation trick inspired by the diffusion model implementation from the DDPM paper such that this parametrisation resembles Langevin dynamics and simplifies the diffusion model’s variational bound to an objective that resembles denoising score matching. Moreover, in the context of molecular conformation generation, this parametrisation trick is analogous to the physical force fields &lt;d-cite key=&quot;schutt2017schnet, zhang2018deep, hu2021forcenet, shuaibi2021rotation&quot;&gt;&lt;/d-cite&gt;, which also gradually push particles towards convergence around the equilibrium states, and is defined by the following equation:&lt;/p&gt; \[\begin{align*}\tag{4} \mu_\theta\left(\mathcal{C}^t, t\right)=\frac{1}{\sqrt{\alpha_t}}\left(\mathcal{C}^t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta\left(\mathcal{G}, \mathcal{C}^t, t\right)\right) \end{align*}\] &lt;p&gt;where \(\epsilon_\theta\) are neural networks with trainable parameters \(\theta\).&lt;/p&gt; &lt;p&gt;Now, we need to make \(\epsilon_\theta\) roto-translational equivariant which we elaborate on in the next section.&lt;/p&gt; &lt;h3 id=&quot;making-the-reverse-process-roto-translation-equivariant&quot;&gt;Making the reverse process roto-translation equivariant&lt;/h3&gt; &lt;p&gt;Firstly, we need to assume the prior distribution of the conformations and the intermediary conformations generated during the forward process are systems with zero centre of mass (CoM) or CoM-free systems &lt;d-cite key=&quot;kohler2020equivariant&quot;&gt;&lt;/d-cite&gt;. By considering CoM-free systems, moving the particles to zero CoM can always ensure translational invariance in the Markov kernels.&lt;/p&gt; &lt;p&gt;GeoDiff employs the use of an equivariant convolutional layer, named graph field network (GFN) inspired by &lt;d-cite key=&quot;thomas2018tensor, satorras2021n&quot;&gt;&lt;/d-cite&gt;. In the \(l\)-th layer, GFN takes node embeddings \(h_l \in \mathbb{R}^{n \times b}\) (\(b\) denotes the feature dimension) and corresponding coordinate embeddings \(x_l \in \mathbb{R}^{n \times 3}\) as inputs, and outputs \(h_{l+1}\) and \(x_{l+1}\) as follows:&lt;/p&gt; \[\begin{align} \tag{5} &amp;amp; \mathbf{m}_{i j}=\Phi_m\left(\mathbf{h}_i^l, \mathbf{h}_j^l,\left\|\mathbf{x}_i^l-\mathbf{x}_j^l\right\|^2, e_{i j} ; \theta_m\right) \\ \tag{6} &amp;amp; \mathbf{h}_i^{l+1}=\Phi_h\left(\mathbf{h}_i^l, \sum_{j \in \mathcal{N}(i)} \mathbf{m}_{i j} ; \theta_h\right) \\ &amp;amp; \mathbf{x}_i^{l+1}=\sum_{j \in \mathcal{N}(i)} \frac{1}{d_{i j}}\left(\mathbf{c}_i-\mathbf{c}_j\right) \Phi_x\left(\mathbf{m}_{i j} ; \theta_x\right) \tag{7} \end{align}\] &lt;p&gt;where&lt;/p&gt; &lt;ul&gt; &lt;li&gt;\(\Phi\) are feed-forward networks&lt;/li&gt; &lt;li&gt;\(d_{ij}\) are interatomic distances&lt;/li&gt; &lt;li&gt;\(\mathcal{N}(i)\) is the neighbourhood of the \(i\)-th node, which consists of both connected atoms and other ones within a radius threshold \(\tau\).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;By introducing the neighbourhood function, we enable the model to accurately represent distant interactions between atoms, as well as the ability to handle partially disconnected molecular graphs. Initial embeddings \(h_0\) are combinations of atom and timestep embeddings, and \(x_0\) are atomic coordinates. A key change in GFN compared to a vanilla GNN is \(x\) being updated as a combination of radial directions weighted by \(\Phi_x\): \(\mathbb{R}^b \rightarrow \mathbb{R}\) as seen in equation \((7)\). This allows the roto-translation equivariance property to be induced in the reverse process.&lt;/p&gt; &lt;h3 id=&quot;improved-training-objective&quot;&gt;Improved Training Objective&lt;/h3&gt; &lt;p&gt;Now, we need to set the training objective having considered the reverse process dynamics. We cannot compute the exact log-likelihood of the generative process, as it involves computing the likelihood of the observed molecular conformation given the parameters of the model. However, this likelihood is difficult to compute, as it would require integrating over all possible intermediate conformations, giving us a high-dimensional integral that cannot be solved analytically. Therefore, the authors have opted to maximize the variational lower bound (ELBO), as defined below:&lt;/p&gt; \[\begin{aligned} \mathbb{E}\left[\log p_\theta\left(\mathcal{C}^0 \mid \mathcal{G}\right)\right] &amp;amp; =\mathbb{E}\left[\log \mathbb{E}_{q\left(\mathcal{C}^{1: T} \mid \mathcal{C}^0\right)} \frac{p_\theta\left(\mathcal{C}^{0: T} \mid \mathcal{G}\right)}{q\left(\mathcal{C}^{1: T} \mid \mathcal{C}^0\right)}\right] \\ &amp;amp; \geq-\mathbb{E}_q\left[\sum_{t=1}^T D_{\mathrm{KL}}\left(q\left(\mathcal{C}^{t-1} \mid \mathcal{C}^t, \mathcal{C}^0\right) \| p_\theta\left(\mathcal{C}^{t-1} \mid \mathcal{C}^t, \mathcal{G}\right)\right)\right]:=-\mathcal{L}_{\mathrm{ELBO}}\end{aligned}\] &lt;p&gt;where \(q\left(\mathcal{C}^{t-1} \mid \mathcal{C}^t, \mathcal{C}^0\right)\) is analytically tractable as \(\mathcal{N}\left(\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} \mathcal{C}^0+\frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t} \mathcal{C}^t, \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t\right)\).&lt;/p&gt; &lt;p&gt;Using the parametrisation trick in the reverse process as seen in equation \((4)\), the ELBO could be simplified by taking the KL divergences between Gaussians as weighted \(\mathcal{L}_2\) distances between the means \(\epsilon_\theta\) and \(\epsilon^3\) as follows: \(\mathcal{L}_{\mathrm{ELBO}}=\sum_{t=1}^T \gamma_t \mathbb{E}_{\left\{\mathcal{C}^0, \mathcal{G}\right\} \sim q\left(\mathcal{C}^0, \mathcal{G}\right), \epsilon \sim \mathcal{N}(0, I)}\left[\left\|\epsilon-\epsilon_\theta\left(\mathcal{G}, \mathcal{C}^t, t\right)\right\|_2^2\right]\) where \(\mathcal{C}^t=\sqrt{\bar{\alpha}_t} \mathcal{C}^0+\sqrt{1-\bar{\alpha}_t} \epsilon\).&lt;/p&gt; &lt;p&gt;The idea behind this objective is to independently sample chaotic conformations of different timesteps from \(q\left(C^{t-1} \mid C^t, C^0\right)\), and use \(\epsilon_\theta\) to approximate the noise vector \(\epsilon\).&lt;/p&gt; &lt;h3 id=&quot;sampling&quot;&gt;Sampling&lt;/h3&gt; &lt;p&gt;Now, we can generate stable molecular conformations via sampling. Given a graph \(\mathcal{G}\), its geometry \(\mathcal{C}^0\) is generated by first sampling chaotic particles \(\mathcal{C}^T \sim p\left(\mathcal{C}^T\right)\). For each timestep in the reverse process \(t=T, T-\) \(1, \cdots, 1\), we first shift the CoM of the conformation to zero, compute the transition means, \(\mu_\theta\left(\mathcal{G}, \mathcal{C}^t, t\right)\), using equation \((4)\) and sample \(\mathcal{C}^{t-1} \sim\) \(p_\theta\left(\mathcal{C}^{t-1} \mid \mathcal{G}, \mathcal{C}^t\right)\). The sampling algorithm is given in pseudo-code below:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align: left&quot;&gt;Algorithm 1 Sampling Algorithm of GEODIFF&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align: left&quot;&gt;Input: the molecular graph \(\mathcal{G}\), the learned reverse model \(\epsilon_\theta\).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: left&quot;&gt;Output: the molecular conformation \(\mathcal{C}\).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: left&quot;&gt;1: Sample \(\mathcal{C}^T \sim p\left(\mathcal{C}^T\right)=\mathcal{N}(0, I)\)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: left&quot;&gt;2: for \(s=T, T-1, \cdots, 1\) do&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: left&quot;&gt;3: \(\quad\) Shift \(\mathcal{C}^s\) to zero CoM&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: left&quot;&gt;4: \(\quad\) Compute \(\mu_\theta\left(\mathcal{C}^s, \mathcal{G}, s\right)\) from \(\epsilon_\theta\left(\mathcal{C}^s, \mathcal{G}, s\right)\) using equation 4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: left&quot;&gt;5: \(\quad\) Sample \(\mathcal{C}^{s-1} \sim \mathcal{N}\left(\mathcal{C}^{s-1} ; \mu_\theta\left(\mathcal{C}^{\mathcal{s}}, \mathcal{G}, s\right), \sigma_t^2 I\right)\)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: left&quot;&gt;6: end for&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: left&quot;&gt;7: return \(\mathcal{C}^0\) as $\mathcal{C}$$&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;but-why-generative-models&quot;&gt;But why generative models?&lt;/h2&gt; &lt;p&gt;The purpose of prediciting molecular conformations is to enable human experts to analyse the properties of the molecules and understand how these properties affect the viability of a molecule as a drug candidate. Therefore, it is important that the molecular conformations generated are diverse to capture the different possible conformations that could occur in nature but the conformations generated should not deviate significantly such that the analysis is affected. To set a threshold for the different possible conformations, the standard metric used has been selecting conformations that are within a certain root-mean-square deviation (RMSD), say a few ångströms, of the true structure.&lt;/p&gt; &lt;p&gt;However, the objective of maximizing the proportion of predictions with RMSD within some tolerance \(\epsilon\) is not differentiable and thus, cannot be used for training with stochastic gradient descent. Instead, maximizing the expected proportion of predictions with RMSD &amp;lt; \(\epsilon\) corresponds to maximizing the likelihood of the true structure under the model’s output distribution, in the limit as \(\epsilon\) goes to 0. This concept inspires the development of a generative model, whose objective is to minimize an upper bound on the negative log-likelihood of observed molecular structures under the distribution of the model. As a result, the problem of molecular docking is treated as a task of learning a distribution over possible positions of a ligand molecule conditioned on the protein structure and a diffusion generative model is developed to represent this space. Therefore, this observation has motivated several works on the use of generative models for molecular conformation generation, such as GeoDiff.&lt;/p&gt; &lt;h2 id=&quot;future-work&quot;&gt;Future work&lt;/h2&gt; &lt;p&gt;While diffusion models have shown promising results over the past months, there is still a need for further research and development to address some of the limitations and drawbacks of the use of diffusion models in conformer generation. As explained in our blog post, generative models are well-suited for molecular conformation generation. However, there are other kinds of established generative models, such as autoencoders that have been used for the same task &lt;d-cite key=&quot;gomez2018automatic&quot;&gt;&lt;/d-cite&gt;. For instance, comparing diffusion models and autoencoders, diffusion models can handle all noise levels with a single set of parameters, without any bottlenecks, which an autoencoder would have &lt;d-cite key=&quot;dieleman2022diffusion&quot;&gt;&lt;/d-cite&gt;. Thus, it is essential to study the advantages and disadvantages of different types and variants of generative models for conformation generation and understand what changes could be done to them to make them more suitable for molecular conformation.&lt;/p&gt; &lt;p&gt;It must be noted that the experiments described in the study are done on benchmark geometric datasets, GEOM-QM9 &lt;d-cite key=&quot;ramageom&quot;&gt;&lt;/d-cite&gt; and GEOM-Drugs &lt;d-cite key=&quot;axelrod2022geom&quot;&gt;&lt;/d-cite&gt;, rather than industrial data, with a relatively small number of samples/drug/molecular compounds. Industry data may exhibit greater variability than standard geometric datasets, as they may be more diverse, complex and subject to greater fluctuations than these geometric datasets, which tend to be more standardized and more predictable.&lt;/p&gt; &lt;p&gt;GeoDiff as a framework sets a precedent for future work that could marry the concepts of geometric deep learning and diffusion models across various domains. In the context of drug discovery, it would be interesting to extend this framework to more challenging structures, such as proteins, which may enable more accurate prediction of protein folding, protein-protein interactions and protein-ligand binding positions which would facilitate the design of new drugs and treatments. Additionally, this framework could potentially be applied to other complex systems beyond proteins, such as RNA molecules, to enable more efficient and accurate prediction of their behaviour and properties. Continued research in this area has the potential to revolutionize drug discovery and development, as well as advance our understanding of the fundamental principles governing the behaviour of complex biological systems.&lt;/p&gt; &lt;p&gt;Research on the application of diffusion models in the life and natural sciences is still in its infancy, with great potential for improvement in terms of both theory as well as empirical testing. The GeoDiff model could be improved in terms of more efficient sampling and improved likelihood maximization methods. Traditionally, generating samples from diffusion models demand iterative approaches that involve a large number of evaluation steps. Recent work, such as the paper on Torsional Diffusion &lt;d-cite key=&quot;jing2022torsional, satorras2021n&quot;&gt;&lt;/d-cite&gt; was able to speed up the sampling process, while also enhancing the quality of the resulting samples. Experimentally, Torsional Diffusion only takes 5 to 20 steps in comparison to GeoDiff which takes around 5000 steps &lt;d-cite key=&quot;galkin_2022&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Looking ahead, GeoDiff has set a clear example for the use of diffusion models on geometric representations which could be extended to several problems, especially in the field of drug discovery. The novel contributions made by&lt;/p&gt; &lt;d-cite key=&quot;xu2022geodiff&quot;&gt;&lt;/d-cite&gt; &lt;p&gt;are motivated by the physical characteristics of the molecular conformation generation problem, which has resulted in a strong candidate method for conformation generation that could act as a springboard for even more effective and efficient methods that would eventually benefit the field of drug discovery as a whole.&lt;/p&gt; </content> </entry> <entry> <title>Language and (Meta-) RL - An ode to structure</title> <link href="https://jocelynshen.com/blog/2023/language-rl/"/> <updated>2023-02-11T00:00:00+00:00</updated> <id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/language-rl</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;When Wittgenstein wrote, “Die Grenzen meiner Sprache bedeuten die Grenzen meiner Welt” (The limits of my language constitute the limits of my world), I doubt he would have even remotely imagined a world where one could ask a large language model like ChatGPT ‘Can you provide me Intelligent quotes on language by famous philosophers?’ before writing one’s blog on the role of language for Reinforcement Learning.&lt;/p&gt; &lt;p&gt;Yet, here we are, living in a time where sequential decision-making techniques like Reinforcement Learning are making increasingly larger strides, not just in robot manipulation &lt;d-cite key=&quot;lee-sciro20a&quot;&gt;&lt;/d-cite&gt; and games considered once to be the pinnacle of human intelligence &lt;d-cite key=&quot;silver-nature16a&quot;&gt;&lt;/d-cite&gt;, but also in an increasingly novel set of scenarios like chemistry &lt;d-cite key=&quot;zhou-acs17a&quot;&gt;&lt;/d-cite&gt;, logistics &lt;d-cite key=&quot;li-aamas19a&quot;&gt;&lt;/d-cite&gt;, etc. While a lot of the theory in RL existed from classical times, the success of integrating Neural Networks as function approximators has created a sort of Cambrian explosion in the last years. Traditionally, a major focus of the field was on developing techniques that can learn to solve an inherent optimization problem, like learning a solution to a maze. As the field evolved in the last years, its scope has started to broaden to encompass bigger questions, like whether a learned policy to solve a maze can generalize to other configurations (Generalization &lt;d-cite key=&quot;kirk-arxiv21a&quot;&gt;&lt;/d-cite&gt;), or whether a policy can be transferred to scenarios where conditions differ slightly from the training conditions (Robustness, Deployability), or how can we design agents in a data-driven manner (AutoRL &lt;d-cite key=&quot;parker-jair22a&quot;&gt;&lt;/d-cite&gt;). Yet, a major bottleneck in current RL techniques is that they are not yet, largely, ready for real-world deployment.&lt;/p&gt; &lt;p&gt;Parallelly, another Cambrian explosion has been happening in the field of Natural Language Processing (NLP). Language models have come a long way since the days of word embedding and sequence-sequence models, to the agent of attention and pre-trained models. Crucially, as this growth continues with newer innovations like ChatGPT, it also leads us to innovative applications of these language models in other fields of Machine Learning, including Reinforcement Learning&lt;/p&gt; &lt;p&gt;In this blog post, I will explore the connection between Natural Language and RL through some recent and not-so-recent ICLR publications that I find very interesting. Since this topic is vast, so much so that a full survey has been written on it (&lt;d-cite key=&quot;Luketina-ijcai19a&quot;&gt;&lt;/d-cite&gt;), I will limit the focus to how language can be used to augment RL pipelines (Language-assisted RL), and not on the use of RL for language training (RL for language). Through this blog, my hope it to visit two ideas in using Language for RL that exist at two very different points in the Deep RL timelines, and yet hold significance in the way they use language to augment the RL pipeline.&lt;/p&gt; &lt;h3 id=&quot;rl-basics&quot;&gt;RL basics&lt;/h3&gt; &lt;p&gt;To better cater to audiences beyond the RL community, I think it would be good to briefly revise some core concepts. RL folks are more than welcome to skip to the next section. I am going to try my best to keep it less math-oriented, but I apologize in advance on behalf of the symbols I will use.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/RL-pipeline-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/RL-pipeline-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/RL-pipeline-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/RL-pipeline.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;(Picture Credits: &lt;a href=&quot;https://jannik-zuern.medium.com/reinforcement-learning-to-survive-in-a-hostile-environment-3658624a5d83&quot;&gt;https://jannik-zuern.medium.com/reinforcement-learning-to-survive-in-a-hostile-environment-3658624a5d83&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;The key idea in RL (shown in the figure below) is to model the learning process as an agent acting in an environment. At every time-step $t$, the environment exists in a state $s$ and the agent can take an action $a$ to change this state to $s’$. Based on this transition, the agent gets a reward $r$. This process is repeated either for some number of steps until a termination condition is reached (also called episodic RL), or indefinitely (Non-episodic RL).&lt;/p&gt; &lt;p&gt;A common way to specify such problems is using Markov Decision Processes (MDPs), which can be written as a Tuple $\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, \mathcal{P}. R, \rho \rangle$ where&lt;/p&gt; &lt;ul&gt; &lt;li&gt;$\mathcal{S}$ is the state-space i.e. states are sampled from this space&lt;/li&gt; &lt;li&gt;$\mathcal{A}$ is the action space&lt;/li&gt; &lt;li&gt;$P: \mathcal{S} \times \mathcal{A} \to \Delta(\mathcal{S})$ is a kernel that defines the probability distribution over the next states i.e. given a state and action, it tells us the probability of ending in the next state&lt;/li&gt; &lt;li&gt;$R: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$ is the reward function&lt;/li&gt; &lt;li&gt;$\rho$ is the initial state distribution&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The aim of the agent is to maximize the sum of rewards obtained, a quantity called the return $G_t$, by learning a policy $\pi$ that either maps a given state to an action (deterministic policies), or distributions over actions (stochastic policies). Thus, we can model any task $\tau$ using the MDP formalism and train an RL agent to learn to policy to solve this task.&lt;/p&gt; &lt;h3 id=&quot;language-generalization-and-multi-task-rl&quot;&gt;Language, Generalization, and Multi-Task RL&lt;/h3&gt; &lt;p&gt;When we talk about settings that go beyond solving a single task $\tau$ we are faced with the problem of training an agent to solve multiple MDPs $\mathcal{M}_1, \mathcal{M}_2, \dots$, and this is non-trivial. While we want the agent to learn to perform well at these tasks, we don’t want to do it the naive way by training the agent to perform all of these tasks separately. Thus, the key challenge is to figure out a way to make this problem more tractable.&lt;/p&gt; &lt;p&gt;A good point to try to look for solutions is to take inspiration from how humans learn in the real world. This is where the idea of knowledge reuse comes into the picture. Consider the case of learning how to ride a motorcycle. This can be broken down into a sequence of tasks like learning how to balance yourself on a motorcycle, learning how to navigate a two-wheeler, learning how to run a motor-based vehicle, etc. A lot of these skills can be acquired even before we come near a motorcycle through other tasks. For example, learning how to walk teaches us navigation, while learning how to ride a bicycle teaches us things about two-wheelers, and so on. Thus, when a human would come to the task of learning how to ride a motorcycle, they would essentially reuse a lot of skills that they have learned before. In other words, humans meta-learn between tasks.&lt;/p&gt; &lt;p&gt;If we look a bit deeper into this process, some interesting concepts come to the forefront:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The only reason a human can meta-learn between these tasks incrementally is that there is an overlap between the requirements of solving these individual tasks. In other words, there is some underlying structure that can be captured by a learner between tasks that allows them to transfer learned knowledge between tasks&lt;/li&gt; &lt;li&gt;Given a learned set of skills, any problem that can be solved by composing these skills can be potentially solved by a learner who has acquired these skills. IN other words, complex tasks that have an underlying compositional structure can be solved by learning individual components and combining them&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So, when we translate this to the problem of solving a collection of MDPs $\mathcal{M}_1, \mathcal{M}_2, \dots$, one of the key bottlenecks is figuring out a way to capture the underlying structure between them. Naively, we could implicitly learn this structure, and a lot of techniques in Meta-RL do exactly this. But what if we had some additional side information that we could leverage to do this more easily?&lt;/p&gt; &lt;p&gt;This is where language shows its importance. The world is full of structure in the form of relations between objects (Relational Structure), Causal relationships between events (Causal Structure), etc. Crucially, while humans do meta-learn between individual tasks, this ability is significantly bolstered and catalyzed by the existence of language &lt;d-cite key=&quot;edmiston-cognition15a&quot;&gt;&lt;/d-cite&gt;. Language not only helps us transfer knowledge but also plays a central role in helping us form necessary abstractions that can boost our ability to form associations between tasks &lt;d-cite key=&quot;keil-EC00a&quot;&gt;&lt;/d-cite&gt;. Thus, a very interesting avenue opens up when we start to consider the use of language as a way of incorporating structure into RL pipelines, be it through structure between MDPs in the Multi-task setting, or through structure in the process of inferring things like scene representation and reward signals in the real-world deployment of RL systems.&lt;/p&gt; &lt;h2 id=&quot;idea-1---policy-sketches&quot;&gt;Idea 1 - Policy Sketches&lt;/h2&gt; &lt;p&gt;Source: &lt;d-cite key=&quot;andreas-icml17a&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;When we talk about composing skills, a long line of work in RL has been on the idea of learning policies hierarchically (Hierarchical RL). For example, in the task of locomotion, controllers can learn individual control policies while a higher-level controller can learn a policy whose actions are to coordinate the lower-level controllers. This is similar to how we try to solve a problem using the dynamic programming method — by breaking it down into subproblems and then combining the solutions together.&lt;/p&gt; &lt;p&gt;To demonstrate the approach followed in this paper, I will use the example provided in the paper shown in the figure below :&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/Policy-Sketches-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/Policy-Sketches-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/Policy-Sketches-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/Policy-Sketches.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;In this simple grid world scenario. we have two tasks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;$\tau_1$ requires an agent to make plans by first collecting wood and then using the collected wood on a workbench&lt;/li&gt; &lt;li&gt;$\tau_2$ requires the agent to make sticks by first collecting wood and then taking the wood to a toolshed&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;At the onset, we see that the first step in both of these tasks is to collect wood. Thus, if an agent learns a sub-policy to collect wood, it can reuse it for both tasks. Each subtask can be associated with a symbol $b$ and the associated policy by $\pi_b$. Now, given a set of learned symbols $b_1, b_2,\dots$ and the corresponding sub-policies $\pi_{b_1}, \pi_{b_1}, \dots$, a high-level task can be described by a sequence of symbols, which the authors call a sketch.&lt;/p&gt; &lt;p&gt;The sketch is akin to a word using these symbols and thus, follows the basic rules of language. Given a sketch, a complex policy $\Pi_b$ can be executed by executing a sequence of policies. To make this technically feasible, the policy executions come with termination conditions that signify the duration for which a policy needs to be executed, something that is very standard in Policy-based.&lt;/p&gt; &lt;p&gt;Thus, the authors recast the hierarchical problem as a problem of modular sketches using language symbols. Additionally, by being associated with symbols, the policies end up being more interpretable.&lt;/p&gt; &lt;p&gt;Going a bit deeper into the technicalities for the interested folks, the authors model the multi-task setting by assuming the tasks to be specified by the reward and initial distribution $(R_\tau, \rho_\tau)$. AT any time step, a subpolicy selects either a low-level action $a \in \mathcal{A}$ or a special stop action indicating termination. They use Policy gradients for the optimization with the crucial factor of having an actor per symbol but one critic per task. Since actors can participate in multiple tasks, by constraining the critic to task, they are able to baseline policies using task-associated values. Finally, to tackle the issue of sparse rewards by using a curriculum learning approach, where the learner is initially presented with shorter sketches, which are progressively increased in length as the learner learns the sub-policies.&lt;/p&gt; &lt;h2 id=&quot;idea-2---reward-specification-via-grounded-natural-language&quot;&gt;Idea 2 - Reward specification via grounded Natural Language&lt;/h2&gt; &lt;p&gt;Source: &lt;d-cite key=&quot;mahmoudieh-icml22a&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;While approaches like policy sketches are very powerful in composing policies together using the symbolic capabilities and inherent compositional structure of language, they still require hand-designed rewards per task. This is something that is prevalent throughout the RL literature.&lt;/p&gt; &lt;p&gt;The fundamental issue with this is that reward signals can be expensive to design in the real world and they usually require knowledge about the true state. On real-world tasks, however, RL agents usually have only access to pixel observations, that could be generated for latent states, for example. A common alternative to reward design is to hand-label goal images or collect demonstrations that can be used to teach an RL agent reward signals. This is a labor-intensive process as well.&lt;/p&gt; &lt;p&gt;The authors of this paper take a different route by leveraging text descriptions. Specifically, language can be used to describe a task by providing the agent with descriptions of a goal and/or spatial relationships between the entities in the environment. When combined with the observation, this can be used to compute the proximity of an agent to the goal and thus, help guide the agent. The authors in this work specifically use language in the form of spatial relationships between entities. For example, a sentence like ‘A on the left of B’ would be interpreted as the x coordinate B being greater than that of A. By using multi-camera scenes, they are able to associate each description with a symbol by comparing the camera view that matches the condition.&lt;/p&gt; &lt;p&gt;Once the labels have been generated, they train the reward model using a contrastive loss where the model essentially predicts which caption matches which image in a sampled batch of random images. Crucially, the aim is to maximize the cosine similarity between the image and text embeddings of the correct pairs and minimize the cosine similarity of the embeddings of the incorrect pairs.&lt;/p&gt; &lt;p&gt;Once this has been achieved, the learned model can be used to provide rewards to RL policies. For this, they first learn several tasks using RL and the reward model. They create a large dataset of the rollouts of the learned policies and pair each trajectory with the goal text description of the tasks it was trained to learn. Finally, this data is used for supervised learning for predicting actions using text and images. The process has been visualized in the figure below:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/Grounding-Pic-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/Grounding-Pic-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/Grounding-Pic-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-language-rl/Grounding-Pic.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this post, we have seen two ways of using language for RL. There have been a lot of other ways recently in this direction. Some examples of these are&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;d-cite key=&quot;lampinen-icml22a&quot;&gt;&lt;/d-cite&gt; &lt;p&gt;augment policy networks with the auxiliary target of generating explanations and use this to learn the relational and causal structure of the world&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;d-cite key=&quot;kumar-neurips22a&quot;&gt;&lt;/d-cite&gt; &lt;p&gt;use language to model compositional task distributions and induce human-centric priors into RL agents.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Given the growth of pre-trained language models, it is only a matter of time before we see many more innovative ideas come around in this field. Language, after all, is a powerful tool to incorporate structural biases into RL pipelines. Additionally, language opens up the possibility of easier interfaces between humans and RL agents, thus, allowing more human-in-the-loop methods to be applied to RL. Finally, the symbolic nature of natural language allows better interpretability in the learned policies, while potentially making them more explainable. Thus, I see this as a very promising direction of future research&lt;/p&gt; </content> </entry> <entry> <title>Transformers Learn Faster by Taking Notes on the Fly</title> <link href="https://jocelynshen.com/blog/2023/taking_notes_on_the_fly/"/> <updated>2023-02-07T00:00:00+00:00</updated> <id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/taking_notes_on_the_fly</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/example-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/example-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/example-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/example.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 1 from the paper, which describes an example situation in which taking notes could be useful. Here COVID is a rare word. Therefore, the model is struggling with the completion task on the left because it has not possibly seen many sentences with the word COVID in it. Thus, we take notes of the contextual information of it as we see examples on it in the training set and we quickly learn to associate it with which words! &lt;/div&gt; &lt;p&gt;Transformers, which were invented by Google in 2017 &lt;d-cite key=&quot;vaswani2017attention&quot;&gt;&lt;/d-cite&gt;, have become the go-to architecture for various tasks in many domains, such as natural language processing and computer vision &lt;d-cite key=&quot;dosovitskiy2020image&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;devlin2018bert&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;radford2018improving&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;brown2020language&quot;&gt;&lt;/d-cite&gt;. The success of transformers are mainly because they have two amazing properties:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;They are phenomenal in grasping the context of words within the bodies of text that they belong to.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;They do not process the input sequences in order. Thus, their operations can easily be parallelized.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Equipped with these powerful features, transformers have excelled in unsupervised pre-training tasks, which is the driving force of several state-of-the-art models, such as BERT and GPT-3. In unsupervised pre-training, a large and diverse dataset is used to train the (baseline) model. If someone wishes to fine-tune the base model for a specific task, they can do so by training it with a relatively smaller, task-specific dataset.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/unsupervised-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/unsupervised-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/unsupervised-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/unsupervised.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; During unsupervised pre-training, the model is trained on a large unlabeled dataset. Then, it becomes a powerful baseline model that can be fine-tuned to work with various tasks. &lt;/div&gt; &lt;p&gt;Generalization can be achieved with a sufficiently large model that is trained on sufficiently diverse and large data &lt;d-cite key=&quot;radford2019language&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;tirumala2022memorization&quot;&gt;&lt;/d-cite&gt;. However, pre-training large models is very time-consuming and costly in terms of environmental impacts and monetary resources &lt;d-cite key=&quot;strubell2019energy&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;chen2021bert2bert&quot;&gt;&lt;/d-cite&gt;. Thus, reducing the pre-training time and cost for transformer-based models is an imminent concern for machine learning practitioners. One area that has room for improvement is how quickly the model learns the embeddings of the rare words. It has been shown by many works that the embeddings of those words are noisy and not optimized &lt;d-cite key=&quot;bahdanau2017learning&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;gong2018frage&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;khassanov2019constrained&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;schick2020s&quot;&gt;&lt;/d-cite&gt;. Furthermore, Wu et al. 2021 empirically observe that 20% of all sentences in the corpus contain a rare word and they propose a &lt;em&gt;“note-taking”&lt;/em&gt; approach improves model’s ability to learn the embeddings of rare words &lt;d-cite key=&quot;wu2021taking&quot;&gt;&lt;/d-cite&gt; . Impressively, they reduce the pre-training time of well-known large language models (LLMs), such as BERT, by 60%. The approach is called Taking Notes on the Fly (TNF) and we will dive deep into how it works in this blog post!&lt;/p&gt; &lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt; &lt;h3 id=&quot;transformers&quot;&gt;Transformers&lt;/h3&gt; &lt;p&gt;Wu et al. &lt;d-cite key=&quot;wu2021taking&quot;&gt;&lt;/d-cite&gt; extends the BERT model &lt;d-cite key=&quot;devlin2018bert&quot;&gt;&lt;/d-cite&gt;, which is a transformer-based model, with an external memory. A transformer is composed of alternating multi-head attention and feed-forward layers. The initial input to the multi-head attention layer is the sum of word embeddings and positional embeddings. Each one-hot encoded token is multiplied with a weight matrix in order to obtain a real-valued non-sparse representation. The weight matrix is learned throughout the training. Because transformers do not process words in order, we also need to provide some information about the position of the token in a sentence. This is incorporated into the training by the “positional embedding (encoding)” \((PE)\) vector, composed of sine and cosine pairs.&lt;/p&gt; \[PE_{\text{pos},2i} = sin(pos / 10000^{2i/d_{embed}} )\] \[PE_{\text{pos},2i+1} = cos(pos / 10000^{2i/d_{embed}} ),\] &lt;p&gt;where \(pos\) is the position of the token in the sentence, \(d_{embed}\) is the embedding dimension of the model, and \(i\) refers to the dimension in the \(PE\) vector. Note that the positional embeddings do not depend on the meaning of the words, but only the position of them!&lt;/p&gt; &lt;p&gt;Self attention mechanism allows the model to relate words in a sentence through a set of learnable query \((Q)\), key \((K)\) and value \((V)\) vectors. The output of the attention function calculates a compatibility score for each pair of words in the sentence. Mathematically, self attention can be expressed as&lt;/p&gt; \[\text{self-attention} (Q,K,V) = softmax(QK^T / \sqrt{(d_k)}),\] &lt;p&gt;where \(d_k\) is the dimension of hidden representations. In order to improve the representational power of the model, &lt;d-cite key=&quot;vaswani2017attention&quot;&gt;&lt;/d-cite&gt; proposed a multi-head attention mechanism. In particular, the \(self-attention\) function is calculated several times independently, results are concatenated, and linearly projected into the desired dimension.&lt;/p&gt; &lt;p&gt;BERT is a masked language model which uses the transformer architecture. During training time, 15% of the words in the sentence are masked or replaced with a random word. The model learns to predict the words that are masked.&lt;/p&gt; &lt;h3 id=&quot;word-distribution-in-texts&quot;&gt;Word Distribution in Texts&lt;/h3&gt; &lt;p&gt;The distribution of the words in a natural language corpora follow Zipf’s law &lt;d-cite key=&quot;zipf1932selected&quot;&gt;&lt;/d-cite&gt;, that is, the frequency \(n^{th}\) most frequent word is proportiional to \(1/n^\alpha, \: where \:\: \alpha \sim 1\).&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/zipf_law-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/zipf_law-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/zipf_law-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/zipf_law.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The frequencies of 50 most common words in Brown Corpus&lt;d-footnote&gt;Details can be found &lt;a href=&quot;http://korpus.uib.no/icame/manuals/BROWN/INDEX.HTM&quot;&gt; in the link.&lt;/a&gt;&lt;/d-footnote&gt;. Green is the word counts estimated by Zipf&apos;s law, blue is the actual count. Image is taken from &lt;d-cite key=&quot;zipf&quot;&gt;&lt;/d-cite&gt;. &lt;/div&gt; &lt;p&gt;In other words, number of popular words are much less than of rare words, yet their frequency is much larger. This harms pretraining of LLMs because of the sparse and inaccurate optimization of neural networks, rare words are much likely to generate noisy and low-quality embeddings &lt;d-cite key=&quot;gao2019representation&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt; &lt;p&gt;Pre-training of LLMs has become a burden in terms of training time and power consumption. Still, it is essential for almost every downstream task in NLP. This computational cost is addressed by several studies in terms of altering the model or utilizing the weight distribution of neural networks’ layers. Particularly, &lt;d-cite key=&quot;clark2020electra&quot;&gt;&lt;/d-cite&gt; added a discriminator to predict if each word in the sentence that is completed by the generator is correct or not. Another important work arised after the observation that the attention distributions of top and bottom layers are quite similar. &lt;d-cite key=&quot;gong2019efficient&quot;&gt;&lt;/d-cite&gt; proposed an iterative algorithm that doubles the number of layers after each training episode.&lt;/p&gt; &lt;p&gt;The efficiency of pretraining LLMs has shown to be incresed, still the heavy-tailed distribution of words in natual language corpora is an obstacle in further development &lt;d-cite key=&quot;strubell2019energy&quot;&gt;&lt;/d-cite&gt;. Note taking approach positively impacts learning performance in humans &lt;d-cite key=&quot;makany2009optimising&quot;&gt;&lt;/d-cite&gt;. This idea is inspired studies in terms of contributing to training efficiency &lt;d-cite key=&quot;wu2021taking&quot;&gt;&lt;/d-cite&gt; and increasing performance in downstream tasks &lt;d-cite key=&quot;feng2022memory&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;fevry2020entities&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;guu2020retrieval&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;khandelwal2019generalization&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;It is shown that the frequency of words affect the embeddings. Additionally, most of the rare words’ embeddings are close to each other in embedding space indepent from its semantic information while the neighbors of frequent words are the ones that have similar meaning &lt;d-cite key=&quot;gong2018frage&quot;&gt;&lt;/d-cite&gt;. Initial studies mainly used subword information to encode semantic information, this approach is shown to be valuable for morphologically rich languages &lt;d-cite key=&quot;pmlr-v32-santos14&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;kim2016character&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;el2019parsimonious&quot;&gt;&lt;/d-cite&gt;. Recently, this problem is also adressed by using adverserial training where a discriminator classifies each word as ‘frequent’ or ‘rare’ allowing semantic information to be encoded &lt;d-cite key=&quot;gong2018frage&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt; &lt;p&gt;Because learning the embeddings of rare words is arduous, it takes a lot of training epochs for the model to make up for the resulting loss in quality. Thus, the authors propose keeping a third type of embedding (besides the word embeddings and positional embeddings), which is designed to retain additional information about the rare words. This embedding type can be considered as &lt;em&gt;taking notes&lt;/em&gt; on the contextual information of these rare words as the training progresses, is also called the note dictionary, and is updated as the training progresses.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/overview-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/overview-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/overview-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/overview.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 2 from the paper, which gives an overview of the note taking process. The note embeddings are randomly initialized and the other two embeddings are computed. Then, their sum is given to the transformer encoder as input. For every rare word encountered when going through the training data, its contextual information is calculated and the corresponding note embeddings are updated accordingly. This process goes on as the data is being fed to the transformer. &lt;/div&gt; &lt;p&gt;At this point, we assume that the text has already been pre-processed using Byte Pair Encoding (BPE&lt;d-footnote&gt;A very nice &lt;a href=&quot;https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10&quot;&gt; blog post&lt;/a&gt; about BPE. Reading it is highly encouraged as it also provides visuals on BPE.&lt;/d-footnote&gt;), which is a popular method that is used as a part of the text embedding process for NLP tasks &lt;d-cite key=&quot;sennrich2015neural&quot;&gt;&lt;/d-cite&gt;. In BPE, each word is represented as a concatenation of sub-word units, which are selected according to how much each they unit occur in the given text. For example, if the sub-word &lt;b&gt;“pre”&lt;/b&gt; occurs in the text frequently, it will be represented with a single character, such as &lt;b&gt;“X”&lt;/b&gt; in this encoding. This way, the textual data is compressed and manageable. Also, because each sub-word unit gets their own embedding, we get a hybrid approach between word-level and character-level embeddings. Therefore, the embedding of each word might very well be made up of multiple consecutive tokens. With this information in mind, let us walk through the steps of note taking!&lt;/p&gt; &lt;p&gt;The first three steps are about initializing the required variables and determining the hyper-parameters of the scheme.&lt;/p&gt; &lt;p&gt;0a. Randomly initialize the note dictionary, \(NoteDict\).&lt;/p&gt; &lt;p&gt;0b. Determine a window size (\(2k\) as denoted in the paper), which corresponds to the number of surrounding tokens whose embedding will be included in the note.&lt;/p&gt; &lt;p&gt;0c. Determine a discount factor, \(\gamma\in (0,1)\). This will determine how much weight we give to each occurrence of the rare word and the corresponding contextual information.&lt;/p&gt; &lt;p&gt;Now, note taking begins!&lt;/p&gt; &lt;p&gt;1.For each word \(w\) in the training corpora, check if the word is a rare word or not. If it is rare, mark the index of the starting and ending sub-word tokens of the word with \(s\) and \(t\), respectively.&lt;/p&gt; &lt;p&gt;2.Compute the output of the transformer encoder on the input embeddings (positional+token+note embeddings). The output will be composed of \(d\)-dimensional vector per token. Call the output of the transformer encoder on position \(j\), \(c_j\in \mathbb{R}^d\).&lt;/p&gt; &lt;p&gt;3.Given a sequence of tokens \(x\) with word \(w\) in it, sum the \(d\)-dimensional input embedding vectors of all tokens located between indices \(s-k\) and \(t+k\) and divide this sum by \(2k+t-s\), namely, the number of tokens within that interval. The resulting vector is the note of \(w\) taken for sequence \(x\), \(Note(w,x)\). Mathematically, we have \(Note(w,x)=\dfrac{1}{2k+t-s}\sum_{j=s-k}^{t+k}c_j\).&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/numberline-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/numberline-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/numberline-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/numberline.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; This figure demonstrates contextual embedding vectors at which locations will be selected and summed with an example. This line represents the indices of a sequence of length 11. Let us assume that the rare word is contained within tokens 4 to 6, and k=2, which makes the window size 2k=4. Thus, we sum the tokens at location 4, 5, 6, as well as 3, 4, (which are the two immediate left tokens) and 7,8 (which are the two immediate right tokens). Finally, we divide the each element of the resulting vector by 6, which is the total number of elements in the interval. &lt;/div&gt; &lt;p&gt;4.To update the note embedding of w, NoteDict(w), take the exponential moving average of its previous value and Note(w,x) using the discount factor, namely, \(NoteDict(w)=(1-\gamma)NoteDict(w)+\gamma Note(w,x)\). This way, we can choose how much importance we assign to each occurrence of a rare word.&lt;/p&gt; &lt;p&gt;This process repeats until all of the sentences are processed this way. Note that, this can be achieved on the fly, as the model processes each sentence. Now that we have our notes neatly stored in \(NoteDict\), let us incorporate them into the training process! We again take the exponential moving average of the sum of the positional and token embeddings (the embedding used in the original transformer paper) with the corresponding \(NoteDict\) value using another parameter called \(\lambda\in(0,1)\). In particular, for every word \(w\) that occurs in both \(NoteDict\) and sequence \(x\), each location corresponding to the word \(w\) and its surrounding \(2k\) tokens is set to the weighted of the sum of the positional and token embeddings with the corresponding NoteDict value. Any other location is set to the sum of the token embeddings and positional embeddings only. The resulting vector will be the input to our model for the next step. Mathematically, for location \(i\in[d]\), which corresponds to (one of the) tokens of word \(w\) in the sequence, we have \(\text{input}_i= \begin{cases} (1-\lambda)(\text{p_embed}_i+\text{t_embed}_i)+\lambda\text{NoteDict}(w), &amp;amp; \text{w is a rare word} \\ \text{p_embed}_i+\text{t_embed}_i, &amp;amp;\text{otherwise} \\ \end{cases}\) where \(\text{p_embed}\) is positional embeddings, \(\text{t_embed}\) is token embeddings and \(\lambda\) (set to 0.5) is the hyperparameter specifying the weight of the notes when computing the embeddings.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/graphs-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/graphs-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/graphs-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-07-taking_notes_on_the_fly/graphs.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 3. from the paper, presenting the loss and GLUE scores of the models with and without taking notes, over many iterations. &lt;/div&gt; &lt;p&gt;The experiments are conducted on BERT and ELECTRA models. The loss values of the pre-training runs with &lt;em&gt;note taking&lt;/em&gt; descrease significantly faster than vanilla pre-training. Moreover, the models trained while taking notes achieve higher GLUE &lt;d-cite key=&quot;wang2018glue&quot;&gt;&lt;/d-cite&gt; scores much faster. Additionally, they report that after one million iterations, the GLUE score of the models pre-trained with notes are superior to their counterparts trained without notes. Finally, they report that when it took one model with note taking to reach a certain GLUE score around 100.000 training iterations, it took the model around 400.000 training iterations to reach that same score without notes. That is a 60% improvement in training time to reach the same performance!&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The ever-increasing data sizes, enlarging models, and hardware resources are some of the major factors in the current success of LLMs. However, this also means immense power consumption and carbon emission. Because pre-training of LLMs is the most computationally intensive phase of a natural language task, efficient pre-training is the concern of this paper. Knowing that the heavy-tailed distribution of word frequencies in any natural language corpora may hinder the efficiency of pre-training, improving data utilization is crucial. Therefore, the authors propose a memory extension to the transformer architecture: “Taking Notes on the Fly”. TNF holds a dictionary where each key is a rare word. The values are the historical contextual information which is updated at each time the corresponding word is encountered. The dictionary is removed from the model during the inference phase. TNF reduces the training time by 60% without any reduction in the performance.&lt;/p&gt; </content> </entry> <entry> <title>Controllable Music Generation via MIDI-DDSP</title> <link href="https://jocelynshen.com/blog/2023/controllable-music-generation-via-midi-ddsp/"/> <updated>2023-02-02T00:00:00+00:00</updated> <id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/controllable-music-generation-via-midi-ddsp</id> <content type="html">&lt;h2 id=&quot;historical-background-and-motivation&quot;&gt;Historical background and Motivation&lt;/h2&gt; &lt;p&gt;Early years of synthesizer: Robert Moog, an American engineer developed a synthesizer in which created and shaped sound connected by patch cords where pitch was controlled via voltage. It was popularized in the late 1960s by rock and pop acts such as the Doors, the Grateful Dead, the Rolling Stones, and the Beatles.&lt;d-cite key=&quot;wikipedia_2023&quot;&gt;&lt;/d-cite&gt; During the same time, American engineer Don Bul created the Buchla Modular Electronic Music System Buchla Modular Electronic Music System in which instead of a traditional keyboard he used touchplates where depending on finger position and force voltage was transmitted. However, Moog’s Synthesizer became more accessible and marketable to musicians during 1964 and the mid-1970s. The earliest versions of synthesizers could only produce a single note at a time. Tom Oberheim, an American engineer, developed some of the early commercial polyphonic synthesizers. The first fully programmable polyphonic synthesizer, - Prophet5 was released in 1978 which used microprocessors to store sounds in patch memory. This allowed synthesisers to go from producing uncertain sounds to “a conventional set of familiar sounds.” After introduction of MIDI in 1982, synthesizer market grew dramatically&lt;d-cite key=&quot;vail2014synthesizer&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h4 id=&quot;notation-used&quot;&gt;Notation used&lt;/h4&gt; &lt;p&gt;DDSP - Differentiable Digital Signal Processing &lt;br /&gt;&lt;/p&gt; &lt;h2 id=&quot;midi---ddsp&quot;&gt;MIDI - DDSP&lt;/h2&gt; &lt;p&gt;While generative models are function approximator and may assist the development of samples across many domains, this expressiveness comes at the expense of interaction, since users are often limited to black-box input-output mappings without access to the network’s internals.&lt;d-cite key=&quot;wu2021midi&quot;&gt;&lt;/d-cite&gt;. This makes sense as we know that having access to the latent space can be very useful for the generative models. Diffusion models lack this!&lt;br /&gt; In computer vision and speech there has been development in methods where users are allowed to interact througout the hierarchy of system making it optimize for realism and control. Whereas in music synthesis methods still lack this interaction in hierarchy of music generation. Recent research states that one can either generate full-band audio or have control of pitch, dynamics and timbre but not both.&lt;d-cite key=&quot;hawthorne2018enabling&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;wu2021midi&quot;&gt;&lt;/d-cite&gt;. Authors of MIDI-DDSP&lt;d-cite key=&quot;wu2021midi&quot;&gt;&lt;/d-cite&gt; take inspiration from process of creating music and propose a generative model of music generation organised in a hierarchy for more realism and control. As traditional synthesizer use MIDI standard audio files, MIDI - DDSP translates note timing, pitch, and expression data into granular control of DDSP synthesiser modules.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/MIDI_intro-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/MIDI_intro-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/MIDI_intro-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/MIDI_intro.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;em&gt;Figure 1: Hierarchy in music synthesis &lt;d-cite key=&quot;engel2020ddsp&quot;&gt;&lt;/d-cite&gt;&lt;/em&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;It has 3 level hierarchy:notes, performance, synthesis as shown in above figure &lt;br /&gt; Notes : Similar to how composer writes series of Notes &lt;br /&gt; Performance : Similar to how performer articulates these notes into dynamics, and expression in music. &lt;br /&gt; Synthesis : Similar to how the expression are then converted to audio by short-time pitch and timbre changes of the physical vibration. &lt;br /&gt;&lt;/p&gt; &lt;p&gt;MIDI-DDSP can be viewed similarly to a multi-level autoencoder. I has 3 separately trainable modules (DDSP Inference, Synthesis Generator, Expression Generator)&lt;br /&gt; &lt;strong&gt;DDSP Inference&lt;/strong&gt; - The DDSP Inference module learns to make predictions about synthesis parameters from audio and then applies those learnings to resynthesized audio using an audio reconstruction loss.&lt;br /&gt; &lt;strong&gt;Synthesis Generator&lt;/strong&gt; - The Synthesis Generator module makes predictions regarding synthesis parameters based on notes and the expression qualities associated with those notes. These predictions are then iterated through the use of a reconstruction loss and an adversarial loss. &lt;br /&gt; &lt;strong&gt;Expression Generator&lt;/strong&gt; - The Expression Generator module uses autoregressive modelling to provide predictions about note expressions based on a given note sequence which is trained via teacher forcing. &lt;br /&gt;&lt;/p&gt; &lt;h3 id=&quot;but-what-is-ddsp&quot;&gt;BUT what is DDSP!&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Challenges of neural audio synthesis and how DDSP overcomes it&lt;/strong&gt; &lt;br /&gt; As shown in below Figure 1(left) shows that strided convolution models generate waveform with overlapping frames and suffer from phase alignment problem. Here phase alignment comes from recording of the same source made with 2 or mics placed at different distance. This distance variation cause the sound to arrive at mics at slightly different times. Figure 1(center) shows spectral leakage which occurs when the Fourier basis frequencies do not completely match the audio, where sinusoids at several nearby frequencies and phases need to be blended to represent a single sinusoid. Although the three waveforms on the right side of Figure 1 appear to have the same sound (a relative phase offset of the harmonics), an autoregressive model would find them to have very different losses. This represent inefficiency of model such that waveform shape does not correspond to perception.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/challenges-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/challenges-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/challenges-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/challenges.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;em&gt;Figure 3: Challenges in neural audio synthesis &lt;d-cite key=&quot;engel2020ddsp&quot;&gt;&lt;/d-cite&gt;&lt;/em&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;DDSP model overcomes above challenge and gain an advantage from the inductive bias of using oscillators while preserving the expressive power of neural networks and end-to-end training.&lt;/p&gt; &lt;d-cite key=&quot;engel2020ddsp&quot;&gt;&lt;/d-cite&gt; &lt;p&gt;&lt;strong&gt;Why making the synthesis differentiable is important?&lt;/strong&gt; The harmonics-plus-noise model, a differentiable additive synthesis model, generates audio in the paper. A sinusoids-plus-noise model version. The harmonics-plus-noise model is a synthesiser, but it requires specifying each harmonic’s amplitude and the filter’s frequency response. The harmonics plus-noise synthesiser accurately recreates actual instrument sounds, but its complex synthesis settings prevent direct engagement.&lt;d-cite key=&quot;masuda2021synthesizer&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/ddsp.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/ddsp.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/ddsp.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/ddsp.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;em&gt; Figure 4:Decomposition of a clip of solo violin.&lt;/em&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;As seen in above animation, the signals for loudness and fundamental frequency are taken from the original audio. As a result of the impacts of the room acoustics, the loudness curve does not reveal clearly differentiated note segmentations. These conditioning signals are input into the DDSP autoencoder, which then makes predictions about amplitudes, harmonic distributions, and noise magnitudes. The entire resynthesis audio is produced by applying the extracted impulse response to the synthesiser audio.&lt;/p&gt; &lt;p&gt;3 main design components: &lt;strong&gt;Expressive&lt;/strong&gt; - due to more params in the synthesizer (also the ability to control the generation process) &lt;strong&gt;Interpretable&lt;/strong&gt; - because of the harmonic oscillator assumption i.e. relying upon fundamental frequency and loudness &lt;strong&gt;Adaptable&lt;/strong&gt; - interpolation between different instruments&lt;/p&gt; &lt;h2 id=&quot;midi-ddsp-architecturesummary&quot;&gt;MIDI-DDSP architecture(summary)&lt;/h2&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/hierachy_generation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/hierachy_generation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/hierachy_generation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/hierachy_generation.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;em&gt;Figure 2: MIDI-DDSP modules &lt;d-cite key=&quot;engel2020ddsp&quot;&gt;&lt;/d-cite&gt;&lt;/em&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;This figure gives a high level view of the MIDI-DDSP modules. We also show that full-stack automatic music generation is possible when MIDI-DDSP is combined with a pretrained note generating model.&lt;/p&gt; &lt;h3 id=&quot;expression-generator&quot;&gt;Expression Generator&lt;/h3&gt; &lt;p&gt;Expression Generator is mainly an autoregressive RNN that is trained to predict “expression controls” from the note sequence. These are the synthesis parameters that will be used in the next network; the synthesis generator.&lt;/p&gt; &lt;p&gt;So what are these Expression Controls that we speak of, you might ask! These controls also represents few of the choices that the performer makes while performing a composed track. Following is the list of controls applied:&lt;/p&gt; &lt;p&gt;&lt;em&gt;Volume&lt;/em&gt;: Controls how loud a note is. &lt;em&gt;Volume fluctuation&lt;/em&gt;: Controls how the loudness of a note changes over the note. &lt;em&gt;Volume peak position&lt;/em&gt;: Controls the location of the peak volume during the course of a note. &lt;em&gt;Vibrato&lt;/em&gt;: Controls the degree of a note’s vibrato, where Vibrato is a musical effect or a technique where a note changes pitch subtly and quickly &lt;em&gt;Brightness&lt;/em&gt;: increases in value produce louder high-frequency harmonics, which in turn controls the timbre of the note. &lt;em&gt;Attack Noise&lt;/em&gt;: Controls the amount of noise at the note’s beginning.&lt;/p&gt; &lt;h3 id=&quot;synthesis-generator&quot;&gt;Synthesis Generator&lt;/h3&gt; &lt;p&gt;Synthesis Generator again is an autoregressive RNN used to predict fundamental frequency, given a conditioning sequence from the previous module. It might now sound obvious that these params are in turn used in the next module, which is the DDSP Inference.&lt;/p&gt; &lt;h3 id=&quot;interaction-with-ddsp-interface&quot;&gt;Interaction with DDSP interface&lt;/h3&gt; &lt;p&gt;⇒ How is this different from DDSP? CNN is utilised on a logarithmic scale. Mel-spectrograms aid models in obtaining more data from audio input, enabling more precise estimation of synthesis parameter. A fully linked network is used on fundamental frequency and loudness in our DDSP inference module. In order to extract contextual information, the output is concatenated with the CNN output and sent to the bi-directional LSTM. To map the characteristics to the synthesis settings, another fully connected layer is utilised.&lt;/p&gt; &lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/Results1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/Results1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/Results1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/Results1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;em&gt;Comparison of reconstruction audio accuracy&lt;d-cite key=&quot;wu2021midi&quot;&gt;&lt;/d-cite&gt;&lt;/em&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;As Shown in above figure in left there is comparison of Mel spectrogram of synthesis results and on right it shows comparison of synthesis quality from listening test. From figure(right) it is seen that MIDI-DDSP inference is perceived as likely as ground truth compared to other methods such as MIDI2Params, Ableton and FluidSynth.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/Results-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/Results-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/Results-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-02-controllable-music-generation-via-midi-ddsp/Results.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;Above figure shows pearson correlation between the input control and the respective output quantity. It is seen that there is strong correlation between input control and note expression output.&lt;/p&gt; &lt;h2 id=&quot;future-direction&quot;&gt;Future direction&lt;/h2&gt; &lt;p&gt;Author mentions in paper that one promising direction for future research is to apply this method to polyphonic recordings by means of multi-instrument transcription and multi-pitch tracking. When making differentiable it goes through many combinations and the model explore all the different possibilities and backpropagate through the soft weighting of all those possible paths, which can very soon become intractable and Reinforcement Learning could possibly be used for this search(as it is a combinatorial problem-&amp;gt;additive synthesis).&lt;/p&gt; </content> </entry> <entry> <title>Adaptive Reward Penalty in Safe Reinforcement Learning</title> <link href="https://jocelynshen.com/blog/2023/Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/"/> <updated>2023-01-31T00:00:00+00:00</updated> <id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning</id> <content type="html">&lt;h1 id=&quot;introduction-to-safe-reinforcement-learning&quot;&gt;Introduction to Safe Reinforcement Learning&lt;/h1&gt; &lt;p&gt;Safe RL can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or the deployment processes &lt;d-cite key=&quot;garcia_comprehensive_2015&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;center&gt; &lt;video autoplay=&quot;&quot; muted=&quot;&quot; loop=&quot;&quot; controls=&quot;&quot; src=&quot;https://jocelynshen.com/deep-learning-blog.github.io/assets/video/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/RL_boat_racing.mp4&quot; style=&quot;width:500px&quot; type=&quot;video/mp4&quot;&gt; &lt;/video&gt; &lt;figcaption&gt; Open AIs CoastRunners agent from their blog post &lt;a href=&quot;https://openai.com/blog/faulty-reward-functions&quot;&gt;&quot;Faulty Reward Functions in the Wild&quot;&lt;/a&gt; in Dec 2016.&lt;/figcaption&gt; &lt;/center&gt; &lt;p&gt;Defining a reward function is crucial in &lt;a href=&quot;https://lilianweng.github.io/posts/2018-02-19-rl-overview/&quot;&gt;Reinforcement Learning&lt;/a&gt; for solving many problems of interest in AI. It is often based on the designers’ intuition of the goal of the system. In the above example of CoastRunners, the goal is to reach the finish line and collect points along the way. Whilst selecting the in-game score the player earned as a reflection of the informal goal of finishing the race is a reasonable reward function, it allows for dangerous and harmful behavior, as visible in the video above. The agent can drive off the track, crash into other boats, and catch fire and still win the game whilst achieving a score on average 20 percent higher than that achieved by human players.&lt;/p&gt; &lt;p&gt;How can we prevent the agents from violating safety constraints (e.g., crashing into other boats)? Recent studies have started to address the problem of safe reinforcement learning from various perspectives, ICLR works including, but not limited to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=HJgEMpVFwB&quot;&gt;Adversarial Policies: Attacking Deep Reinforcement Learning&lt;/a&gt;, Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell, &lt;strong&gt;ICLR 2020&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2201.09802.pdf&quot;&gt;Constrained Policy Optimization via Bayesian World Models&lt;/a&gt;, Yarden As, Ilnura Usmanova, Sebastian Curi and Andreas Krause, &lt;strong&gt;ICLR 2022&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=TBIzh9b5eaz&quot;&gt;Risk-averse Offline Reinforcement Learning&lt;/a&gt;, Núria Armengol Urpí, Sebastian Curi, and Andreas Krause, &lt;strong&gt;ICLR 2021&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=S1vuO-bCW&quot;&gt;Leave No Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning&lt;/a&gt;, Benjamin Eysenbach, Shixiang Gu, Julian Ibarz, and Sergey Levine, &lt;strong&gt;ICLR 2018&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=iaO86DUuKi&quot;&gt;Conservative Safety Critics for Exploration&lt;/a&gt;, Homanga Bharadhwaj, Aviral Kumar, Nicholas Rhinehart, Sergey Levine, Florian Shkurti, and Animesh Garg, &lt;strong&gt;ICLR 2021&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=TQt98Ya7UMP&quot;&gt;Balancing Constraints and Rewards with Meta-gradient D4PG&lt;/a&gt;, Dan A. Calian, Daniel J. Mankowitz, Tom Zahavy, Zhongwen Xu, Junhyuk Oh, Nir Levine, and Timothy Mann, &lt;strong&gt;ICLR 2021&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We chose to illustrate the method of Reward Constrained Policy Optimization (RCPO) &lt;d-cite key=&quot;Tessler2018RCPO&quot;&gt;&lt;/d-cite&gt; in this blog post because it is a simple yet effective method of introducing the ideas of safe RL. By providing a high-level constraint, the agent learns to respect it and achieve the perfect balance between meeting that constraint and maximizing the reward. Moreover, this removes the need to manually extend and tune the reward function since it is adaptively shaped during the learning!&lt;/p&gt; &lt;h1 id=&quot;a-formalism-for-safe-reinforcement-learning-constrained-mdps&quot;&gt;A Formalism for Safe Reinforcement Learning: Constrained MDPs&lt;/h1&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/CMDP_Illustration-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/CMDP_Illustration-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/CMDP_Illustration-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/CMDP_Illustration.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Illustration of a Constrained Markov Decision Process (MDP) adapted from &lt;a href=&quot;https://lilianweng.github.io/posts/2018-02-19-rl-overview&quot;&gt;Lilian Weng&lt;/a&gt;. &lt;br /&gt; Based on an observation (also called state) from the environment, the agent selects an action. This action is executed in an environment resulting in a new state and a reward that evaluates the action. Given the new state, the feedback loop repeats. &lt;/div&gt; &lt;p&gt;In Reinforcement Learning, the world is modeled as a Markov Decision Process (MDP) and the goal is to select a policy \(\pi\) which maximizes an expected cumulative reward \(J^π\).&lt;/p&gt; &lt;p&gt;\(J^π\) can be taken to be the infinite horizon discounted total return as&lt;/p&gt; \[J^\pi = \mathbb{E}_{s\sim\mu} \left[ \sum_{t=0}^T \gamma^t r(s_t, a_t) \right]\] &lt;p&gt;where \(\gamma\) is the discount factor, and \(r(s_t,a_t)\) is the reward function.&lt;/p&gt; &lt;p&gt;However, the agents must obey safety constraints in many real-world applications while achieving the goal. We can introduce a constraint objective analogous to the reward objective. This objective is typically defined as the expected constraint value over N time steps \(J^π_C = \mathbb{E}_{s\sim\mu} \left[ C(s) \right]\). The method of aggregating individual constraints over time can vary, e.g., using the average or the maximum constraint value over N time steps or even a discounted sum.&lt;/p&gt; &lt;p&gt;In the example of the robot, the aim could be to prolong the motor life of the various robots while still enabling them to perform the task at hand. Thus we constrain the robot motors from using high torque values. Here, constraint C is defined as the average torque the agent has applied to each motor, and the penalty \(c(s, a)\) becomes the average amount of torque the agent decided to use at each time step.&lt;/p&gt; &lt;p&gt;We limit the allowable amount of torque applied to \(\alpha\). &lt;br /&gt; The constrained MDP for our safe reinforcement learning problem is:&lt;/p&gt; \[\max_{\pi \in \Pi} J^\pi_R \text{ s.t. } J^\pi_C \leq \alpha\] &lt;h1 id=&quot;constrained-policy-optimization&quot;&gt;Constrained Policy Optimization&lt;/h1&gt; &lt;p&gt;Constrained objectives are often solved using the Lagrange relaxation technique. With parameterized approaches such as Neural Networks, the objective is then to find the networks parameters \(\theta\) that maximize \(J^\pi_R\) subject to the constraint \(J^\pi_C \leq \alpha\) given the Lagrangian multiplier \(\lambda\):&lt;/p&gt; \[\min_{\lambda}\max_{\theta} [J^{\pi_\theta}_R - \lambda (J^{\pi_\theta}_C - \alpha)]\] &lt;p&gt;We now have our new global objective function that is subject to optimization!&lt;/p&gt; &lt;h3 id=&quot;what-exactly-does-the-lagrangian-do&quot;&gt;What exactly does the Lagrangian do?&lt;/h3&gt; &lt;p&gt;Intuitively, the Lagrangian multiplier \(\lambda\) determines how much weight is put onto the constraint. If \(\lambda\) is set to 0, the constraint is ignored, and the objective becomes the reward objective \(J^\pi_R\). If \(\lambda\) is set very high, the constraint is enforced very strictly, and the global objective function reduces to the constraint objective \(J^π_C\). Let’s look at a simple example to &lt;strong&gt;demonstrate the effect of the Lagrangian multiplier \(\lambda\)&lt;/strong&gt;. We’ll use the simple CartPole Gym environment. The reward in this environment is +1 for every step the pole was kept upright.&lt;/p&gt; &lt;p&gt;We can now add an example constraint to the environment. Let’s say we want to keep the cart in the left quarter of the x-axis. We, therefore, define the constraint value as the x-position of the cart and the upper bound \(\alpha\) as -2.&lt;/p&gt; &lt;p&gt;Let’s see how with different lambda values, the constraint is enforced.&lt;/p&gt; &lt;center&gt; &lt;video autoplay=&quot;&quot; muted=&quot;&quot; loop=&quot;&quot; controls=&quot;&quot; src=&quot;https://jocelynshen.com/deep-learning-blog.github.io/assets/video/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/cart_pole_lambda.mp4&quot; style=&quot;width:500px&quot; type=&quot;video/mp4&quot;&gt; &lt;/video&gt; &lt;figcaption&gt; The green area represents the &quot;safe zone&quot;, where the x-position is smaller than -2, and the red area is the &quot;unsafe zone&quot;. &lt;br /&gt; The lower the lambda, the more the constraint is ignored. The higher the lambda, the more the constraint is enforced, and the main reward objective is ignored. At λ = 1,000,000 the cart shoots to the right to tilt the pole to the left but does so ignoring the following balancing act, which is observable at λ ∈ {10, 100}.&lt;/figcaption&gt; &lt;/center&gt; &lt;p&gt;Tuning the \(\lambda\) through &lt;a href=&quot;https://gibberblot.github.io/rl-notes/single-agent/reward-shaping.html&quot;&gt;reward shaping&lt;/a&gt; is no easy feat. The Lagrangian is a scaling factor, i.e., if the constraint values are inherently larger than the reward values, we will need a substantially lower \(\lambda\) than when the constraint values are significantly smaller than the possible reward values. That means that the range of good lambda values is large and differs with every environment.&lt;/p&gt; &lt;h3 id=&quot;how-can-we-learn-an-optimal-lagrangian&quot;&gt;How can we learn an optimal Lagrangian?&lt;/h3&gt; &lt;p&gt;Luckily, it is possible to &lt;strong&gt;view the Lagrangian as a learnable parameter&lt;/strong&gt; and update it through gradient descent since the globally constrained optimization objective \(J^{\pi_{\theta}}\) is differentiable. In short, we can simply use the derivative of the objective function w.r.t \(\lambda\) and update the Lagrangian.&lt;/p&gt; \[\frac{\partial J^{\pi_{\theta}}}{\partial \lambda} = -(J^{\pi_{\theta}}_C - \alpha)\] \[\lambda \gets max(\lambda - lr_{\lambda}(-(\mathbb{E}^{\pi_\theta}_{s\sim\mu} \left[C\right] - \alpha)), 0)\] &lt;p&gt;Hereby \(lr_{\lambda}\) is the learning rate for the Lagrangian multiplier. The max function ensures that the Lagrangian multiplier is always positive.&lt;/p&gt; &lt;h1 id=&quot;reward-constrained-policy-optimization&quot;&gt;Reward Constrained Policy Optimization&lt;/h1&gt; &lt;p&gt;Actor-Critic based approaches such as &lt;a href=&quot;https://spinningup.openai.com/en/latest/algorithms/ppo.html&quot;&gt;PPO&lt;/a&gt; &lt;d-cite key=&quot;Schulman2017PPO&quot;&gt;&lt;/d-cite&gt; have empirically been shown to compete at the top of a plethora of quality benchmarks. In this class of algorithms, the actor learns a policy \(\pi\), whereas the critic learns the value function using temporal difference learning. Intuitively, using the critic reduced the variance and enabled training using a finite number of samples.&lt;/p&gt; &lt;h3 id=&quot;how-to-integrate-the-constraint-into-the-actor-critic-approach&quot;&gt;How to integrate the constraint into the Actor-Critic approach?&lt;/h3&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/Algorithm-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/Algorithm-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/Algorithm-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/Algorithm.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;If we look at the RCPO algorithm illustrated above, we can see that implementing the constraint into the Actor-Critic approach is done in a few lines of code. First, we need to collect the constraint during the policy rollout. Then we can integrate the constraint values (the guiding penalty) into the reward during the computation of the policy and value gradients, as demonstrated in line 7. &lt;br /&gt; This is done by formulating the constraint as the infinite horizon discounted total cost, similar to the usual returns of an MDP.&lt;/p&gt; \[J^\pi_{C_\gamma}(s) \hat{=} \mathbb{E}^{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t c(s_t, a_t) | s_0 = s \right]\] &lt;p&gt;Now we can simply include the guiding penalty to the reward function via the Lagrange multiplier to arrive at the penalized reward function:&lt;/p&gt; \[\hat{r} = r(s,a) - \lambda c(s,a)\] &lt;p&gt;Finally, we can compute the gradient of the Lagrangian in line 11 and update \(\lambda\) in line 14 as discussed in the previous section and repeat the whole process for \(K\) times.&lt;/p&gt; &lt;h1 id=&quot;implementation&quot;&gt;Implementation&lt;/h1&gt; &lt;p&gt;To facilitate reproducibility, we integrated RCPO into the stable-baselines3&lt;d-cite key=&quot;stable-baselines3&quot;&gt;&lt;/d-cite&gt; PPO implementation.&lt;/p&gt; &lt;h3 id=&quot;integrating-the-guiding-penalty&quot;&gt;Integrating the guiding penalty&lt;/h3&gt; &lt;p&gt;For the computation of returns with PPO, we use the Temporal Difference Error (TD estimate) and the Generalized Advantage Estimation (GAE) advantage. &lt;br /&gt; To integrate the constraint into the reward function, we need to add the Lagrangian-scaled constraint value to the reward, as discussed in the RCPO section. This is done when computing the TD error estimate.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;compute_returns_and_advantages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;last_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;th&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndarray&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# ... &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rewards&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constraint_lambda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constraints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_non_terminal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# ... &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;blockquote&gt; &lt;p&gt;The discussed integration of the constraint into the reward function is implemented into the computation of the advantages and returns. When the lambda parameter is set to 0, the constraint is ignored and the reward function is the same as in the original PPO implementation.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Additionally, it was necessary to extend the rollout buffer to collect the constraint values at each time step. To receive the constraint values, we customized the gym environments to return those in the info dictionary.&lt;/p&gt; &lt;h3 id=&quot;updating-the-lagrangian-multiplier&quot;&gt;Updating the Lagrangian multiplier&lt;/h3&gt; &lt;p&gt;Due to the fact that PPO (1) collects multiple episodes until the rollout buffers are full and (2) supports vectorized environments, the logic for collecting and aggregating the constraint values across the episodes and parallel environments is a bit more complex. &lt;br /&gt; Nevertheless, we have chosen the aggregation method to be the average over all time steps in one complete episode and across all those episodes themselves, i.e., episodes that have reached a terminal state.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# lambda &amp;lt;- lambda - lr_lambda * -(C - alpha) = lambda + lr_lambda * (C - alpha) &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_constraint_lambda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constraint_lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rollout_buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constraint_lambda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr_constraint_lambda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_constraint_lambda&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rollout_buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constraint_lambda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rollout_buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constraint_lambda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;blockquote&gt; &lt;p&gt;After aggregating the constraint values across the episodes and parallel environments into self.C, the Lagrangian is updated using gradient descent. The max function is used to ensure that the Lagrangian multiplier is always positive.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt; &lt;p&gt;As a proof-of-the-principle experiment, we reproduced the HalfCheetah task in &lt;a href=&quot;https://gymnasium.farama.org/environments/mujoco/&quot;&gt;OpenAI MuJoCo Gym&lt;/a&gt; from Tessler C. et al.&lt;d-cite key=&quot;Tessler2018RCPO&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;The results of the experiments are shown in the following figures. We kept (almost) all hyperparameters the same as in the original paper and let the agents train for \(1,000,000\) time steps.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/experiments_results_smooth_constraints-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/experiments_results_smooth_constraints-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/experiments_results_smooth_constraints-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/experiments_results_smooth_constraints.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Rewards and average torque of the experiments on the HalfCheetah environment. The x-axis represents the time steps and the maximum torque constraint is illustrated by the dashed line. &lt;/div&gt; &lt;p&gt;The results demonstrate that the RCPPO trained an agent that successfully walked forward while respecting the safety constraint. We achieved comparable results to the original experiments in the paper. &lt;br /&gt; Interestingly, low \(\lambda\) values seem to be less stable than higher \(\lambda\) values. The guiding penalty appears to enforce the constraint and improve the learning process overall. They limit the amount of torque the agent is allowed to apply, hinder the exploration of unsafe and poor-performing local minima and guide the policy to a safe and more optimal solution. &lt;br /&gt; Nevertheless, the poor performance of the unconstrained agents may be due to the neural network architecture being relatively small (i.e., 2 layers of 64 hidden units).&lt;/p&gt; &lt;h3 id=&quot;qualitative-observations&quot;&gt;Qualitative observations&lt;/h3&gt; &lt;p&gt;Finally ,let’s see how our HalfCheetah agents walk under the To do so, we have recorded videos of the agents walking forward with different \(\lambda\) values. The results can be seen below.&lt;/p&gt; &lt;center&gt; &lt;video autoplay=&quot;&quot; muted=&quot;&quot; loop=&quot;&quot; controls=&quot;&quot; src=&quot;https://jocelynshen.com/deep-learning-blog.github.io/assets/video/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/HalfCheetah_Experiments.mp4&quot; style=&quot;width:500px&quot; type=&quot;video/mp4&quot;&gt; &lt;/video&gt; &lt;figcaption&gt;Visualization of the HalfCheetah agents learned through RCPPO and with different selected Lagrangian multipliers.&lt;/figcaption&gt; &lt;/center&gt; &lt;p&gt;We can again observe that the lower the lambda is, the more the constraint is ignored and the higher the lambda, the more the constraint is enforced and the main reward objective is ignored.&lt;br /&gt; At λ ∈ {10, 100}, the robot applies 0 torque to ultimately oblige to the constraint ignoring the main reward objective to walk forward, which is observable at λ ∈ {RCPPO, 0, 0.00001}. With λ ∈ {0, 0.00001} the robot can walk forward, but it is visible that it moves its legs much quicker and more aggressively than the RCPPO agent. Furthermore, the RCPPO agent walks perfectly, whilst the other (moving) agents tumble over their own hecktick steps.&lt;/p&gt; &lt;h1 id=&quot;discussion&quot;&gt;Discussion&lt;/h1&gt; &lt;h3 id=&quot;theoretical-assumptions-vs-empirical-results&quot;&gt;Theoretical assumptions vs. empirical results&lt;/h3&gt; &lt;p&gt;We had to select higher values for the Lagrangian multiplier than what were used in the original paper. In the paper, a \(\lambda\) value of 0.1 is already very high as it leads to a reward of \(-0.4\) and torque of \(0.1387\), whereas in our case a \(\lambda\) value of \(1.0\) leads to a reward of about \(1 500\) with an average torque of \(0.39\). &lt;br /&gt; This affected the reward shaping process but also meant we had to increase the Lagrangian’s respective learning rate when training it as a parameter to grow quicker. As a result, \(lr_{\lambda}\) becomes larger than \(lr_{\pi}\), which &lt;strong&gt;ignores one of the assumptions made in the paper&lt;/strong&gt;, yet leads to coherent results.&lt;/p&gt; &lt;p&gt;A possible reason for the slower and weaker impact of the constraint could be attributed to the clipping of the trust region. This technique ensures that the policy does not change too much between updates and prevents it from landing in a bad local minimum that it can not escape. This is done by clipping the policy update to a specific range. Therefore, even with “high” values of lambda w.r.t. the original paper, the policy will not change significantly to conform to the constraint.&lt;/p&gt; &lt;p&gt;Not only did we have to select a higher learning rate for the Lagrangian, but we also did not include different learning rates for the policy and the value function, &lt;strong&gt;ignoring the three times scales approach&lt;/strong&gt; proposed in the original paper. Additionally, in the original paper the RCPPO algorithm updated their networks (actor and critic) after each episode. In our implementation, we need to fill the rollout buffer with potentially multiple episodes, thus reducing the frequency of network parameters and Lagrangian updates. Nevertheless, the PPO algorithm implements a parameter update loop of n epochs after each rollout, which to a degree counteracts the discussed lower update frequency of all parameters.&lt;/p&gt; &lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt; &lt;p&gt;The results of the experiments show that the RCPO approach can learn a policy that can optimize the main reward objective while respecting the constraint.&lt;/p&gt; &lt;p&gt;Safe Reinforcement Learning is a critical area of research in the field of artificial intelligence, as it has the potential to shape the future of autonomous systems in a multitude of domains, ranging from robotics to finance. &lt;br /&gt; The more complex systems become, the more difficult it is to ensure safety requirements, especially through simple reward shaping. An approach such as RCPO can ensure that the safety constraints are respected while enforcing them by only providing the constraint itself.&lt;/p&gt; </content> </entry> <entry> <title>Underfitting and Regularization: Finding the Right Balance</title> <link href="https://jocelynshen.com/blog/2023/Underfitting-and-Regularization-Finding-the-Right-Balance/"/> <updated>2023-01-15T00:00:00+00:00</updated> <id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2023/Underfitting-and-Regularization-Finding-the-Right-Balance</id> <content type="html">&lt;h2 id=&quot;goal-of-this-blog-post&quot;&gt;Goal of this blog post&lt;/h2&gt; &lt;p&gt;Network Augmentation aka NetAug &lt;d-cite key=&quot;DBLP:conf/iclr/CaiG0022&quot;&gt;&lt;/d-cite&gt; caters to training small neural network architectures like MobileNetV2-Tiny for the best top-k percent accuracy. The paper argues that training small neural networks technically differs from that of large neural networks because the former is prone to underfitting. NetAug is contrary to traditional methods like dropout&lt;d-cite key=&quot;10.5555/2627435.2670313&quot;&gt;&lt;/d-cite&gt;, network pruning&lt;d-cite key=&quot;9043731&quot;&gt;&lt;/d-cite&gt;, quantization&lt;d-cite key=&quot;jacob2018quantization&quot;&gt;&lt;/d-cite&gt;, data augmentation&lt;d-cite key=&quot;https://doi.org/10.48550/arxiv.1712.04621&quot;&gt;&lt;/d-cite&gt; and other regularization techniques&lt;d-cite key=&quot;10.1007/s10462-019-09784-7&quot;&gt;&lt;/d-cite&gt; NetAug can be viewed as reversed form of dropout, as we enlarge the target model during the training phase instead of shrinking it. In this blog post, we identify some pitfalls with NetAug and propose potential workarounds.&lt;/p&gt; &lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt; &lt;p&gt;NetAug solely focuses on improving the performance of the tiny neural networks during inference, whilst optimizing their memory footprint to deploy them on edge devices. Tiny neural networks are usually inclined to underfit. Hence, the traditional training paradigms will not work for these small models because they fundamentally tackle the problem of overfitting and not overcome the underfitting issue. Several techniques in the recent time like data augmentation, pruning, dropout, knowledge distillation have been proposed to improve the generalizability of neural networks.&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;dl&gt; &lt;dt&gt;&lt;strong&gt;Knowledge Distillation&lt;/strong&gt;&lt;d-cite key=&quot;hinton2015distilling&quot;&gt;&lt;/d-cite&gt;&lt;/dt&gt; &lt;dd&gt;It is quite difficult to deploy and maintain an ensemble. However, previous research has shown that it is possible to learn a single cumbersome model that has the same performance as an ensemble. In most knowledge distillation methods there exist a large teacher model that transfers its knowledge as a learned mapping via training to a small student model with the teacher models output. There exists several techniques like self-distillation in which the teacher model trains itself continuously. Convectional KD methods try to optimise the objective function such that the loss function penalizes the difference between the student and teacher model.&lt;/dd&gt; &lt;/dl&gt; &lt;/li&gt; &lt;li&gt; &lt;dl&gt; &lt;dt&gt;&lt;strong&gt;Regularization&lt;/strong&gt;&lt;d-cite key=&quot;10.1007/s10462-019-09784-7&quot;&gt;&lt;/d-cite&gt;&lt;/dt&gt; &lt;dd&gt;Regularization is used to prevent overfitting of any ML model by reducing the variance, penalizing the model coefficients and complexity of the model. Regularization techniques are mainly composed of data augmentation one of the most simplest and conveninet ways to expand the size of the dataset such that it prevents overfitting issues that occur with a relatively small dataset. Dropout is also popularly applied while training models, in which at every iteration incoming and outgoing connections between certain nodes are randomly dropped based on a particular probability and the remaining neural network is trained normally.&lt;/dd&gt; &lt;/dl&gt; &lt;/li&gt; &lt;li&gt; &lt;dl&gt; &lt;dt&gt;&lt;strong&gt;Tiny Deep learning&lt;/strong&gt;&lt;d-cite key=&quot;lin2020mcunet&quot;&gt;&lt;/d-cite&gt;,&lt;d-cite key=&quot;lin2021mcunetv2&quot;&gt;&lt;/d-cite&gt;,&lt;d-cite key=&quot;lin2022ondevice&quot;&gt;&lt;/d-cite&gt;&lt;/dt&gt; &lt;dd&gt;Several challenges are paved while transitioning from conventional high end ML systems to low level clients, maintaining the accuracy of learning models, provide train-to-deploy facility in resource economical tiny edge devices, optimizing processing capacity. This method includes AutoML procedures for designing automatic techniques for architecturing apt neural netowrks for a given target hardware platform includes customised fast trained models,auto channel pruning methods and auto mixed precision quantization. The other approaches like AutoAugment methods automatically searches for improvised data augmentation within the network to prevent overfitting. There exists network slimming methods to reduce model size, decrease the run-time memory footprint and computing resource.&lt;/dd&gt; &lt;/dl&gt; &lt;/li&gt; &lt;li&gt; &lt;dl&gt; &lt;dt&gt;&lt;strong&gt;Network Augmentation&lt;/strong&gt;&lt;d-cite key=&quot;DBLP:conf/iclr/CaiG0022&quot;&gt;&lt;/d-cite&gt;&lt;/dt&gt; &lt;dd&gt;This method was proposed to solve the problem of underfitting in tiny neural networks. This is done by augmenting the given model (referred to as base model) into a larger model and encourage it to work as a sub-model to get extra supervision in additoin to functioning independently. This will help in increasing the representation power of the base model because of the gradient flow from the larger model and it can be viewed equivalently as “&lt;strong&gt;reverse-dropout&lt;/strong&gt;”.&lt;/dd&gt; &lt;/dl&gt; &lt;/li&gt; &lt;li&gt; &lt;dl&gt; &lt;dt&gt;&lt;strong&gt;Neural Architechture Search (NAS)&lt;/strong&gt;&lt;d-cite key=&quot;https://doi.org/10.48550/arxiv.1808.05377&quot;&gt;&lt;/d-cite&gt;&lt;/dt&gt; &lt;dd&gt;This method was proposed to solve the problem of architecture optimization and weight optimization based on the underlying training data. NAS usually involves defining an architectural search space and then searching for the best architecture based on the performance on the validation set. Recent approaches include weight sharing, nested optimization and joint optimization during training. However, there are drawbacks in using these approaches because they are computationally expensive and suffer from coupling between architecture parameters and model weights. This will degrade the performance of the inherited weights.&lt;/dd&gt; &lt;/dl&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;formulation-of-netaug&quot;&gt;Formulation of NetAug&lt;/h2&gt; &lt;p&gt;The end goal of any ML model is to be able to minimize the loss function with the help of gradient descent. Since tiny neural network have a very small capacity the gradient descent is likely to get stuck in local minima. Instead of traditional regularization techniques which add noise to data and model, NetAug proposes a way to increase the capacity of the tiny model without changing its architecture for efficient deployment and inference on edge devices. This is done by augmenting the tiny model (referred to as base model) into a larger model and jointly training both the base model independently and also the augmented model so that the base model benefits from the extra supervision it receives from the augmented model. However, during inference only the base model is used.&lt;/p&gt; &lt;p&gt;To speed-up the training, a single largest augmented model is constructed by augmenting the width of the each layer of the base model using an augmentation factor \(r\). After building the largest augmented model, we construct other augmented models by selecting a subset of channels from the largest augmented model. NetAug proposes a hyper-parameter \(s\), named diversity factor, to control the number of augmented model configurations. We set the augmented widths to be linearly spaced between \(w\) and \(r \times w\). For instance, with \(r = 3\) and \(s = 2\), the possible widths would be \([w, 2w, 3w]\).&lt;/p&gt; &lt;h2 id=&quot;pitfalls-in-netaug&quot;&gt;Pitfalls in NetAug&lt;/h2&gt; &lt;ol&gt; &lt;li&gt; &lt;dl&gt; &lt;dt&gt;&lt;strong id=&quot;training_aug_models&quot;&gt;Training the Generated Augmented Models&lt;/strong&gt;&lt;/dt&gt; &lt;dd&gt;NetAug randomly samples sub-models from the largest model by augmenting the width instead of depth, its highly important to ensure we speed up the training time and reduce the number of traning iterations for these generated sub-models thereby enhancing the training convergence and reaching optimization faster. We theoretically aim at aiding this by introducing a re-parametrisation technique during training that involves sharing and unsharing of weights to attain convergence much faster. &lt;!-- Rough idea for solution: The parameter s is the diversity factor, to control the number of augmented model configurations. Try to penalise/introduce a reward for s to favor a particular configurations that perform better/provide better auxiliary supervision. --&gt;&lt;/dd&gt; &lt;/dl&gt; &lt;/li&gt; &lt;li&gt; &lt;dl&gt; &lt;dt&gt;&lt;strong id=&quot;naive_loss&quot;&gt;Naive Loss Function&lt;/strong&gt;&lt;/dt&gt; &lt;dd&gt;NetAug computes loss in a very trivial form, i.e, by simply performing a weighted sum over the loss of the base model with that from the respective sampled augmented models. However, it was mentioned in the paper that sampling more than one sub-models from the largest augmented model in each training step is resulting in degradation of the base model’s performance. This can be attributed to the fact that simple weighted sum of losses from the base supervision and auxiliary supervision is causing the auxiliary supervision to shadow the base model. We propose different mixing strategies to circumvent this problem. &lt;!--3. **Overfitting of the Augmented Models**: As the network grows lineary even the number of augmented models increase, thereby overfitting during the training process. Having large number of such models with a poor representation among them will simply increase the training overhead and will cause haphazard confusion while selecting the largest augmented model. NetAug focuses on design and implementing a single best largest, going ahead with just the largest paramater model would yield in bad accuracy and will worsen the overall model performance.--&gt;&lt;/dd&gt; &lt;/dl&gt; &lt;/li&gt; &lt;li&gt; &lt;dl&gt; &lt;dt&gt;&lt;strong id=&quot;generating_aug_model&quot;&gt;Generating the Largest Augmented Model&lt;/strong&gt;&lt;/dt&gt; &lt;dd&gt;For a particular network, rather than tuning for just a single network hyperparameter (i.e., network, depth, width etc.), what if we instead tune all the closely relevant network hyperparameters for every augmented sub-model? To advocate this it’s sensible to compare the entire distribution of hyperparameter across the model. This can be tackled using NAS to find the best largest augmented model and then use it for auxiliary supervision of the base model.&lt;/dd&gt; &lt;/dl&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;introducing-netaug-with-relation-knowledge-distribution-rkd&quot;&gt;Introducing NetAug with Relation Knowledge Distribution (RKD)&lt;/h2&gt; &lt;p&gt;Knowledge distillation in learned models is constituted of:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;dl&gt; &lt;dt&gt;&lt;strong&gt;Individual Knowledge Distillation&lt;/strong&gt;&lt;/dt&gt; &lt;dd&gt;Outputs of individual samples represented by the teacher and student are matched.&lt;/dd&gt; &lt;/dl&gt; &lt;/li&gt; &lt;li&gt; &lt;dl&gt; &lt;dt&gt;&lt;strong&gt;Relational Knowledge Distillation&lt;/strong&gt;&lt;/dt&gt; &lt;dd&gt;Relation among examples represented by the teacher and student are matched. RKD is a generalization of convectional knowledge distillation that combines with NetAug to boost the performance due to its complementarity with conventional KD, that aims at transferring structural knowledge using mutual relations of data examples in the teacher’s output presentation rather than individual output themselves. Contrary to conventional approaches called as Individual KD (IKD) that transfers individual outputs of the teacher model \(f_T(\cdot)\) to the student model \(f_S(\cdot)\) point-wise, RKD transfers relations of the outputs structure-wise and computes a relational potential \(\psi\) for every \(n\)-tuple of data instance and transfers the relevant information through the potential from the teacher to the student models. In addition to knowledge represented in the output layers and the intermediate layers of a neural network, knowledge that captures the relationship between feature maps are also used to train a student model.&lt;/dd&gt; &lt;/dl&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We specifically aim at training the teacher and student model in a online setting, in this online type of distillation training method both the teacher and the student model are trained together simultaneously.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/rkd-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/rkd-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/rkd-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/rkd.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;&lt;strong&gt;Relational Knowledge distillation can be expressed as&lt;/strong&gt; -&lt;br /&gt; Given a teacher model \(\:T\:\) and a student model \(S\), we denote \(f_T(\cdot)\) and \(f_S(\cdot)\) as the functions of the teacher and the student, respectively, and \(\psi\) as a function extracting the relation, we have&lt;/p&gt; \[\begin{equation} \mathcal{L}_{\text{RKD}} = \sum \limits_{\{x_1, \ldots, x_n\} \in \chi^N} l \big(\psi(t_1,\cdots,t_n), \: \psi(s_1, \cdots, s_n)\big) \end{equation}\] &lt;p&gt;where \(\mathcal{L}_{\text{RKD}}\) is the loss function, \(t_i = f_T(x_i)\) and \(s_i = f_S(x_i)\) and \(x_i \in \chi\) denotes the input data.&lt;/p&gt; &lt;h3 id=&quot;loss-functions-in-rkd&quot;&gt;Loss Functions in RKD&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Distance-wise distillation loss (pair)&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;div class=&quot;small_img row mt-1&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-D-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-D-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-D-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-D.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;This method is known as RKD-D. It transfers relative distance between points on embedding space. Mathematically, \(\begin{equation}\psi_{D}(t_i, t_j) = \frac{1}{\mu}\big\| t_i - t_j\big\|_2\end{equation}\) where \(\psi_d(\cdot, \cdot)\) denotes distance wise potential function \(\begin{equation}\mu = \frac{1}{|\chi^2|}\sum\limits_{(x_i, x_j) \in \chi^2} \big\| t_i - t_j\big\|_2\end{equation}\) \(\begin{equation}\boxed{\mathcal{L}_{\text{RKD-D}} = \sum \limits_{(x_i, x_j) \in \chi^2} l_\delta \big(\psi_D(t_i, t_j), \psi_D(s_i, s_j)\big)}\end{equation}\) where \(l_{\delta}\) denotes the Huber Los&lt;/p&gt; \[\begin{equation}l_\delta(x, y) = \begin{cases} \frac{1}{2} (x-y)^2\:\:\:\: \text{for } |x-y| \leq 1 \\ |x - y| - \frac{1}{2}\:\:\: \text{otherwise.} \end{cases}\end{equation}\] &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Angle-wise distillation loss (triplet)&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;div class=&quot;small_img row mt-1&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-A-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-A-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-A-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-A.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;This method is known as RKD-A. RKD-A transfers angle formed by three points on embedding space. Mathematically, \(\begin{equation}\psi_{A}(t_i, t_j, t_k) = \cos \angle t_it_jt_k = \langle \boldsymbol{e}^{ij}, \boldsymbol{e}{jk}\rangle\end{equation}\) where \(\psi_A(\cdot, \cdot, \cdot)\) denotes angle wise potential function \(\begin{equation}\boldsymbol{e}^{ij} = \frac{t_i - t_j}{\big\|t_i - t_j\big\|_2}, \: \boldsymbol{e}^{jk} = \frac{t_k - t_j}{\big\|t_k - t_j\big\|_2}\end{equation}\)&lt;/p&gt; \[\begin{equation}\boxed{\mathcal{L}_{\text{RKD-A}} = \sum\limits_{(x_i, x_j, x_k) \in \chi^3} l_\delta \big(\psi_A(t_i, t_j, t_k), \psi_A(s_i, s_j, s_k)\big)}\end{equation}\] &lt;h3 id=&quot;combining-rkd-with-netaug&quot;&gt;Combining RKD with NetAug&lt;/h3&gt; &lt;p&gt;We propose the following loss function to solve &lt;a href=&quot;#naive_loss&quot;&gt;naive loss problem of NetAug&lt;/a&gt;&lt;/p&gt; \[\begin{equation}\mathcal{L}_{\text{aug}} = \underbrace{\mathcal{L}(W_t)}_{\text{base supervision}} \:+\: \underbrace{\alpha_1 \mathcal{L}([W_t, W_1]) + \cdots + \alpha_i \mathcal{L}([W_t, W_i]) + \cdots}_{\text{auxiliary supervision, working as a sub-model of augmented models}} \:+\: \lambda_{\text{KD}}\,\underbrace{\mathbf{\mathcal{L}_{\text{RKD}}}}_{\text{relational knowledge distillation}}\label{eqn:loss_func}\end{equation}\] &lt;p&gt;where \([W_t, W_i]\) represents an augmented model where \([W_t]\) represents the tiny neural network and \([W_i]\) contains weight of the sub-model sampled from the largest augmented model, \(\alpha\) is scaling hyper-parameter for combining loss from different augmented models and finally \(\lambda_{\text{KD}}\) is a tunable hyperparameter to balance RKD and NetAug.&lt;/p&gt; &lt;h3 id=&quot;netaug-training-with-rkd&quot;&gt;NetAug Training with RKD&lt;/h3&gt; &lt;p&gt;In NetAug, they train only one model for every epoch, training all the augmented models all once is not only computationally expensive but also impacts the performance. The proportion of the base supervision will decrease when we sample more augmented networks, which will make the training process biased toward augmented networks and shadows the base model.&lt;/p&gt; &lt;p&gt;To further enhance the auxiliary supervision, we propose to use RKD in an online setting i.e., the largest augmented model will act as a teacher and the base model will act as a student. Both the teacher and the student are trained simultaneously.&lt;/p&gt; &lt;p&gt;We train the both the base model and the augmented model via gradient descent based on the loss function \(\eqref{eqn:loss_func}\). The gradient update for the base model is then given by&lt;/p&gt; \[\begin{equation}W^{n+1}_t = W^n_t - \eta \bigg(\frac{\partial \mathcal{L}(W^n_t)}{\partial W^n_t} + \alpha \frac{\partial \mathcal{L}([W^n_t, W^n_i])}{\partial W^n_t} + \lambda \frac{\partial \mathcal{L}_{\text{RKD}}([W^n_t, W^n_l])}{\partial W^n_t}\bigg)\end{equation}\] &lt;p&gt;Similar update equations can be obtained for the largest augmented model and the sub-models as well.&lt;/p&gt; &lt;h2 id=&quot;fasten-auxillary-model-training&quot;&gt;Fasten Auxillary Model Training&lt;/h2&gt; &lt;p&gt;We propose this method to solve the problem &lt;a href=&quot;#training_aug_models&quot;&gt;training the generated augmented models&lt;/a&gt;. Based on &lt;d-cite key=&quot;yang2021speeding&quot;&gt;&lt;/d-cite&gt; we propose to speed up the training process for the augmented sub models such that it can attain faster convergence and reduce the number of training iterations thereby obtain a better performance. In this technique, in the early phase of training, the neural network is trained with weights shared across all the layers of the model, to learn the commonly shared component across weights of different layers, and towards the later phase of training we un-share weights and continue training until convergence. Weight sharing for initial training steps will contrain the model complexity effectively. It brings the weights closer to the optimal value, which provides a better initialization for subsequent training steps and improved model generalization.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Mathematical Formulation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Denote the neural model as consisting of \(L\) stacked structurally similar modules as \(\mathcal{M} = \{\mathcal{M}_i\}, \, i=1,\cdots, L\) and \(w = \{w_i\}, \, i=1,\cdots, L\) denote the corresponding weights. These weights are re-parametrized as&lt;/p&gt; \[\begin{equation}w_i = \frac{1}{\sqrt{L}}w_0 + \tilde{w}_i, \:\:\:\: i=1,\cdots,L\end{equation}\] &lt;p&gt;Here \(w_0\) represents the shared weights across all modules and is referred to as &lt;strong&gt;stem-direction&lt;/strong&gt; and \(\tilde{w}_i\) represents the unshared weights across all modules and is referred to as &lt;strong&gt;branch-directions&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training Strategy&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Denote \(T\) as the number of training steps, \(\eta\) as the step size and \(\alpha \in (0, 1)\) is a tunable hyper-paramter indicating the fraction of weight sharing steps. Then we train \(\mathcal{M}\) as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Sharing weights in early stage:&lt;/strong&gt; For the first \(\tau = \alpha \cdot T\) steps, we update the shared weights \(w_0\) alone with gradient \(g_0\)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unsharing weights in later stage:&lt;/strong&gt; For the next \(t \geq \alpha \cdot T\), we update only the unshared weights \(\tilde{w}_i\) with gradient \(\tilde{g}_i\)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The effective gradient updates for \(w_i\) can be found using chain rule as follows:&lt;/p&gt; \[\begin{equation}g_0 = \frac{\partial \mathcal{L}}{\partial w_0} = \sum\limits_{i=1}^L \frac{\partial \mathcal{L}}{w_i}\,\frac{\partial w_i}{\partial w_0} = \frac{1}{\sqrt{L}}\sum\limits_{i=1}^L g_i \end{equation}\] \[\begin{equation}\tilde{g}_i = \frac{\partial \mathcal{L}}{\partial \tilde{w}_i} = \frac{\partial \mathcal{L}}{\partial w_i}\,\frac{\partial w_i}{\partial \tilde{w}_i} = g_i\end{equation}\] &lt;p&gt;where \(g_i\) denotes the gradients of \(w_i\) and \(\mathcal{L}\) denotes the loss function.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/NetAug_SWE-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/NetAug_SWE-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/NetAug_SWE-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/NetAug_SWE.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;generating-largest-augmented-model-via-single-path-one-shot-nas&quot;&gt;Generating largest augmented model via Single Path One-Shot NAS&lt;/h2&gt; &lt;p&gt;We propose this method to solve the problem of &lt;a href=&quot;#generating_aug_model&quot;&gt;generating the largest augmented model&lt;/a&gt;. In NetAug, the largest augment model is generated randomly just based on the hyperparameters \(r\) and \(s\). Single Path One-Shot NAS with uniform sampling &lt;d-cite key=&quot;guo2020single&quot;&gt;&lt;/d-cite&gt; revists the pitfalls of weight coupling in previous weight sharing methods. The one-shot paradigm is made attractive for real world tasks and better generalization. It is hyperparameter free, single path strategy works well because it can decouple the weights of different operations. This implementation can be more efficient in multiple search space. Using this technique we generate largest optimized augmented model by&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Supernet weight Optimization&lt;/strong&gt;&lt;!-- $$\begin{equation} &lt;&gt; \end{equation}$$--&gt; : \(\begin{equation}W_{\mathcal{A}}=\mathop{\arg \min}_{W} \: \mathcal{L}_{\text{train}}\big(\mathcal{N}(\mathcal{A},W)\big)\end{equation}\)&lt;/p&gt; &lt;p&gt;The \(\mathcal{A}\) is the architecture search space represented as a directed acyclic graph which is encoded as a supernet \(\mathcal{N}(\mathcal{A}, W)\). During an SGD step in the above equation, each edge in the supernet graph is randomly dropped, using a dropout rate parameter. In this way, the co-adaptation of the node weights is reduced during training making the supernet training easier.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Architecture Search Optimization&lt;/strong&gt; : \(\begin{equation}a^* = \mathop{\arg \max}_{a \in \mathcal{A}} \text{ ACC}_{\text{val}}\bigg(\mathcal{N}\big(a,W_{\mathcal{A}}(a)\big)\bigg)\end{equation}\)&lt;/p&gt; &lt;p&gt;During search, each sampled architecture a inherits its weights from \(W_{\mathcal{A}}\) as \(W_{\mathcal{A}}(a)\). The architecture weights are ready to use making the search very efficient and flexible. This type of sequential optimization works because, the accuracy of any architecture \(a\) on a validation set using inherited weight \(W_{\mathcal{A}}(a)\) (without extra fine tuning) is highly predictive for the accuracy of \(a\) that is fully trained. Hence we try to minimize the training loss even further for better performance, supernet weights \(W_{\mathcal{A}}\) such that all the architectures in the search space are optimized simultaneously.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; \[\begin{equation}W_{\mathcal{A}} = \mathop{\arg \min}_{W} \: \mathbb{E}_{a \sim \Gamma(\mathcal{A})}\bigg[\mathcal{L}_{\text{train}}\big(\mathcal{N}(a, W(a))\big)\bigg]\end{equation}\] &lt;p&gt;where \(\Gamma(\mathcal{A})\) is the prior distribution of \(a \in \mathcal{A}\). This stochastic training of the supernet is helpful in better generalization of the optimized model and is also computationally efficient. To overcome the problem of weight coupling, the supernet \(\mathcal{N}(\mathcal{A},W)\) is chosen such that each architecture is a single path so that this realization is hyperparameter free as compared to traditional NAS approaches. The distribution \(\Gamma(\mathcal{A})\) is fixed apriori as a uniform distribution during our training and is not a learnable parameter.&lt;/p&gt; &lt;h2 id=&quot;evaluation-and-inference&quot;&gt;Evaluation and Inference&lt;/h2&gt; &lt;p&gt;Evaluation metric is the core for building any accurate machine learning model. We propose to implement the evaluation metrics precision@\(k\) ,recall@\(k\) and f1-score@\(k\) for the augmented sub-models sampled from largest augmented model and for the base model itself, where \(k\) represents the top k accuracy on the test set. These metrics will assist in evaluating the training procedure better than vanilla accuracy because we are trying to tackle the problem of underfitting, and an underfitted model is more likely to make the same prediction for every input and presence of class imbalance will lead to erroneous results.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The main conclusion of this blog post is to further refine tiny neural networks effectively without any loss in accuracy and prevent underfitting in them. The paper implements this in a unique way apart from the conventional techniques such as regularization, dropout and data augmentation. This blog adds to other techniques that are orthogonal to NetAug can be used combined with NetAug for improvised results.&lt;/p&gt; &lt;!-- Notes/Overview/Category of the Paper, comment out later ### Category Solves a known problem of underfitting by proposing a new method ### Context Regularization, over-fitting, underfitting, data augmentation, model augmentation, training paradigm, TinyML, knowledge distallation, security, privacy ### Correctness Assumptions appear to be valid, but there are few setbacks (like eq 1 of the paper) ### Contributions NetAug, Ablation study, improving the performance of underfitting models, new ways to consider loss functions ### Clarity Well-written overall. Somethings are not clear--&gt; &lt;!-- Questions/clarifications asked by us --&gt; &lt;!--```markdown Hi All, I hope everyone is doing well. I&apos;m pursuing a BS from India and my friend is an employee. We came across this paper Network Augmentation for Tiny Deep Learning that was published in ICLR 22. We are unclear about a few things, would be great if you can clarify/justify them to us. 1. Why does sampling all augmented networks in one training step decrease the performance, given the training loss increases as one of the reasons? 2. Initially it&apos;s mentioned that NetAug is very different from the other knowledge distillation methods later the results show both NetAug and knowledge distillation combined together perform well, we would like to understand it more mathematically so as to why? 3. Things like data loading and communication cost are justified to consume maximum time, but don&apos;t we think we can spend training in doing something more potential like maybe in fact learning the feature maps/representations better? Regards, Nevasini. ``` ```markdown Hello Han Cai and team, We are eager to get the above points clarified. Specifically, if you can answer along the following lines, it would be really helpful. For 1 above: It was mentioned in the paper that it was found sampling more than one augmented network from the maintained single largest augmented model in each training step hurts the performance. We think this counters the intuition and spirit behind the idea of NetAug and the augmented loss proposed in equation (1) of the paper. For 2 above: Can you further provide insights on why KD and NetAug can be complementary when the underlying motive in NetAug is to solve underfitting and in KD is to generalize well on unseem data with the help of a teacher? These both motives look like they want to solve the generalizability of the model under consideration. Furthermore, KD can be done to tiny models as well right? For 3 above: We understand that data loading and communication c0st can be a bottleneck in training tiny neural networks. But in the case of NetAug, along with base supervision, we are training the augmented model as well (meaning we update the weights W_i). This means the training cost is expected to increase by 50% and experiments should also show a similar change. Thanks and regards, Nevasini and Ipsit. ``` ```markdown--&gt; &lt;!-- Their reply --&gt; &lt;!--Hi Nevasini and Ipsit, 1. I think it is because the proportion of the base supervision will decrease when we sample more augmented networks, which will make the training process biased toward augmented networks. 2. I think the mechanisms of KD and NetAug are different. KD can be viewed as a better training objective with adaptive label smoothing. It does not encourage the base network to work as a subnetwork of larger networks, different from NetAug. 3. For tiny neural networks, the cost of the additional forward+backward is smaller than you expected. For example, training MobileNetV2-Tiny with NetAug on ImageNet only increases the training time by 16.7% on our GPU servers. Best, Han--&gt; </content> </entry> <entry> <title>Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-agent Reinforcement Learning</title> <link href="https://jocelynshen.com/blog/2022/riit/"/> <updated>2022-12-13T00:00:00+00:00</updated> <id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/riit</id> <content type="html">&lt;blockquote&gt; &lt;p&gt;QMIX [&lt;a href=&quot;#8&quot;&gt;8&lt;/a&gt;], a very classical multi-agent reinforcement learning (MARL) algorithm, is often considered to be a weak performance baseline due to its representation capability limitations. However, we found that by improving the implementation techniques of QMIX we can enable it to achieve state-of-the-art on the StarCraft Multi-Agent Challenge (SMAC) testbed [&lt;a href=&quot;#10&quot;&gt;10&lt;/a&gt;]. Specifically, we evaluated the effect of optimizer, number of rollout processes, replay buffer size, eligibility traces, hidden size and the degree of annealed exploration on QMIX, and tried to explain their role in MARL. Furthermore, the key factor of the monotonicity constraint of QMIX was found in this post, we tried to explain its role and corroborated its superior performance by combining it with another actor-critic style algorithm. We have open-sourced the code at &lt;a href=&quot;https://github.com/hijkzzz/pymarl2&quot;&gt; https://github.com/hijkzzz/pymarl2 &lt;/a&gt; for researchers to evaluate the effects of these proposed techniques. We hope this research will advance the MARL community and contribute to the establishment of new baselines of QMIX.&lt;/p&gt; &lt;/blockquote&gt; &lt;h2 id=&quot;background&quot;&gt;&lt;a name=&quot;Background&quot;&gt;Background&lt;/a&gt;&lt;/h2&gt; &lt;h3 id=&quot;from-rl-to-marl&quot;&gt;&lt;a name=&quot;From_RL_to_MARL&quot;&gt;From RL to MARL&lt;/a&gt;&lt;/h3&gt; &lt;p&gt;Since AlphaZero beats humans at Go, RL has become a consistent hot spot in academia and industry. The agent of RL can obtain some rewards by interacting with the environment and taking actions to maximize these cumulative rewards. Actually, almost all the RL problems can be described as &lt;strong&gt;Markov Decision Processes&lt;/strong&gt; as illustrated in Figure &lt;a href=&quot;#mdp&quot;&gt;1&lt;/a&gt;.&lt;/p&gt; &lt;div id=&quot;mdp&quot; class=&quot;img-height-200 img-center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/mdp-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/mdp-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/mdp-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/mdp.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;center&gt;Figure 1: The agent-environment interaction in a Markov decision process. (Image source: Sec. 3.1 Sutton &amp;amp; Barto (2017) &lt;a ref=&quot;#14&quot;&gt;[14]&lt;/a&gt;)). $R_t, S_t, A_t$ denote the reward, state and action at timestep $t$.&lt;/center&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;Just as its name implies, MARL contains multiple agents trained by RL algorithms in the same environment. Many complex multi-agent systems such as robot swarm control, autonomous vehicle coordination, and sensor networks, can be modeled as MARL tasks. The interaction of these agents would make them work together to achieve a common goal.&lt;/p&gt; &lt;div style=&quot;display:flex; margin-bottom:-30px; margin-left :150px; margin-right :150px&quot;&gt; &lt;div id=&quot;chase&quot; class=&quot;img-height-100&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/chase.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/chase.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/chase.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/chase.gif&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div id=&quot;magent&quot; class=&quot;img-height-100&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/magent.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/magent.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/magent.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/magent.gif&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt; &lt;div style=&quot;display:flex; margin-top:-30px; margin-left :50px; margin-right :50px&quot;&gt; &lt;div id=&quot;hide&quot; class=&quot;img-height-200&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/hide.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/hide.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/hide.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/hide.gif&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div id=&quot;smac&quot; class=&quot;img-height-200&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/smac.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/smac.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/smac.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/smac.gif&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt; &lt;div style=&quot;margin-bottom: 20px&quot;&gt;&lt;center&gt;Figure 2: Some multi-agent cooperative scenarios [from-left-to-right]. &lt;a href=&quot;https://github.com/openai/multiagent-particle-envs&quot;&gt; &lt;br /&gt; (a) Chasing in Multi-Agent Particle Environment (Predator-Prey); &lt;/a&gt; &lt;a href=&quot;https://github.com/geek-ai/MAgent&quot;&gt; (b) MAgent Environment; &lt;/a&gt; &lt;a href=&quot;https://openai.com/blog/emergent-tool-use&quot;&gt; &lt;br /&gt; (c) Hide &amp;amp; Seek; &lt;/a&gt; &lt;a href=&quot;https://github.com/oxwhirl/smac&quot;&gt; (d) StarCraft Multi-Agent Challenge. &lt;/a&gt;&lt;/center&gt;&lt;/div&gt; &lt;p&gt;In this general setting, agents usually have a limited sight range to observe their surrounding environment. As shown in Figure &lt;a href=&quot;#smac_obs&quot;&gt;3&lt;/a&gt;, the cyan border indicates the sight and shooting range of the agent, which means the agent could only obtain the information of terrain or other agents in that range. This restricted field of view may also result in the difficulty of agents to access to global state information, making its policy updates subject to bias and unsatisfactory performance. In general, these kinds of multi-agent scenarios can be modeled as &lt;strong&gt;Decentralized Partially Observable Markov Decision Processes&lt;/strong&gt; (Dec-POMDP) [&lt;a href=&quot;#6&quot;&gt;6&lt;/a&gt;].&lt;/p&gt; &lt;p&gt;Even though many RL algorithms [&lt;a href=&quot;#14&quot;&gt;14&lt;/a&gt;] and their variants have been successfully extended to the cooperative scenarios in MARL setting, few of their performance is satisfactory. One of the most troublesome issues is &lt;em&gt;Non-Stationarity&lt;/em&gt;. Specifically, as a part of the environment, the changing policies of other agents during training would make the observation non-stationary from the perspective of any individual agent [&lt;a href=&quot;#28&quot;&gt;28&lt;/a&gt;] and significantly slow down the policy optimization of MARL. This situation has forced researchers to seek a method that can exploit global information during training but does not destroy the ability of the agents to only use their respective observations during execution, to find a joint policy $\boldsymbol{\pi} = \langle \pi^{1},…,\pi^{n}\rangle$ to maximize global reward. Naturally, the simplicity and effectiveness of the &lt;strong&gt;Centralized Training with Decentralized Execution&lt;/strong&gt; (CTDE) paradigm have attracted the attention of the community, and many MARL algorithms based on CTDE were proposed, making a remarkable contribution to MARL.&lt;/p&gt; &lt;p&gt;In the rest of this section, we briefly introduce Dec-POMDP and CTDE to facilitate the understanding of the contents of MARL, the QMIX algorithm and the following text.&lt;/p&gt; &lt;div style=&quot;float:left; margin-left :150px; margin-right :150px;&quot;&gt;&lt;div name=&quot;smac_obs&quot; class=&quot;img-height-100&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/smac_agent_obs-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/smac_agent_obs-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/smac_agent_obs-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/smac_agent_obs.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;center&gt;Figure 3: The partial observation of agents&lt;br /&gt;(Image source: SMAC &lt;a ref=&quot;#10&quot;&gt;[10]&lt;/a&gt;). &lt;/center&gt;&lt;br /&gt;&lt;/div&gt; &lt;h3 id=&quot;decentralized-partially-observable-markov-decision-process&quot;&gt;&lt;a name=&quot;Decentralized_Partially_Observable_Markov_Decision_Process&quot;&gt;Decentralized Partially Observable Markov Decision Process&lt;/a&gt;&lt;/h3&gt; &lt;p&gt;A &lt;strong&gt;Decentralized Partially Observable Markov Decision Process&lt;/strong&gt; (Dec-POMDP) model, as described in [&lt;a href=&quot;#8&quot;&gt;8&lt;/a&gt;][&lt;a href=&quot;#28&quot;&gt;28&lt;/a&gt;], is typically used to represent a full cooperative multi-agent task. The model consists of a tuple denoted by $G=(S, U, P, r, Z, O, n, \gamma)$, and involves $n$ agents, where $n$ is an integer between 1 and $n$, inclusive. The true state of the environment, denoted by $s \in S$, describes global information that is relevant to both agents and other auxiliary features. At each timestep $t$, a transition in the environment occurs via a joint action $\mathbf{u} \in \mathbf{U} \equiv U^{n}$, which is composed of an action $u^i \in U$, chosen by each agent. This transition is driven by the state transition function $P\left(s^{\prime} \mid s, \mathbf{u}\right): S \times \mathbf{U} \times S \rightarrow[0,1]$. Additionally, there is a shared global reward function, denoted by $r(s, \mathbf{u}): S \times \mathbf{U} \rightarrow \mathbf{R}$, which is optimized by the whole team. Finally, each agent has a partial observation described by $o^i \in O$, which is derived from the observation function $Z(o^i \mid s, u^i) : S \times U \rightarrow O$. All agents work cooperatively to maximize the shared global reward $R_{t}=\sum_{k=0}^{T} \gamma^{k} r_{t+k}$, which is described by the joint value function \(Q^{\boldsymbol{\pi}}\left(s_{t}, \mathbf{u}_{t}\right) = \mathbb{E}_{s_{t+1: \infty}, \mathbf{u}_{t+1: \infty}}\left[R_{t} \mid s_{t}, \mathbf{u}_{t}\right]\).&lt;/p&gt; &lt;h3 id=&quot;centralized-training-with-decentralized-execution-and-value-decomposition&quot;&gt;&lt;a name=&quot;Centralized_Training_with_Decentralized_Execution_and_Value_Decomposition&quot;&gt;Centralized Training with Decentralized Execution and Value Decomposition&lt;/a&gt;&lt;/h3&gt; &lt;p&gt;To better explore the factors affecting the QMIX algorithm, our focus lies in the &lt;strong&gt;Centralized Training with Decentralized Execution&lt;/strong&gt; (CTDE) paradigm of MARL algorithms. These algorithms under this paradigm have access to the true state $s$ and the action-observation histories $\tau^{i}$ of all agents to centrally train policies, but each agent can only rely on its local observation $o^{i}$ for decision-making. Some value-based algorithms implemented under CTDE follow the Individual-Global-Max (&lt;strong&gt;IGM&lt;/strong&gt;) principle [&lt;a href=&quot;#11&quot;&gt;11&lt;/a&gt;], ensuring consistency between the joint action-value function $Q_{tot} \left(\boldsymbol{\tau}, \mathbf{u}\right)$ and individual agent-utilities $[Q_i\left(\tau^i, u^i\right)] _{i=1} ^{n}$:&lt;/p&gt; \[\underset{\mathbf{u}}{\operatorname{argmax}}\ Q_{tot} \left(\boldsymbol{\tau}, \mathbf{u}\right) = (\underset{u^{1}}{\operatorname{argmax}}\ Q_{1} \left(\tau^{1}, u^{1}\right), \ldots, \underset{u^{n}}{\operatorname{argmax}}\ Q_{n} \left(\tau^{n} , u^{n}\right)). \tag{1} \label{eq1}\] &lt;p&gt;One of the most typical ways to efficiently train the joint value function \(Q_{tot} \left(\boldsymbol{\tau}, \mathbf{u}\right)\) is to decompose it into the utility functions \([Q_i\left(\tau^i, u^i\right)] _{i=1} ^{n}\) and maintain updating consistency between them via IGM. The simplest factorization structure, called &lt;em&gt;additivity&lt;/em&gt;, has been proposed by VDN [&lt;a href=&quot;#13&quot;&gt;13&lt;/a&gt;], which makes VDN simply factorize $Q_{tot}$ into a sum of per-agent utilities \(Q_{tot}^{\mathrm{VDN}} \left(\boldsymbol{\tau}, \boldsymbol{u}\right)=\sum_{i=1}^{n} Q_{i} \left(\tau^{i}, u^{i}\right)\). VDN’s simplicity and equal weighting of each utility in the joint value function makes it ineffective for cooperative tasks, which has motivated the QMIX structure and other more efficient decomposition approaches.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; &lt;h3 id=&quot;notations&quot;&gt;&lt;a name=&quot;Notations&quot;&gt;Notations&lt;/a&gt;&lt;/h3&gt; &lt;p&gt;In this subsection, we define the notations used in this post. Specifically, in traditional RL, time steps $t$ are usually represented in the update formula and the value function of RL is considered to be estimated by the pairwise variables at the current time step $t$ and the next time step $t+1$. Since the &lt;em&gt;ID&lt;/em&gt; of the agent also needs to be represented in the MARL algorithm, it may cause ambiguity when expressed in the same formula as the time step $t$. For simplicity of expression, variables without $t$ are indicated to be implemented at the current time step, while variables at the next time step are indicated with an apostrophe in the upper right corner in the rest of the context, e.g., $s$ means the current state and $s^{\prime}$ indicates the next time step state, the same approach applies to actions $u$ and observations $o$. All the notations are listed in Table &lt;a href=&quot;#table1&quot;&gt;1&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;table1&quot;&gt; &lt;/a&gt;&lt;/p&gt; &lt;center&gt; Table 1: All the notations used in this post. &lt;/center&gt; &lt;style type=&quot;text/css&quot;&gt; .tg {border-collapse:collapse;border-spacing:0;} .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top} &lt;/style&gt; &lt;table class=&quot;tg&quot;&gt; &lt;thead&gt; &lt;tr&gt; &lt;th class=&quot;tg-c3ow&quot;&gt;Notation&lt;/th&gt; &lt;th class=&quot;tg-c3ow&quot;&gt;Description&lt;/th&gt; &lt;th class=&quot;tg-c3ow&quot;&gt;Notation&lt;/th&gt; &lt;th class=&quot;tg-c3ow&quot;&gt;Description&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$s$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the current state (at time $t$)&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$S$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the set of all states&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$s^{\prime}$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the next state (at time $t+1$)&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$U$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the set of all actions&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$u^{i}$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the action of agent $i$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$N$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the set of all agents&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$\mathbf{u}$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the joint actions (at time $t$)&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$\tau^{i}$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the action-observation history of agent $i$&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$o^{i}$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the observation of agent $i$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$${\tau}$$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the joint action-observation histories&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$$o$$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the joint observation&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$r(s, \mathbf{u})$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the joint reward supplied by environments&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$Q_{i}(\tau^{i}, u^{i})$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the utility function of agent $i$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$\gamma$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the discount factor&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$Q_{tot}({\tau}, \mathbf{u})$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the joint value function &lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$P(s^{\prime} \mid s, \mathbf{u})$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the transition function&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$Z(o^{i} \mid s, u^{i})$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the observation function&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$\epsilon$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;action selection probability of $\epsilon$-greedy&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$N$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the set of all agents with $n$ agents&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$$\theta$$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the set of parameters of agents network, with $[\theta^{i}]_{i=1}^{n}$&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$b$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;sampled batch size for training&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$\phi$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the parameter of mixing network&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$TS$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the $T$otal rollout $S$amples&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$PP$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the number of rollout $P$rocesses in $P$arallel&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$SE$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the number of $S$amples in each &lt;br /&gt; $E$pisode&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;$PI$&lt;/td&gt; &lt;td class=&quot;tg-c3ow&quot;&gt;the $P$olicy $I$teration number&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;qmix-and-monotonicity-constraint&quot;&gt;&lt;a name=&quot;QMIX_and_Monotonicity_Constraint&quot;&gt;QMIX and Monotonicity Constraint&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;To deal with the relationship between the individual agent and the cooperative group, QMIX [&lt;a href=&quot;#8&quot;&gt;8&lt;/a&gt;] learns a joint action-value function $Q_{tot}$ and factorizes the joint policy into the individual policy of each agent. In other words, as illustrated in Figure &lt;a href=&quot;#frame&quot;&gt;4&lt;/a&gt;, QMIX integrates all the individual $Q_{i}$ with a mixing network to obtain a centralized value function $Q_{tot}$, which can be more appropriately updated by the global reward.&lt;/p&gt; &lt;div id=&quot;frame&quot; class=&quot;img-height-310 image-center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/qmix_frame-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/qmix_frame-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/qmix_frame-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/qmix_frame.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;center&gt;Figure 4: Framework of QMIX. (Image source: QMIX &lt;a ref=&quot;#8&quot;&gt;[8]&lt;/a&gt;). On the left is Mixing Network (A Hypernetwork), and on the right is the Agent network.&lt;/center&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;Still, it also can be represented in Eq.(\ref{eq2})&lt;/p&gt; \[Q_{tot}(s, \boldsymbol{u} ; \boldsymbol{\theta}, \phi) = g_{\phi}\left(s, Q_{1}\left(\tau^{1}, u^{1} ; \theta^{1}\right), \ldots, Q_{n}\left(\tau^{n}, u^{n} ; \theta^{n}\right)\right);\] \[with \quad \frac{\partial Q_{tot}(s, \boldsymbol{u} ; \boldsymbol{\theta}, \phi)}{\partial Q_{i}\left(\tau^{i}, u^{i}; \theta^{i}\right)} \geq 0, \quad \forall i \in N. \tag{2} \label{eq2}\] &lt;p&gt;where $\theta^i$ is the parameter of the agent network $i$, $u^{i}$ denotes the action of agent $i$, and $\phi$ is the trainable parameter of the mixing network. The the mixing network $g_{\phi}(\cdot)$ is responsible to factorize $Q_{tot}$ to each utility $Q_{i}$. The &lt;em&gt;Monotonicity Constraint&lt;/em&gt; is also implemented in the mixing network $g_{\phi}(\cdot)$, which inputs the global state $s$ and outputs &lt;em&gt;non-negative&lt;/em&gt; wights through a &lt;em&gt;hyper-network&lt;/em&gt; as illustrated in the left part of Figure &lt;a href=&quot;#frame&quot;&gt;4&lt;/a&gt;, which will result in \(\frac{\partial Q_{tot}(s, \boldsymbol{u} ; \boldsymbol{\theta}, \phi)}{\partial Q_{i}\left(\tau^{i}, u^{i}; \theta^{i}\right)} \geq 0\). This delicate design ensures consistency between joint actions and the individual actions of each agent, then guarantees the Individual-Global-Max (IGM) principle. Benefiting from the monotonicity constraint in Eq. (\ref{eq2}), maximizing joint $Q_{tot}$ is precisely the equivalent of maximizing individual $Q_i$, which would also allow the optimal individual action to maintain consistency with optimal joint action. Furthermore, QMIX learns the centralized value function $Q_{tot}$ by sampling a multitude of transitions from the replay buffer and minimizing the mean squared temporal-difference (TD) error loss:&lt;/p&gt; \[\mathcal{L}(\theta)= \frac{1}{2} \sum_{i=1}^{b}\left[\left(y_{i}^{}-Q_{tot}(s, u ; \theta, \phi)\right)^{2}\right] \tag{3} \label{eq3}\] &lt;p&gt;where the TD target value \(y=r+\gamma \underset{u^{\prime}}{\operatorname{max}} Q_{tot}(s^{\prime},u^{\prime};\theta^{-},\phi^{-})\), and $\theta^{-}, \phi^{-}$ are the target network parameters copied periodically from the current network and kept constant for a number of iterations. $b$ is the sampled training batch size. Due to the strong constraints in Eq.(\ref{eq2}), QMIX is still criticized for the insufficient expressive capacity of the joint value function [&lt;a href=&quot;#3&quot;&gt;3&lt;/a&gt;].&lt;/p&gt; &lt;h2 id=&quot;extension-to-qmix&quot;&gt;&lt;a name=&quot;Extension_to_QMIX&quot;&gt;Extension to QMIX&lt;/a&gt;&lt;/h2&gt; &lt;h3 id=&quot;experimental-design&quot;&gt;&lt;a name=&quot;Experimental_Design&quot;&gt;Experimental Design&lt;/a&gt;&lt;/h3&gt; &lt;p&gt;To facilitate the study of proper techniques affecting the training effectiveness and sample efficiency of QMIX, we perform a set of experiments designed to provide insight into some methods that have been proven effective in single-agent RL but may be ambiguous in MARL. In particular, we investigate the effects of &lt;strong&gt;Adam optimizer with parallel rollout process; the incremental replay buffer size; the number of parallel rollout processes; $\epsilon$-exploration steps; the implementation of $Q(\lambda)$ in centralized value function; the hidden size of the recurrent network of agents&lt;/strong&gt;. And we also dive into the &lt;strong&gt;role of monotonicity constraints in QMIX&lt;/strong&gt;. For all experiments, we generally implement PyMARL [&lt;a href=&quot;#10&quot;&gt;10&lt;/a&gt;] framework to implement QMIX. To ensure fairness we run independent 3 to 6 experimental trials for each evaluation, each with a random seed. Unless otherwise mentioned, we use default settings as in PyMARL whenever possible, while incorporating the techniques of interest. To prevent the training process of the algorithm from crashing by chance, we remove the highest and lowest scores when counting the calculated returns and win rates for the test episode. All the results are plotted with the median and shaded the interval, and the final scores were &lt;strong&gt;&lt;em&gt;not&lt;/em&gt;&lt;/strong&gt; smoothed for the sake of image aesthetics, and we did so to verify exactly what direct effect the proposed techniques could have on QMIX.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;StarCraft Multi-Agent Challenge (SMAC)&lt;/strong&gt; As a commonly used testing environment, SMAC [&lt;a href=&quot;#10&quot;&gt;10&lt;/a&gt;] sets an example to offer a great opportunity to tackle the cooperative control problems in the multi-agent domain. We focus on the micromanagement challenge in SMAC, which means each agent is controlled by an independent agency that conditions on a limited observation area, and these groups of units are trained to conquer the enemy consisting of built-in AI. According to the quantity and type of enemy, all testing scenarios could be divided into &lt;em&gt;Easy, Hard&lt;/em&gt;, and &lt;em&gt;Super-Hard&lt;/em&gt; levels. Since QMIX can effectively solve the &lt;em&gt;Easy&lt;/em&gt; tasks, we pay attention to some &lt;em&gt;Hard&lt;/em&gt; and &lt;em&gt;Super-Hard&lt;/em&gt; scenarios that QMIX failed to win, especially in &lt;em&gt;Corridor, 3s5z_vs_3s6z&lt;/em&gt;, and &lt;em&gt;6h_vs_8z&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Predator-Prey (PP)&lt;/strong&gt; is representative of another classical problem called &lt;em&gt;relative overgeneralization&lt;/em&gt; [&lt;a href=&quot;#16&quot;&gt;16&lt;/a&gt;] . The cooperating predators are trained to chase a faster running prey, and hope to capture this escaping robot with the fewest steps possible. We leverage Predator-Prey-2 (a variant of Predator-Prey) proposed in FACMAC [&lt;a href=&quot;#29&quot;&gt;29&lt;/a&gt;], whose policy of prey is replaced with a hard-coded heuristic policy. The heuristic policy asks the prey to move to the farthest sampled position to the closest predator. If one of the cooperative agents collides with the prey, a team reward of +10 is emitted; otherwise, no reward is given. In the original simple tag environment, each agent can observe the relative positions of the other two agents, the relative position and velocity of the prey, and the relative positions of the landmarks. This means each agent’s private observation provides an almost complete representation of the true state of the environment.&lt;/p&gt; &lt;p&gt;To introduce partial observability to the environment, the view radius is added to the agent, which restricts the agents from receiving information about other entities (including all landmarks, the other two agents, and the prey) that are out of range. Specifically, we set the view radius such that the agents can only observe other agents roughly 60% of the time. These environments require greater cooperation between agents.&lt;/p&gt; &lt;h3 id=&quot;optimizer&quot;&gt;&lt;a name=&quot;Optimizer&quot;&gt;Optimizer&lt;/a&gt;&lt;/h3&gt; &lt;p&gt;As an important part of training neural networks, the selection of an optimizer is very important since it could seriously affect the training effect of the reinforcement learning agent. Without a further illustration, QMIX uses RMSProp [&lt;a href=&quot;#21&quot;&gt;21&lt;/a&gt;] to optimize the neural networks of agents as they prove stable in SMAC. While Adam [&lt;a href=&quot;#1&quot;&gt;1&lt;/a&gt;] is famous for the fast convergence benefiting from the momentum in training, which seems to be the first choice for AI researchers. We reckon that the momentum property in Adam would have some advantages in learning the sampled data which is generated by agents interacting with the environment as in MARL. And then, on the other hand, QMIX is criticized for performing sub-optimally and sampling inefficiency when equipped with the A2C framework, which is implemented to promote the training efficiency of the RL algorithm. VMIX [&lt;a href=&quot;#12&quot;&gt;12&lt;/a&gt;] argues this limitation is brought about by the value-based inherent Q function, so they extend QMIX to the actor-critic style algorithm to take advantage of the A2C framework. This controversy attracts our attention to evaluate the performance of QMIX using Adam, as well as the parallel sampling paradigm.&lt;/p&gt; &lt;div id=&quot;optimizer&quot; class=&quot;img-height-210 image-center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/optimizer.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/optimizer.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/optimizer.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/optimizer.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;center&gt;Figure 5: The performance of QMIX optimized by Adam and RMSProp.&lt;/center&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt; As shown in Figure &lt;a href=&quot;#optimizer&quot;&gt;5&lt;/a&gt;, we run the Adam-supported QMIX with &lt;strong&gt;8 rollout processes&lt;/strong&gt;. Different from what was described in VMIX, the performance and efficiency of QMIX could be greatly improved by Adam. We speculate the reason is the momentum property in Adam could fastly fit the newly sampled data from the parallel rollout processes and then enhance the performance, while RMSProp failed.&lt;/p&gt; &lt;h3 id=&quot;rollout-process-number&quot;&gt;&lt;a name=&quot;Rollout_Process_Number&quot;&gt;Rollout Process Number&lt;/a&gt;&lt;/h3&gt; &lt;p&gt;Naturally, we come to focus on the benefits of parallel data sampling in QMIX. A2C [&lt;a href=&quot;#5&quot;&gt;5&lt;/a&gt;] provides an excellent example to reduce training time and improve the training efficiency in single-agent RL. As we implement the algorithms under the paradigm of A2C, there is usually a defined total number of samples and an unspecified number of rollout processes. The total number of samples $TS$ can be calculated as $TS = SE \cdot PP \cdot PI$, where $TS$ is the total sum of sampled data, $SE$ denotes the number of samples in each episode, $PP$ and $PI$ denote the number of rollout processes in parallel and the policy iteration number, respectively. This section aims to perform analysis and spur discussion on the impact of the parallel rollout process on the final performance of QMIX.&lt;/p&gt; &lt;div id=&quot;process_number&quot; class=&quot;img-height-210 image-center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/process_number.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/process_number.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/process_number.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/process_number.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;center&gt;Figure 6: The performance of different rollout process numbers of QMIX. When given the total number of samples, the performance of fewer processes achieves better performance.&lt;/center&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt; Still, we use Adam-supported QMIX to evaluate the effect of the number of the rollout process. Since we could choose the &lt;em&gt;Parallel&lt;/em&gt; model to sample the interacting data of the agent with the environment in PyMARL, we can theoretically get more &lt;strong&gt;on-policy&lt;/strong&gt; data which is close to the updating policy in training. Figure &lt;a href=&quot;#process_number&quot;&gt;6&lt;/a&gt; shows that when $TS$ and $PP$ is given, the performance enhancement of QMIX is not consistent with the increase in rollout process number. The intuitive explanation is when we set the fewer rollout processes, the greater the quantity of policy would iterate [&lt;a href=&quot;#14&quot;&gt;14&lt;/a&gt;]. Besides, too fast updated data in parallel may cause the factitious unstable training in policy updating, i.e., it is difficult for agents to learn effective information from rapidly sampled data from the replay buffer. The more times policies are iterated, the more information the agents would learn which lead to an increase in performance. However, it also causes longer training time and loss of stability. We suggest trying the fewer rollout process in the beginning and then balancing between training time and performance.&lt;/p&gt; &lt;h3 id=&quot;replay-buffer-size&quot;&gt;&lt;a name=&quot;Replay_Buffer_Size&quot;&gt;Replay Buffer Size&lt;/a&gt;&lt;/h3&gt; &lt;p&gt;Replay buffer plays an important role in improving sample efficiency in off-policy single-agent RL. Its capacity would greatly affect the performance and stability of algorithms. Researchers usually set a very large capacity of replay buffer in Deep Q-network (DQN) [&lt;a href=&quot;#4&quot;&gt;4&lt;/a&gt;] to stabilize the training. Some research on the effect of replay buffer in single-agent RL has already been carried out in [&lt;a href=&quot;#22&quot;&gt;22&lt;/a&gt;] , which poses the distribution of sampled training data should be close as possible to the agents’ policies to be updated. Actually, there are two factors affected when we change the capacity of the replay buffer: (1) the replay capacity (total number of transitions/episodes stored in the buffer); and (2) the replay ratio (the number of gradient updates per environment transition/episode) of old policies. When we increase the capacity of the replay buffer, the aged experiences of old policies would grow as the replay ratio is fixed. Then the distribution of outdated experiences would also be much different from the updating policy, which would bring additional difficulty to the training agents. From the results in [&lt;a href=&quot;#22&quot;&gt;22&lt;/a&gt;], there seems to be an optimal range of choices between replay buffer size and replay ratio of experiences in RL, where we would like to know whether it is consistent with the results in MARL.&lt;/p&gt; &lt;div id=&quot;replay_buffer&quot; class=&quot;img-height-210 image-center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/buffer_size.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/buffer_size.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/buffer_size.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/buffer_size.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;center&gt;Figure 7: Setting the replay buffer size to 5000 episodes allows for QMIX’s learning to be stable.&lt;/center&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt; The results seem not to be consistent with that in single-agent RL. Figure &lt;a href=&quot;#replay_buffer&quot;&gt;7&lt;/a&gt; shows the large replay buffer size of QMIX would cause instability during training. When we increase the buffer size from the default setting in PyMARL, the performance would almost continuously declines. We speculate the reason is the fast-changing distribution of experiences in a larger buffer would make it more difficult to fit sampled data due to the enormous joint action space. Since the samples become obsolete more quickly, these aged policies would also be more different from the updating policy, which brings additional difficulty. On the other hand, we find the same performance decline when we squeeze the buffer. We reckon that a small buffer would accelerate the updating speed of sampling data in a disguised way, which makes it tough to fit the data and learn a good policy. We believe that researchers should be cautious to increase the buffer size in other multi-agent applications.&lt;/p&gt; &lt;h3 id=&quot;eligibility-traces&quot;&gt;&lt;a name=&quot;Eligibility_Traces&quot;&gt;Eligibility Traces&lt;/a&gt;&lt;/h3&gt; &lt;p&gt;The well-known trade-off between bias and variance of bootstrapping paradigm is a classic research topic in RL. Since we implement the Centralized Value Function (CVF) to alleviate the &lt;em&gt;Non-Stationarity&lt;/em&gt; multi-agent settings, the estimated accuracy of CVF is critical to MARL and then guides the policies of agents to update. Eligibility traces such as TD($\lambda$)[&lt;a href=&quot;#14&quot;&gt;14&lt;/a&gt;], Peng’s Q($\lambda$)[&lt;a href=&quot;#2&quot;&gt;2&lt;/a&gt;], and TB($\lambda$)[&lt;a href=&quot;#7&quot;&gt;7&lt;/a&gt;] achieve a balance between return-based algorithms (where return refers to the sum of discounted rewards $\sum_{k} \gamma^{k} r_{t+k}$) and bootstrap algorithms (where return refers $r_t + V(s_{t+1})$), then speed up the convergence of agents’ policies. As a pioneer, SMIX [&lt;a href=&quot;#20&quot;&gt;20&lt;/a&gt;] equipped QMIX with the SARSA($\lambda$) to estimate the accurate CVF and get decent performance. As another example of eligibility trace in Q-learning, we study the estimation of CVF using Peng’s Q$(\lambda)$ for QMIX.&lt;/p&gt; &lt;div id=&quot;qlambda1&quot; class=&quot;img-height-210 image-center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/td_lambda.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/td_lambda.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/td_lambda.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/td_lambda.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;center&gt;Figure 8: Q(λ) significantly improves the performance of QMIX, but large values of λ lead to instability in the algorithm.&lt;/center&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt; As the same in single-agent RL, the Q-networks without sufficient training usually have a large bias in bootstrapping returns. Figure &lt;a href=&quot;#qlambda1&quot;&gt;8&lt;/a&gt; shows that, with the help of Q$(\lambda)$, the performance of QMIX has generally improved across all scenarios. It means the more accurate estimate of CVF would still provide a better direction of policy updating for each agent. However, the value of $\lambda$ in Peng’s Q$(\lambda)$ is not so radical as in single-agent RL, which would lead to failed convergence due to the large variance. We recommend a small $\lambda$, such as $0.5$, when using $Q(\lambda)$ in MARL.&lt;/p&gt; &lt;h3 id=&quot;hidden-size&quot;&gt;&lt;a name=&quot;Hidden_Size&quot;&gt;Hidden Size&lt;/a&gt;&lt;/h3&gt; &lt;p&gt;Searching for an optimal scale and architecture of neural networks is a very tough problem in the field of machine learning. Researchers typically use empirically small networks to train the agents in deep reinforcement learning. Since the role of neural networks is to extract the features of input states and actions, the size of the neural network would also have a great impact on the performance of MARL algorithms. The study in [&lt;a href=&quot;#23&quot;&gt;23&lt;/a&gt;] has revealed that networks with a complex structure like ResNet[&lt;a href=&quot;#25&quot;&gt;25&lt;/a&gt;] and DenseNet[&lt;a href=&quot;#26&quot;&gt;26&lt;/a&gt;] can extract more useful information for training, while Ba [&lt;a href=&quot;#24&quot;&gt;24&lt;/a&gt;] poses that the width of neural networks is probably more important than its depth. The subsequent study on QMIX [&lt;a href=&quot;#19&quot;&gt;19&lt;/a&gt;] makes preliminary research on the depth of neural networks, which showed a limited improvement in performance. Though, there is little research on the width of neural networks in MARL. Instead of searching for an optimal network architecture here, we just want to make a pilot study on the effect of the hidden size of network width in QMIX.&lt;/p&gt; &lt;div id=&quot;hiddensize&quot; class=&quot;img-height-210 image-center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/hidden_size.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/hidden_size.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/hidden_size.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/hidden_size.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;center&gt;Figure 9: Impact of the hidden size of network in QMIX.&lt;/center&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt; The study in [&lt;a href=&quot;#24&quot;&gt;24&lt;/a&gt;] illustrates the ability of infinity-width networks to fit any complex function, which would theoretically provide the performance gain from increasing network width. As shown in Figure &lt;a href=&quot;#hiddensize&quot;&gt;9&lt;/a&gt;, the final performance or the efficiency of policy training would have varying degrees of improvement when we increase the hidden size of the network from 64 to 256 in QMIX, where &lt;strong&gt;QMIX-ALL-Hidden refers to the size of the network including Recurrent Neural Network (RNN) and mixing part, while QMIX-RNN-Hidden just refers to RNN&lt;/strong&gt;. Also, the results reveal the spectacular effect of increasing the network width of RNN, which would allow for about a 20% increase in the Super-Hard scenarios &lt;em&gt;3s5z_vs_3s6z&lt;/em&gt;. While the performance improvement is limited in enlarging the mixing network. We speculate that more units in the network are needed to represent the complex temporal context information in RNN, which is not included in the mixing network. We advise researchers to appropriately increase the network width of RNN to achieve better performance.&lt;/p&gt; &lt;h3 id=&quot;exploration-steps&quot;&gt;&lt;a name=&quot;Exploration_Steps&quot;&gt;Exploration Steps&lt;/a&gt;&lt;/h3&gt; &lt;p&gt;Exploration and exploitation are other classic trade-offs in reinforcement learning. Agents need some directed mechanisms to explore the states that may be of higher value or inexperienced. The most versatile method of exploration in RL is $\epsilon$-greedy action, which makes the agent select random actions with probability $\epsilon$, or select the greedy action with $1 - \epsilon$. The value of $\epsilon$ would drop-down with training and then stays at a small constant. The annealing period of $\epsilon$-greedy determines how fast the drop down will be. This exploration mechanism is usually implemented for each agent to select their action, which has been criticized by MAVEN [&lt;a href=&quot;#3&quot;&gt;3&lt;/a&gt;] for lacking joint exploratory policy over an entire episode. However, we can still get more exploration when $\epsilon$ drops slower, then we evaluate the performance of the annealing period of $\epsilon$-greedy in some Super-Hard scenarios in SMAC.&lt;/p&gt; &lt;div id=&quot;exploration&quot; class=&quot;img-height-210 image-center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/exploration.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/exploration.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/exploration.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/exploration.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;center&gt;Figure 10: Experinments for the impact of ε anneal period.&lt;/center&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt; Apparently, appropriately increasing the annealing period of $\epsilon$-greedy from 100K steps to 500K would get explicit performance gain in those hard exploration scenarios, where QMIX failed with the default setting. However, as shown in Figure &lt;a href=&quot;#exploration&quot;&gt;10&lt;/a&gt;, too large steps like 1000K would also bring additional exploration noise even making the training collapse. The results above confirm the $\epsilon$-greedy mechanism is still the proper and simplest choice in MARL but should be elaboratively tuned for different tasks.&lt;/p&gt; &lt;h3 id=&quot;integrating-the-techniques&quot;&gt;&lt;a name=&quot;Integrating_the_Techniques&quot;&gt;Integrating the Techniques&lt;/a&gt;&lt;/h3&gt; &lt;p&gt;These techniques mentioned above indeed impact QMIX in hard cooperative scenarios of SMAC, which really catches our attention to exhaust the extreme performance of QMIX. We combine these techniques and finetune all the hyperparameters in QMIX for each scenario of SMAC. As shown in Table &lt;a href=&quot;#table2&quot;&gt;2&lt;/a&gt;, the Finetuned-QMIX would almost conquer all the scenarios in SMAC and exceed the effect of the original QMIX by a large margin in some Hard and Super-Hard scenarios.&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;table2&quot;&gt; &lt;/a&gt;&lt;/p&gt; &lt;center&gt; Table 2: Best median test win rate of Finetuned-QMIX and QMIX (batch size=128) &lt;/center&gt; &lt;center&gt; in all testing scenarios. &lt;/center&gt; &lt;table style=&quot;text-align: center; width: 600px; margin: 0 auto; margin-bottom:20px; margin-top:20px&quot;&gt; &lt;thead&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;Senarios&lt;/b&gt;&lt;/td&gt; &lt;td&gt;&lt;b&gt;Difficulty&lt;/b&gt;&lt;/td&gt; &lt;td&gt;&lt;b&gt;QMIX&lt;/b&gt;&lt;/td&gt; &lt;td&gt;&lt;b&gt;Finetuned-QMIX&lt;/b&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;10m_vs_11m&lt;/td&gt; &lt;td&gt;Easy&lt;/td&gt; &lt;td&gt;98%&lt;/td&gt; &lt;td&gt;&lt;b&gt;100%&lt;/b&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;8m_vs_9m&lt;/td&gt; &lt;td&gt;Hard&lt;/td&gt; &lt;td&gt;84%&lt;/td&gt; &lt;td&gt;&lt;b&gt;100%&lt;/b&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5m_vs_6m&lt;/td&gt; &lt;td&gt;Hard&lt;/td&gt; &lt;td&gt;84%&lt;/td&gt; &lt;td&gt;&lt;b&gt;90%&lt;/b&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3s_vs_5z&lt;/td&gt; &lt;td&gt;Hard&lt;/td&gt; &lt;td&gt;96%&lt;/td&gt; &lt;td&gt;&lt;b&gt;100%&lt;/b&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;bane_vs_bane&lt;/td&gt; &lt;td&gt;Hard&lt;/td&gt; &lt;td&gt;&lt;b&gt;100%&lt;/b&gt;&lt;/td&gt; &lt;td&gt;&lt;b&gt;100%&lt;/b&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2c_vs_64zg&lt;/td&gt; &lt;td&gt;Hard&lt;/td&gt; &lt;td&gt;&lt;b&gt;100%&lt;/b&gt;&lt;/td&gt; &lt;td&gt;&lt;b&gt;100%&lt;/b&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;corridor&lt;/td&gt; &lt;td&gt;Super hard&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;td&gt;&lt;b&gt;100%&lt;/b&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MMM2&lt;/td&gt; &lt;td&gt;Super hard&lt;/td&gt; &lt;td&gt;98%&lt;/td&gt; &lt;td&gt;&lt;b&gt;100%&lt;/b&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3s5z_vs_3s6z&lt;/td&gt; &lt;td&gt;Super hard&lt;/td&gt; &lt;td&gt;3%&lt;/td&gt; &lt;td&gt;&lt;b&gt;93% (Hidden Size = 256)&lt;/b&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;27m_vs_3s6z&lt;/td&gt; &lt;td&gt;Super hard&lt;/td&gt; &lt;td&gt;56%&lt;/td&gt; &lt;td&gt;&lt;b&gt;100%&lt;/b&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6h_vs_8z&lt;/td&gt; &lt;td&gt;Super hard&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;td&gt;&lt;b&gt;93% (λ = 0.3)&lt;/b&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;role-of-monotonicity-constraint&quot;&gt;&lt;a name=&quot;Role_of_Monotonicity_Constraint&quot;&gt;Role of Monotonicity Constraint&lt;/a&gt;&lt;/h2&gt; &lt;h3 id=&quot;amazing-performance-in-policy-based-methods&quot;&gt;&lt;a name=&quot;Amazing_Performance_in_Policy-Based_Methods&quot;&gt;Amazing Performance in Policy-Based Methods&lt;/a&gt;&lt;/h3&gt; &lt;div id=&quot;qmix_sy&quot; class=&quot;img-height-180 image-center img-margin-left-30&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/riit.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/riit.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/riit.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/riit.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;center&gt;Figure 11: Architecture for AC-MIX: &lt;b&gt;|·|&lt;/b&gt; denotes &lt;b&gt;absolute value operation&lt;/b&gt;, implementing the monotonicity constraint of QMIX. &lt;b&gt;W&lt;/b&gt; denotes the non-negative mixing weights. Agent $i$ denotes the agent&apos;s network, which can be trained end-to-end by maximizing the $Q_{tot}$.&lt;/center&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;The novelty of QMIX is the IGM consistency between $\text{argmax} Q_{tot}$ and $\text{argmax} \sum_{i}^{n} Q_{i}$, which is implemented in the mixing network. &lt;strong&gt;We still expect to study the role of &lt;em&gt;monotonicity constraint&lt;/em&gt; in MARL&lt;/strong&gt;. Therefore, we propose an actor-critic style algorithm called Actor-Critic-Mixer (AC-MIX), which has a similar architecture to QMIX. As illustrated in Figure &lt;a href=&quot;#qmix_sy&quot;&gt;11&lt;/a&gt;, we use the monotonic mixing network as a centralized critic, which integrates $Q_{i}$ of each agent, to optimize the decentralized policy networks $π^i_{θ_i}$ in an end-to-end pattern. We still add the Adaptive Entropy $\mathcal{H}(\cdot)$ &lt;a href=&quot;#18&quot;&gt;[18]&lt;/a&gt; of each agent in the optimization object of Eq.(\ref{eq4}) to get more exploration, and the detail of the algorithm will be described in Appendix &lt;a href=&quot;#A&quot;&gt;A&lt;/a&gt;.&lt;/p&gt; \[\max _{\theta} \mathbb{E}_{t, s_{t}, \tau_{t}^{1}, \ldots, \tau_{t}^{n}}\left[Q_{\theta_{c}}^{\pi}\left(s_{t}, \pi_{\theta_{1}}^{1}\left(\cdot \mid \tau_{t}^{1}\right), \ldots, \pi_{\theta_{n}}^{n}\left(\cdot \mid \tau_{t}^{n}\right)\right) + \mathbb{E}_{i}\left[\mathcal{H}\left(\pi_{\theta_{i}}^{i}\left(\cdot \mid \tau_{t}^{i}\right)\right)\right]\right] \tag{4} \label{eq4}\] &lt;div id=&quot;riit_abla&quot; class=&quot;img-height-210 image-center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/monotonicity_riit.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/monotonicity_riit.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/monotonicity_riit.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/monotonicity_riit.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;center&gt;Figure 12: Comparing AC-MIX w./ and w./o. monotonicity constraint&lt;/center&gt; &lt;center&gt;(remove absolute value operation) on SMAC and Predator-Prey-2&lt;/center&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;As the monotonicity constraint on the critic of AC-MIX is theoretically no longer required as the critic is not used for greedy action selection. We can evaluate the effects of the monotonicity constraint by removing the absolute value operation in the mixing network. The results in Figure &lt;a href=&quot;#riit_abla&quot;&gt;12&lt;/a&gt; demonstrate the &lt;em&gt;monotonicity constraint&lt;/em&gt; significantly improves the performance of AC-MIX. Then to explore the generality of &lt;em&gt;monotonicity constraints&lt;/em&gt; in the parallel sampling framework of MARL, we extend the above experiments to VMIX [&lt;a href=&quot;#12&quot;&gt;12&lt;/a&gt;] . VMIX adds the monotonicity constraint to the value network of A2C, and learns the policy of each agent by advantage-based policy gradient [&lt;a href=&quot;#14&quot;&gt;14&lt;/a&gt;] as illustrated in Figure &lt;a href=&quot;#vmix_net&quot;&gt;13&lt;/a&gt;. Still, the result from Figure &lt;a href=&quot;#vmix_abla&quot;&gt;14&lt;/a&gt; shows that the monotonicity constraint improves the sample efficiency in value networks.&lt;/p&gt; &lt;div id=&quot;vmix_net&quot; class=&quot;img-height-180 image-center img-margin-left-60&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/vmix.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/vmix.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/vmix.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/vmix.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;center&gt;Figure 13. Architecture for VMIX: |·| denotes absolute value operation&lt;/center&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;div id=&quot;vmix_abla&quot; class=&quot;img-height-210 image-center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/monotonicity_vmix.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/monotonicity_vmix.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/monotonicity_vmix.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/monotonicity_vmix.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;center&gt;Figure 14: Comparing VMIX w./ and w./o. monotonicity constraint&lt;/center&gt; &lt;center&gt;(remove absolute value operation) on SMAC&lt;/center&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;h3 id=&quot;what-is-under-the-hood&quot;&gt;&lt;a name=&quot;What_is_Under_the_Hood&quot;&gt;What is Under the Hood?&lt;/a&gt;&lt;/h3&gt; &lt;p&gt;Observed from the results of previous experiments, &lt;strong&gt;the &lt;em&gt;monotonicity constraints&lt;/em&gt; in the mixing network indeed improve performance and sample efficiency of training&lt;/strong&gt;, but on the flip side of the coin, QMIX is still criticized for the insufficient expressive capacity of the centralized critic [&lt;a href=&quot;#3&quot;&gt;3&lt;/a&gt;], which may cause poor performance. The abnormal question naturally occurred to us: &lt;em&gt;Why the performance of AC-MIX would be better than AC-MIX-nonmonotonic which aims to relax the monotonicity constraint of mixing network&lt;/em&gt;?&lt;/p&gt; &lt;p&gt;To answer this question we first need to reexamine the &lt;strong&gt;IGM&lt;/strong&gt; principle. Since in QMIX, $Q_{tot}$ is decomposed by the mixing network into the sum of the weighted $[Q_i] _{i=1}^{n}$, as shown in Figure &lt;a href=&quot;#frame&quot;&gt;4&lt;/a&gt;, where the weights and bias of mixing network are generated by the &lt;em&gt;Hypernetwork&lt;/em&gt;, then the monotonicity in QMIX can be defined simplistically as a constraint on the relationship between \(Q_{tot}\) and each \(Q_{i}\) :&lt;/p&gt; \[Q_{tot} = \sum_{i=1}^{N}w_{i}(s_{t}) \cdot Q_{i} + b(s_{t}), \\ w_{i} = \frac{\partial Q_{tot}}{\partial Q_{i}} \geq 0, \forall i \in N. \tag{5} \label{5}\] &lt;p&gt;From the sufficient condition above, the weight $w_{i}$ in &lt;em&gt;Mixing Network&lt;/em&gt; would be forced to be greater or equal to zero $w_{i} \geq 0$. To put it another way, it makes the parameter space smaller for searching $w_{i}$ weights to decompose $Q_{tot}$. As illustrated in the schematic diagram &lt;a href=&quot;#diagram&quot;&gt;15&lt;/a&gt;, assume there is only 1 agent in the environment, the parameter searching space will be directly halved and the optimal $w_{1}$ will be found in the region where $w \geq 0$, i.e., the green region. Similarly, when the number of agents is 2 or 3, its parameter searching space for $w_i$ will be restricted to the first quadrant, and the same can be recursively extended to the case of high-dimensional parameter space. &lt;strong&gt;In other words, the search area of exhausting the whole joint state-action space would also be decreased exponentially by $(\frac{1}{2})^{N}$ ($N$ denotes the number of $w_{i}$, as well as the number of agents).&lt;/strong&gt; Then the optimal solution in the original domain cannot be expressed correctly in the restricted region. Since the essence of learning in MARL is to search for the optimal joint-policy parameterized by weights and bias of agents and mixing network, QMIX could find a satisfying policy more quickly in these &lt;strong&gt;reduced&lt;/strong&gt; parameter spaces.&lt;/p&gt; &lt;div id=&quot;diagram&quot; style=&quot;display:flex; margin:20px 0; gap:5px&quot;&gt; &lt;div id=&quot;1_agent&quot; class=&quot;img-height-100&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/1_agent.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/1_agent.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/1_agent.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/1_agent.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div id=&quot;2_agent&quot; class=&quot;img-height-100&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/2_agent.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/2_agent.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/2_agent.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/2_agent.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div id=&quot;3_agent&quot; class=&quot;img-height-100&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/3_agent.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/3_agent.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/3_agent.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/3_agent.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt; &lt;div style=&quot;margin-bottom: 20px&quot;&gt;&lt;center&gt;Figure 15: the weight parameter space diagram of different number of agents in QMIX [from-left-to-right]. (a) weight parameter space of only 1 agent; (b) weight parameter space of 2 agents; (c) weight parameter space of 3 agents.&lt;/center&gt;&lt;/div&gt; &lt;p&gt;As a side effect, the global optimum may not be in the parameter space that QMIX needs to search at all due to the monotonicity of the mixing network. One effective way is to estimate the $Q_{tot}$ as accurately as possible in the hope that it could find the global optimum, this probably explains why $Q(\lambda)$ in the previous section could result in such a performance improvement in SMAC. On the other hand, we could delicately design the reward function to be approximately monotonic when we use QMIX to solve cooperative multi-agent tasks. Then adapting the algorithm to the test environment is not a good idea, after all, we still need to figure out how to use QMIX more effectively or develop other more efficient algorithms.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;&lt;a name=&quot;Conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;In this post, we revisited the performance of the QMIX as a baseline algorithm in the SMAC environment. We found that the application of hyperparameters and other RL techniques have a great impact on the effectiveness of QMIX. We evaluated the effect of optimizer, number of rollout processes, replay buffer size, eligibility traces, hidden size and the degree of annealed exploration on QMIX, and tried to explain their role in MARL. Furthermore, we dived into the monotonicity in QMIX, and found the absolute operation in mixing network would decrease the parameter searching space of the joint state-action area exponentially by $(\frac{1}{2})^{N}$, which would make QMIX find the satisfying policy more quickly but with the drawback of inaccurate evaluated joint value function of optimal policy. We hope that our findings will stimulate some inspiration for the value decomposition method in MARL and provoke the community to think about the performance of QMIX as a new benchmark.&lt;/p&gt; &lt;h2 id=&quot;authorship-credit-attribution-and-acknowledgement&quot;&gt;&lt;a name=&quot;Authorship,_Credit_Attribution_and_Acknowledgement &quot;&gt;Authorship, Credit Attribution and Acknowledgement&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;Jian Hu was responsible for the key ideas, open source code and all experiments, as well as the first draft of the paper.&lt;/p&gt; &lt;p&gt;Siying Wang was responsible for the writing of the blog.&lt;/p&gt; &lt;p&gt;Siyang Jiang participated in writing the first draft of the paper.&lt;/p&gt; &lt;p&gt;Weixun Wang provided feedback on revisions.&lt;/p&gt; &lt;p&gt;Siyang Jiang was supported by the fund which aims to improve scientific research capability of key construction disciplines in Guangdong province “Light-weight federal learning paradigm and its application” (No:2022ZDJS058) and Foundation for Distinguished Young Talents in Higher Education of Guangdong, China. (NO. 2022KQNCX084)&lt;/p&gt; &lt;h2 id=&quot;appendix&quot;&gt;&lt;a name=&quot;Appendix&quot;&gt;Appendix&lt;/a&gt;&lt;/h2&gt; &lt;h3 id=&quot;a-pseudo-code-of-ac-mix-&quot;&gt;A Pseudo-code of AC-MIX&lt;a id=&quot;A&quot;&gt; &lt;/a&gt;&lt;/h3&gt; &lt;p&gt;In this subsection, we show the pseudo-code for the training procedure of AC-MIX. (1) Training the critic network with offline samples and 1-step TD error loss improves the sample efficiency for critic networks; (2) We find that policy networks are sensitive to old sample reuse. Training policy networks end-to-end and critic with TD($\lambda$) and online samples improve the learning stability of AC-MIX.&lt;/p&gt; &lt;div id=&quot;algorithm_riit&quot; class=&quot;img-height-600 image-center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/algorithm_riit.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/algorithm_riit.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/algorithm_riit.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-13-riit/algorithm_riit.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;h3 id=&quot;b-hyperparameters&quot;&gt;B HYPERPARAMETERS&lt;/h3&gt; &lt;p&gt;In this subsection, we present our hyperparameters tuning process. We get the optimal hyperparameters for each algorithm by grid search, shown in Table &lt;a href=&quot;#t3&quot;&gt;3&lt;/a&gt;.&lt;/p&gt; &lt;center&gt; Table 3: Hyperparameters Search on SMAC. The bold type indicates the &lt;/center&gt; &lt;center&gt; selected hyperparameters. &lt;/center&gt; &lt;table style=&quot;text-align: center; width: 700px; margin: 0 auto; margin-bottom:20px; margin-top:20px;&quot;&gt;&lt;a name=&quot;t3&quot;&gt; &lt;/a&gt; &lt;thead&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;Tricks&lt;/b&gt;&lt;/td&gt; &lt;td&gt;&lt;b&gt;QMIX&lt;/b&gt;&lt;/td&gt; &lt;td&gt;&lt;b&gt;AC-MIX&lt;/b&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Optimizer&lt;/td&gt; &lt;td&gt;&lt;b&gt;Adam&lt;/b&gt;,RMSProp&lt;/td&gt; &lt;td&gt;&lt;b&gt;Adam&lt;/b&gt;,RMSProp&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Learning Rates&lt;/td&gt; &lt;td&gt;0.0005, &lt;b&gt;0.001&lt;/b&gt;&lt;/td&gt; &lt;td&gt;0.0005, &lt;b&gt;0.001&lt;/b&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Batch Size (episodes)&lt;/td&gt; &lt;td&gt;32, 64, &lt;b&gt;128&lt;/b&gt;&lt;/td&gt; &lt;td&gt;32, &lt;b&gt;64&lt;/b&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Replay Buffer Size&lt;/td&gt; &lt;td&gt;&lt;b&gt;5000&lt;/b&gt;, 10000, 20000&lt;/td&gt; &lt;td&gt;2000, &lt;b&gt;5000&lt;/b&gt;, 10000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Q(λ)/TD(λ)&lt;/td&gt; &lt;td&gt;0, 0.3, &lt;b&gt;0.6&lt;/b&gt;, 0.9&lt;/td&gt; &lt;td&gt;0.3, &lt;b&gt;0.6&lt;/b&gt;, 0.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Entropy/Adaptive Entropy&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;0.005, 0.01, &lt;b&gt;0.03&lt;/b&gt;, 0.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;ε Anneal Steps&lt;/td&gt; &lt;td&gt;50K, &lt;b&gt;100K, 500K&lt;/b&gt;, 1000K&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rollout Processes Number&lt;/strong&gt;. For SMAC, 8 rollout processes for parallel sampling are used to obtain as many samples as possible from the environments at a high rate. And 4 rollout processes are used for Predator-Prey-2.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Other Settings&lt;/strong&gt;. We set all discount factors $\gamma$ = 0.99. We update the target network every 200 episodes.&lt;/p&gt; &lt;h2 id=&quot;reference&quot;&gt;&lt;a name=&quot;Reference&quot;&gt;Reference&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;&lt;a name=&quot;1&quot; href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;[1] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR 2015, San Diego, CA, USA, May 7-9, 2015, 2015. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;2&quot; href=&quot;https://arxiv.org/abs/2103.00107&quot;&gt;[2] Tadashi Kozuno, Yunhao Tang, Mark Rowland, Remi Munos, Steven Kapturowski, Will Dabney, Michal Valko, and David Abel. Revisiting peng’s q (λ) for modern reinforcement learning. arXiv preprint arXiv:2103.00107, 2021. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;3&quot; href=&quot;https://arxiv.org/abs/1910.07483&quot;&gt;[3] Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. MAVEN: multi-agent variational exploration. In NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,pp. 7611–7622, 2019. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;4&quot; href=&quot;https://arxiv.org/abs/1312.5602&quot;&gt;[4] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, DaanWierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;5&quot; href=&quot;http://proceedings.mlr.press/v48/mniha16.html&quot;&gt;[5] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In ICML 2016, New York City, NY, USA, June 19-24, 2016, pp. 1928–1937, 2016.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;6&quot; href=&quot;https://www.comp.nus.edu.sg/~leews/publications/rss09.pdf&quot;&gt;[6] Sylvie CW Ong, Shao Wei Png, David Hsu, and Wee Sun Lee. Pomdps for robotic tasks with mixed observability. 5:4, 2009. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;7&quot; href=&quot;https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1079&amp;amp;context=cs_faculty_pubs&quot;&gt;[7] Doina Precup, Richard S. Sutton, and Satinder P. Singh. Eligibility traces for off-policy policy evaluation. In ICML 2000, Stanford University, Stanford, CA, USA, June 29 - July 2, 2000, pp.759–766. Morgan Kaufmann, 2000. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;8&quot; href=&quot;http://proceedings.mlr.press/v80/rashid18a.html&quot;&gt;[8] Tabish Rashid, Mikayel Samvelyan, Christian Schr ̈oder de Witt, Gregory Farquhar, Jakob N.Foerster, and Shimon Whiteson. QMIX: monotonic value function factorization for deep multi-agent reinforcement learning. In ICML 2018, Stockholmsmassan, Stockholm, Sweden, July10-15, 2018, pp. 4292–4301, 2018. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;9&quot; href=&quot;https://ui.adsabs.harvard.edu/abs/2020arXiv200610800R/abstract&quot;&gt;[9] Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted QMIX: Expand-ing Monotonic Value Function Factorisation. arXiv preprint arXiv:2006.10800, 2020. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;10&quot; href=&quot;https://arxiv.org/abs/1902.04043&quot;&gt;[10] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, NantasNardelli, Tim G. J. Rudner, Chia-Man Hung, Philip H. S. Torr, Jakob Foerster, and ShimonWhiteson. The StarCraft Multi-Agent Challenge.arXiv preprint arXiv:1902.04043, 2019. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;11&quot; href=&quot;http://proceedings.mlr.press/v97/son19a.html&quot;&gt;[11] Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Hostallero, and Yung Yi. QTRAN: learning to factorize with transformation for cooperative multi-agent reinforcement learning. In ICML 2019, 9-15 June 2019, Long Beach, California, USA, pp. 5887–5896, 2019. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;12&quot; href=&quot;https://www.aaai.org/AAAI21Papers/AAAI-2412.SuJ.pdf&quot;&gt;[12] Jianyu Su, Stephen Adams, and Peter A. Beling. Value-Decomposition Multi-Agent Actor-Critics. arXiv:2007.12306, 2020. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;13&quot; href=&quot;https://arxiv.org/abs/1706.05296&quot;&gt;[13] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi,Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Grae-pel. Value-Decomposition Networks For Cooperative Multi-Agent Learning. arXiv preprint arXiv:1706.05296, 2017. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;14&quot; href=&quot;https://go.gale.com/ps/i.do?id=GALE%7CA61573878&amp;amp;sid=googleScholar&amp;amp;v=2.1&amp;amp;it=r&amp;amp;linkaccess=abs&amp;amp;issn=07384602&amp;amp;p=AONE&amp;amp;sw=w&quot;&gt;[14] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;15&quot; href=&quot;https://arxiv.org/abs/2008.01062&quot;&gt;[15] Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. QPLEX: Duplex Dueling Multi-Agent Q-Learning. arXiv:2008.01062, 2020. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;16&quot; href=&quot;https://www.aaai.org/ocs/index.php/SSS/SSS18/paper/viewPaper/17508&quot;&gt;[16] Ermo Wei, Drew Wicke, David Freelan, and Sean Luke. Multiagent Soft Q-Learning. arXivpreprint arXiv:1804.09817, 2018. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;17&quot; href=&quot;https://arxiv.org/abs/2002.03939&quot;&gt;[17] Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and HongyaoTang. Qatten: A General Framework for Cooperative Multiagent Reinforcement Learning. arXiv preprint arXiv:2002.03939, 2020. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;18&quot; href=&quot;https://arxiv.org/abs/2010.09776&quot;&gt;[18] Ming Zhou, Jun Luo, and Julian Villella et al. Smarts: Scalable multi-agent reinforcement learning training school for autonomous driving, 2020. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;19&quot; href=&quot;https://www.jmlr.org/papers/volume21/20-081/20-081.pdf&quot;&gt;[19] Rashid T, Samvelyan M, Schroeder de Witt C, et al. Monotonic value function factorisation for deep multi-agent reinforcement learning. Journal of Machine Learning Research, 2020, 21.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;20&quot; href=&quot;https://ojs.aaai.org/index.php/AAAI/article/view/6223&quot;&gt;[20] Wen C, Yao X, Wang Y, et al. Smix (λ): Enhancing centralized value functions for cooperative multi-agent reinforcement learning. Proceedings of the AAAI Conference on Artificial Intelligence. 2020, 34(05): 7301-7308. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;21&quot; href=&quot;http://www.cs.toronto.edu/~hinton/coursera/lecture6/lec6.pdf&quot;&gt;[21] Hinton G, Srivastava N, Swersky K. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent. Cited on, 2012, 14(8): 2.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;22&quot; href=&quot;http://proceedings.mlr.press/v119/fedus20a.html&quot;&gt;[22] Fedus W, Ramachandran P, Agarwal R, et al. Revisiting fundamentals of experience replay. International Conference on Machine Learning. PMLR, 2020: 3061-3071. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;23&quot; href=&quot;http://proceedings.mlr.press/v119/ota20a.html&quot;&gt;[23] Ota K, Oiki T, Jha D, et al. Can Increasing Input Dimensionality Improve Deep Reinforcement Learning?. International Conference on Machine Learning. PMLR, 2020: 7424-7433.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;24&quot; href=&quot;https://arxiv.org/abs/1312.6184&quot;&gt;[24] Ba L J, Caruana R. Do deep nets really need to be deep?. arXiv preprint arXiv:1312.6184, 2013.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;25&quot; href=&quot;https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html&quot;&gt;[25] He K, Zhang X, Ren S, et al. Deep residual learning for image recognition. Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;26&quot; href=&quot;https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.html&quot;&gt;[26] Huang G, Liu Z, Van Der Maaten L, et al. Densely connected convolutional networks. Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 4700-4708.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;27&quot; href=&quot;http://proceedings.mlr.press/v48/wangf16.html&quot;&gt;[27] Wang Z, Schaul T, Hessel M, et al. Dueling network architectures for deep reinforcement learning. International conference on machine learning. PMLR, 2016: 1995-2003.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;28&quot; href=&quot;https://www.ccs.neu.edu/home/camato/publications/OliehoekAmato16book.pdf&quot;&gt;[28] Oliehoek, Frans A., and Christopher Amato. A concise introduction to decentralized POMDPs. Springer, 2016.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a name=&quot;29&quot; href=&quot;https://proceedings.neurips.cc/paper_files/paper/2021/file/65b9eea6e1cc6bb9f0cd2a47751a186f-Paper.pdf&quot;&gt;[29] Peng, Bei, et al. Facmac: Factored multi-agent centralised policy gradients. Advances in Neural Information Processing Systems 34 (2021): 12208-12221.&lt;/a&gt;&lt;/p&gt; </content> </entry> <entry> <title>From HiPPO to H3</title> <link href="https://jocelynshen.com/blog/2022/hippo-to-h3/"/> <updated>2022-12-01T00:00:00+00:00</updated> <id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/hippo-to-h3</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;The recently-introduced &lt;strong&gt;H3 (Hungry Hungry HiPPOs)&lt;/strong&gt;&lt;d-cite key=&quot;h3&quot;&gt;&lt;/d-cite&gt; language modeling layer achieves promising results on language tasks over very long sequences, outperforming similar sized Transformers &lt;d-cite key=&quot;gpt-neox-lib&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;opt&quot;&gt;&lt;/d-cite&gt;. Moreover, the H3 layer exhibits a lower time complexity than transformers, and the authors demonstrate empirically that it provides faster inference as a result.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/h3_2.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/h3_2.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/h3_2.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/h3_2.svg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;In this post, we will explore the motivation behind H3 and the techniques that led to its development, including the Structured State Space Sequence(S4) model&lt;d-cite key=&quot;s4&quot;&gt;&lt;/d-cite&gt; and the High-order Polynomial Projection Operators (HiPPO) model&lt;d-cite key=&quot;hippo&quot;&gt;&lt;/d-cite&gt; from which H3 gets its name. We will also demonstrate the performance of H3 on two synthetic language modeling tasks that appear in the original paper. These tasks provide early indicators of a model’s capacity for &lt;em&gt;in-context learning&lt;/em&gt;&lt;d-cite key=&quot;inductionheads&quot;&gt;&lt;/d-cite&gt;, a property thought to be a key factor in the emergent capabilities of contemporary language models. Along the way, we will implement a simplified H3 model in PyTorch and discuss how it differs from the &lt;a href=&quot;https://github.com/hazyresearch/h3&quot;&gt;official implementation&lt;/a&gt;&lt;d-cite key=&quot;h3&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h2 id=&quot;state-space-models&quot;&gt;State Space Models&lt;/h2&gt; &lt;p&gt;H3 and S4 come from the world of &lt;strong&gt;state space models (SSMs)&lt;/strong&gt;, which have a long history in control theory and signal processing. SSMs encode input-output relationships as &lt;em&gt;dynamical systems&lt;/em&gt; with some visible and some internal, or hidden, states. The system’s hidden state \(X\) evolves over time according to the differential equation shown below, and the output \(y\) is a function of the current state and the input \(u\). We can write a general SSM as follows:&lt;/p&gt; \[\begin{align*} X&apos;(t) &amp;amp;= A(t)X(t) + B(t)u(t) \\ y(t) &amp;amp;= C(t)X(t) + D(t)u(t) \end{align*}\] &lt;p&gt;It’s fine for our purposes to assume that \(A(t), B(t), C(t), D(t)\) are linear; for any given \(t\), they may be expressed as matrix multiplications. We call systems like this &lt;strong&gt;linear state space models&lt;/strong&gt;. Many SSMs are constructed such that \(A(t), B(t),C(t), D(t)\) do not depend on \(t\) and as such are called &lt;strong&gt;time-invariant&lt;/strong&gt; &lt;d-cite key=&quot;httyh&quot;&gt;&lt;/d-cite&gt;. A (linear) time-invariant SSM could be written as follows:&lt;/p&gt; \[\begin{align*} X&apos;(t) = AX(t) + Bu(t)\\ y(t) = CX(t) + Du(t) \end{align*}\] &lt;h3 id=&quot;discrete-ssms&quot;&gt;Discrete SSMs&lt;/h3&gt; &lt;p&gt;In practice, we often work with discrete-time SSMs, in which the state evolves according to recurrence relation like the one below:&lt;/p&gt; \[\begin{align*} Xt &amp;amp;= AX_{t-1} + Bu_t\\ yt &amp;amp;= CX_t+ Du_t \end{align*}\] &lt;p&gt;These discrete steps may correspond to a uniform grid in time, but we can also (i) directly encode the time between observations in the input \(u\), or (ii) use a variable sampling rate, in which case the lengths of each time step can be learned by the model. A variable or adaptive sampling rate could be useful in a number of scenarios:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;When some features of the input are more frequently sampled than others (e.g. a GPS signal that is sampled at 1 Hz and an accelerometer that is sampled at 100 Hz).&lt;/li&gt; &lt;li&gt;When downstream tasks may require different sampling rates / timescales&lt;/li&gt; &lt;/ol&gt; &lt;h4 id=&quot;example-kalman-filter&quot;&gt;Example: Kalman filter&lt;/h4&gt; &lt;p&gt;Perhaps the most ubiquitous example of a state space model is the Kalman filter. It is modeled as a discrete state space recurrence with the addition of noise terms \(w_t\) and \(v_t\):&lt;/p&gt; \[X_t = AX_{t-1} + Bu_t + w_t y_t = CX_t + Du_t + v_t\] &lt;p&gt;Let’s unpack this a bit:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;\(w_t\) is the &lt;strong&gt;process noise&lt;/strong&gt;, which represents the uncertainty in the state transition. trajectory. This reflects how well/poorly the dynamics of the system are specified by the linear model.&lt;/li&gt; &lt;li&gt;\(v_t\) is the &lt;strong&gt;measurement noise&lt;/strong&gt;, which represents the uncertainty in the measurement. Notably, a better measurement tool (e.g. a graduated cylinder instead of a measuring cup) can reduce this noise source, but not the process noise \(w_t\).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This model excels in particular at filtering out the measurement noise \(v_t\), which is why it is so popular in signal processing and mechanical control systems. In these settings, work is often done to characterize the noise sources and to tune the model accordingly.&lt;/p&gt; &lt;p&gt;The Kalman filter has some desirable properties, including:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;A simple derivation for its update rule, which makes it easy to understand and implement.&lt;/li&gt; &lt;li&gt;A closed-form solution for the posterior distribution of the state given the observations as well as a constant-time update rule&lt;/li&gt; &lt;li&gt;If its assumptions hold, i.e. the system is correctly specified by the model and the covariance matrices are known, the Kalman filter is minimizes the mean squared error of the state estimate.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The Kalman filter also has some limitations. For nonlinear systems, the Extended Kalman Filter and Unscented Kalman Filter are popular extensions. There is also a growing body of work on the use of Kalman filters in deep learning&lt;d-cite key=&quot;deepkalman&quot;&gt;&lt;/d-cite&gt;, and this is true of other SSMs as well.&lt;/p&gt; &lt;h3 id=&quot;deep-ssms&quot;&gt;Deep SSMs&lt;/h3&gt; &lt;p&gt;Despite a number of desirable properties that made them historically successsful, many SSMs are poorly suited to deep learning. This may owe itself to any number of reasons, including:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Contemporary hardware works more easily with parallelizable operations, and the natural interpretation of an SSM is sequential.&lt;/li&gt; &lt;li&gt;Although &lt;em&gt;some&lt;/em&gt; instances of SSMs can be very efficient and information rich, the general case, in which the transition matrices \(A, B, C\) and \(D\) are arbitrarily initialized and learned, is not.&lt;/li&gt; &lt;li&gt;State space models require careful initialization and tuning of hyperparameters,and they are not easily interpretable.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;With this in mind, there appears to be a need for a new class of SSMs that are more amenable to deep learning. Ideally, these models would:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Perform well even with irregularly sampled data or unequal timescales&lt;/li&gt; &lt;li&gt;Be easy to initialize and train&lt;/li&gt; &lt;li&gt;Exhibit fast inference and training times&lt;/li&gt; &lt;li&gt;Come with theoretical guarantees and empirical evidence of their performance&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;And so, we arrive at HiPPO.&lt;/p&gt; &lt;h2 id=&quot;hippo-high-order-polynomial-projection-operators&quot;&gt;HiPPO: High-order Polynomial Projection Operators&lt;/h2&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/hippo-2x2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/hippo-2x2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/hippo-2x2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/hippo-2x2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;So HiPPO is introduced to address these concerns with deep state space models. How does it work? Well, Let’s jump right into the original definition:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Definition 1&lt;/strong&gt;. Given a time-varying measure family \(\mu^{(t)}\) supported on \(\\(-\infty, t \\]\), an N-dimensional subspace \(G\) of polynomials, and a continuous function \(f:R≥0→R\), HiPPO defines a projection operator \(proj_t\) and a coefficient extraction operator \(coef\), at every time \(t\), with the following properties:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;\(proj_t\) takes the function \(f\) restricted up to time \(t\) ,\(f≤t:= f(x)\rvert_t\), and maps it to a polynomial \(g(t)\in G\), that minimizes the approximation error \(\lVert f≤t− g(t)\rVert _{L2((t))}\)&lt;/li&gt; &lt;li&gt;\(coef_t: G → RN\) maps the polynomial \(g(t)\) to the coefficients \(c(t)\) RN of the basis of orthogonal polynomials defined with respect to the measure \(\mu^{(t)\)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Let’s work through the ideas in this definition:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;For a function \(f(t)\) , define \(f_t := f\) restricted to the interval \((-\infty, t]\) . For example, below we have \(f_0 ... f_5\).&lt;/li&gt; &lt;/ol&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/hippo-2x2_4-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/hippo-2x2_4-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/hippo-2x2_4-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/hippo-2x2_4.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;For each t, our goal is to approximate ft using a simple and expressive functional form.&lt;/p&gt; &lt;p&gt;To do this, we need to select a method for describing the approximation error between two given functions. We will do so using a family of measures \(\mu_t\)) that evolve with t. Then, we must select a candidate subspace \(G\subset F\) from which to search for the function \(g(t)\) that minimizes the approximation error between \(f(t)\) and \(g(t)\). We will choose GF to be a space of polynomial functions, and we will express each g(t)G according to a chosen basis of orthogonal polynomials L = (l1 ,l2, … , lN).&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/hippo-2x2_1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/hippo-2x2_1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/hippo-2x2_1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/hippo-2x2_1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;With this in place, we can define a function c(t) that maps each t to the corresponding coefficients of g(t) with regards to the orthogonal basis \(L=(l_1, l_2, ... , l_p)\) . Then, we can differentiate \(c(t)\) to produce the ordinary differential equation shown below, which captures the way \(c(t)\) evolves through time as determined by \(f(t)\). \(\frac{d}{dt}c(t) = A(t)c(t) + B(t)f(t)\)&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/hippo-2x2_2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/hippo-2x2_2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/hippo-2x2_2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/hippo-2x2_2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Rather than working with the continous ODE, the authors we will discretize it into a recurrence relation given as&lt;/p&gt; \[c_{k+1} = A_kc_k + B_kf_k\] &lt;p&gt;If we can find operators \(Ak, Bk\) that satisfy the above recurrence, then they satisfy the HiPPO definition and therefore provide the best approximation for the sequence \({fk}\).&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/hippo-2x2_3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/hippo-2x2_3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/hippo-2x2_3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/hippo-2x2_3.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Now, let’s apply the HiPPO definition to a specific case. We will see how the construction of a HiPPO operator starts with the selection of a measure family \(\mu^{(t)}\) and then proposes a suitable basis of orthogonal polynomials \(L\) to use in the approximation.&lt;/p&gt; &lt;h3 id=&quot;hippo-legt-legendre-time-invariant&quot;&gt;HiPPO-LegT (Legendre Time-invariant)&lt;/h3&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/legt-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/legt-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/legt-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/legt.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;HiPPO-LegT is based on Legendre Polynomials. These are defined as the system of polynomials that satisfy the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;They are orthogonal on the interval \([-1, 1]\) with respect to the uniform measure on that interval.&lt;/li&gt; &lt;li&gt;Denote by \(P_n\) the nth Legendre polynomial. Then, for all \(n\), \(P_n(1) = 1\) and \(P_n(-1) = (-1)^n\).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Gu et al. first consider the Legendre polynomials rescaled and shifted to be orthonormal on the interval \([0, 1]\) with respect to the measure \(\mu^{(t)}(x) = \frac 1 \theta \mathbb I_{\lbrack t-\theta, t\rbrack}\), where \(\theta\) is a constant. In other words, the measure is uniform on the interval \([t-\theta, t]\) and zero elsewhere.&lt;/p&gt; &lt;p&gt;Taking this basis and measure family, the authors show that the discretized ODE for HiPPO-LegT takes the form of a state space model with the following transition matrices:&lt;/p&gt; &lt;p&gt;\(A_{nk} = \frac{1}{\theta}\begin{cases} (-1)^{n-k}(2n+1) &amp;amp; n \geq k \\ 2n+1 &amp;amp; n \leq k \end{cases}\) x \(B_n = \frac 1 \theta (2n+1)(-1)^n\)&lt;/p&gt; &lt;p&gt;In &lt;em&gt;other&lt;/em&gt; other words, this just corresponds to a simple sliding window average with equal weights. This naturally motivates the question: what about other types of moving averages? What happens, for example, if we use exponential weighting instead of uniform? But more on that later. First, let’s point out some related work:&lt;/p&gt; &lt;h4 id=&quot;the-legendre-memory-unit&quot;&gt;The Legendre Memory Unit&lt;/h4&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/spiking-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/spiking-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/spiking-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/spiking.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;The HiPPO-LegT state space model parameterization is equivalent to that of the Legendre Memory Unit &lt;d-cite key=&quot;lmu&quot;&gt;&lt;/d-cite&gt; (LMU) &lt;d-cite key=&quot;lmu&quot;&gt;&lt;/d-cite&gt;, which was originally designed to mimic dynamics of biological Spiking Neural Networks. Gu et al. cite the LMU as motivation for HiPPO, but also note that the LMU “approaches the problem from the opposite direction as us; it considers approximating spiking neurons in the frequency domain, while we directly solve an interpretable optimization problem in the time domain”&lt;d-cite key=&quot;hippo&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h3 id=&quot;hippo-lagt-laguerre-time-invariant&quot;&gt;HiPPO-LagT: Laguerre Time-invariant&lt;/h3&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/lagt-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/lagt-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/lagt-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/lagt.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;As noted, the HiPPO-LegT model is determined by the choice of a uniform measure that acts like a sliding window average. We can construct the HiPPO operator that corresponds to exponential moving averages as well. In this case, the measure is given by the following:&lt;/p&gt; \[\mu^{(t)}(x) = e ^ {x - t} \mathbb I_{\lbrack -\infty, t\rbrack}(x).\] &lt;p&gt;To find the corresponding state space model, we need to first find a polynomial basis that is orthonormal with respect to this measure. Luckily, this is precisely what the &lt;strong&gt;Laguerre polynomials&lt;/strong&gt; are. The Laguerre polynomials are defined as the system of polynomials that satisfy the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;They are orthogonal on the interval \([0, \infty)\) with respect to the measure \(\mu^{(t)}(x) = e ^ {x - t} \mathbb I_{\lbrack -\infty, t\rbrack}(x)\).&lt;/li&gt; &lt;li&gt;Denote by \(L_n\) the nth Laguerre polynomial. Then, for all \(n\), \(L_n(0) = 1\) and \(L_n&apos;(0) = 0\).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The transition matrices for the HiPPO-LagT state space model are given by:&lt;/p&gt; \[A_{nk} = \begin{cases} 1 &amp;amp; n \geq k \\ 0 &amp;amp; n \leq k \end{cases}\] \[B_n = 1\] &lt;h3 id=&quot;hippo-legs-legendre-scale-invariant&quot;&gt;HiPPO-LegS: Legendre Scale-invariant&lt;/h3&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/legS-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/legS-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/legS-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/legS.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;The last two HiPPO variants, HiPPO-LegT and HiPPO-LagT, are both time-invariant, meaning that the transition matrices are the same for all values \(t\). Let’s now consider a scale-invariant variant, HiPPO-LegS.&lt;/p&gt; &lt;p&gt;We start with the &lt;strong&gt;scaled Legendre measure (LegS)&lt;/strong&gt;, which assigns uniform weight across an ever-growing interval \([-\infty, t]\). Under this measure, the authors derive the continuous time ODE for HiPPO-LegS, which is given by&lt;/p&gt; \[\frac{d}{dt} c(t) = A(t)c(t) + B(t)f(t)\\ A(t) = \frac 1 t A \\ B(t) = \frac 1 t B\] &lt;p&gt;where \(A\) and \(B\) are the transition matrices for the HiPPO-LegT model. The authors then discretize this ODE to obtain the state space model for HiPPO-LegS. The transition matrices are given by:&lt;/p&gt; \[\begin{align*} A_{nk} &amp;amp;= \begin{cases} (2n+1)^{1/2}(2k+1)^{1/2} &amp;amp; n &amp;gt; k \\ n + 1 &amp;amp; n = k \\ 0 &amp;amp; n &amp;lt; k \end{cases} B_n &amp;amp;= (2n+1)^{1/2} \end{align*}\] &lt;h2 id=&quot;structured-state-spaces&quot;&gt;Structured State Spaces&lt;/h2&gt; &lt;p&gt;While the HiPPO framework the specific HiPPO operators that we’ve discussed so far have proven effective in encoding memory, they are not in general efficient to compute. The “default” view of a discrete state space model is a recurrence relation; each step of the model is computed by multiplying the previous state by the transition matrix.&lt;/p&gt; &lt;p&gt;This, along with the “vanishing gradient” problem, is why RNNs are so slow and difficult to train. Any time we want to compute the state of the model at time \(t\), we have to compute the state at time \(t-1\), \(t-2\), and so on, all the way back to \(t-n\), where \(n\) is the number of steps in the model. And this is something we need to do often during backpropagation!&lt;/p&gt; &lt;p&gt;To remedy this, Gu et al. introduced the Structured State Space Sequence (S4) Model, which uses a special factorization scheme to speed up matrix multiplications without sacrificing expressive power or numerical stability. In fact, because the factorization is so much more efficient, we are able to change the way we view and work with the SSM. Instead of thinking of the model as a recurrence relation that emits a new state and evolves its internal state at each step, we can think of it as a convolution that can be computed across the entire sequence at once. But what do we mean by “structure”?&lt;/p&gt; &lt;h3 id=&quot;structured-matrices&quot;&gt;Structured Matrices&lt;/h3&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/s4conv-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/s4conv-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/s4conv-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/s4conv.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;A structured matrix is a matrix than can be multiplied more efficiently than the naive O(n^2) complexity. For example, the fast fourier transform (FFT) can be computed in O(n log n) time because of its structured representation. Similarly, diagonal and block-diagonal matrices are can be multiplied in O(n) time. Matrix structure is a generalization of matrix sparsity to matrices that are not necessarily sparse, but permit efficient multiplication, inversion, and other operations.&lt;/p&gt; &lt;h3 id=&quot;linear-time-invariant-ssms-as-convolutions&quot;&gt;Linear Time-invariant SSMs as Convolutions:&lt;/h3&gt; &lt;p&gt;The recurrent form of an arbitrary SSM is poorly suited for training on modern hardware, where parallel computation is the norm. And yet, there is a well-known way to describe a linear recurrence as a convolution: the &lt;strong&gt;discrete convolution&lt;/strong&gt;. The discrete convolution is a linear operation that takes two sequences and produces a third sequence.&lt;/p&gt; &lt;p&gt;The basic idea is that we can “unroll” the recurrence relation into and then refactor it as the convolution of the input sequence with a convolutional kernel \(\bar{K}\). The original authors describe this as follows:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/s4conv-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/s4conv-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/s4conv-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/s4conv.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Simple, right?&lt;/p&gt; &lt;h3 id=&quot;normal-plus-low-rank-nplr-factorization&quot;&gt;Normal-plus-low-rank (NPLR) Factorization&lt;/h3&gt; &lt;p&gt;Not so simple!&lt;/p&gt; &lt;p&gt;Just because we can &lt;em&gt;write&lt;/em&gt; the SSM as a convolution doesn’t mean that we can &lt;em&gt;compute&lt;/em&gt; it as a convolution. In fact, the naive implementation of the convolutional kernel is not efficient at all.&lt;/p&gt; &lt;p&gt;The authors of the S4 paper propose using a parameterization called “normal-plus-low-rank” (NPLR). The motivation is simple enough. We know that if the transition matrix \(A\) was diagonal, then we could compute the convolution in \(O((N + L)\log^2(N+L))\) time. Compare this to \((O^2L)\) operations for the naive implementation. This would be all well and good, but we know that the transition matrix is not diagonal.&lt;/p&gt; &lt;p&gt;It turns out, though, that for any of the HiPPO matrices previously discussed, the \(A\) matrix can be written as the sum of a &lt;strong&gt;normal matrix&lt;/strong&gt; and a low-rank matrix. And this gets us much closer to where we need to go.&lt;/p&gt; &lt;p&gt;A normal matrix is a matrix that is the sum of a diagonal matrix and a symmetric matrix. The low-rank matrix is a matrix that can be written as the product of two matrices of lower rank. More explicitly, we have \(A = \Lambda - PP^*\) for some diagonal \(\Lambda \in \mathbb C^{N\times N}\), \(P \in \mathbb C^{N\times r}\), and \(r \ll N\).&lt;/p&gt; &lt;p&gt;Unfortunately, trying to perform the repeated matrix multiplications in the convolution kernel is still not efficient while we have that pesky low-rank term hanging around.&lt;/p&gt; &lt;p&gt;From here, the authors propose and execute a series of clever techniques to get S4 to be more efficient and less prone to divergence. This is, however, our exit ramp for S4. Long live its successor, S4D!&lt;/p&gt; &lt;h2 id=&quot;simplifying-s4-diagonalization&quot;&gt;Simplifying S4: Diagonalization&lt;/h2&gt; &lt;p&gt;S4D is a diagonal version of S4, and bypasses some of those mind-bending techniques that show up in the implementation of S4.&lt;/p&gt; &lt;p&gt;First the paper recounts the definition of continuous SSMs and their convolutional forms:lg&lt;/p&gt; \[\begin{aligned} \dot{x}(t) &amp;amp;= A(t)x(t) + B(t)u(t) &amp;amp; K(t) &amp;amp;= Ce^{tA}B \\ y(t) &amp;amp;= C(t)x(t) + D(t)u(t) &amp;amp; y(t) &amp;amp;= (K \ast u)(t) \end{aligned}\] &lt;p&gt;We’ve seen the discrete version of this convolution before in the S4 paper, via the construction of the matrix&lt;/p&gt; \[\begin{aligned} \bar{K} = (\bar{C}\bar{B}, \bar{C}\bar{A}\bar{B}, \bar{C}\bar{A^2}\bar{B}, \dots, \bar{C}\bar{A}^{L-1}\bar{B}) = (\bar{C}\bar{A}^i\bar{B})\big|_L \end{aligned}\] &lt;p&gt;We should think of the continous convolution as a linear combination of an infinite number of orthogonal polynomial basis functions. The discrete convolution is a finite approximation of this infinite sum.&lt;/p&gt; &lt;p&gt;The authors of S4D propose a new parameterization for the SSM that is more efficient to compute. The key idea is to diagonalize the transition matrix \(A\), and then use the diagonalization to compute the convolution kernel in the frequency domain.&lt;/p&gt; &lt;p&gt;Some of the big ideas here are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;The \(A\) matrix is constrained to have &lt;strong&gt;negative eigenvalues&lt;/strong&gt;,** which keeps the kernel stable. There are multiple ways to enforce this constraint, but the authors do the following: parameterizing \(A\) as \(A = -\exp(A_{Re})+i\cdot A_{Im}\). They also note that any other activation functions bounded on one side could be used in place of \(\exp\) here. They conduct ablation tests with softmax and ReLu, as well as without any activation function at all.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The &lt;strong&gt;\(B\) vector is frozen&lt;/strong&gt; at \(B = 1\) for all time. This is because the convolutional term \(\bar K\) relies only on the elementwise product of \(B\) and \(C\) and not the two vectors themselves. In ablations, the authors show that there may be some minor benefit to learning \(B\), but not to any great extent.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The SSM parameters \(A, B, C, D\) are now complex-valued, but the input and output, \(u\) and \(y\) respectively, are still real-valued. How did we get here? The authors noted in S4 that complex matrices are, in a sense, more likely to be diagonalizable than real matrices – the set of diagonal matrices is dense in the set of complex matrices. However, because the convolution kernel is conjugate symmetric (it is the same as its complex conjugate), we can use the real-valued form of the Fast Fourier Transform, which is much more efficient to compute. By using the real-valued form of the FFT, we can compute the convolution kernel in \(O(N\log N)\) time, which is much faster than the \(O(N^2)\) time of the naive implementation.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;One quick note about complex tensors in deep neural networks: we will soon see that PyTorch has its quirks in working with these data types.&lt;/p&gt; &lt;h3 id=&quot;s4d-initializations&quot;&gt;S4D Initializations&lt;/h3&gt; &lt;p&gt;The authors share three different initialization schemes for S4D.&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;S4D-LegS&lt;/strong&gt;: Simply drop the low-rank term of the normal-plus-low-rank factorization of HiPPO-LegS used in S4. Recall the NPLR form was given by \(A = A^{(D)} - PP^T\), with \(A^{(D)}\) being the diagonal matrix containing the eigenvalues of \(A\).&lt;/p&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;S4D-Inv&lt;/strong&gt;: Approximate \(A^{(D)}\) via an empirically determined scaling law. First, we determine that the real component of \(A^{(D)}\) is \(-\frac 1 2 \mathbf 1\), where \(\mathbf 1\) is the matrix of all ones. Next, let \(\mathcal I (A)_n\) denote the n_largest imaginary component in the eigenvalues of \(A^{(D)}\). Then, the authors observe the following: &lt;ul&gt; &lt;li&gt;\(\mathcal I (A)_0 \rightarrow \frac 1 2 N^2 + c\) for constant \(c \approx 0.5236\).&lt;/li&gt; &lt;li&gt;All other eigenvalues satisfy an inverse scaling in n: \(\mathcal I (A)_n = \Omega (\frac 1 n)\).&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Simplify the imaginary part of \(A_n\) even further by setting each component to its corresponding Fourier series frequency, i.e. by setting \(\mathcal I (A)_n = i \pi n\) for all \(n\). &lt;strong&gt;This is the initialization used in the our implementation below.&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;s4d-implementation&quot;&gt;S4D Implementation&lt;/h3&gt; &lt;p&gt;We’ve already seen how frameworks like HiPPO and models like S4D permit a wide range of possible parameterizations. For the sake of keeping this codebase minimal and easy to understand, we will only implement one specific parameterization of S4D. Our parameterization is based on the following choices:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Discretization: Zero-order hold (ZOH). The original paper implements ZOH as well as bilinear interpolation.&lt;/li&gt; &lt;li&gt;Scaling: &lt;strong&gt;S4D-Lin&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Eigenvalue constraint: \(-\exp(A_{Re})\) We constrain \(A\) to have negative real eigenvalues parameterizing it as \(A = -\exp(A_{Re})+i\cdot A_{Im}\).&lt;/li&gt; &lt;li&gt;Parameterization: &lt;ul&gt; &lt;li&gt;\(A\): Trainable, complex-valued, real component parametrized as $$\exp(log).&lt;/li&gt; &lt;li&gt;\(B\): Ones (fixed). Corresponds to the ZOH discretization.&lt;/li&gt; &lt;li&gt;\(C\): Trainable, complex-valued, normally distributed initial conditions&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Alright, let’s get to it. We’ll start with a module that generates the kernel for the S4D layer. We should be able to specify the dimension &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d_model&lt;/code&gt; of each input/output value as well as the dimension &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d_state&lt;/code&gt; of the state matrix. Lastly, we will specify the minimum and maximum time step &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dt_min&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dt_max&lt;/code&gt; respectively.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;S4DKernel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Generates a kernel for the S4D layer. Args: d_model: The number of heads. d_state: The dimension of the state matrix dt_min: The minimum time step. dt_max: The maximum time step. Returns: A kernel of shape (L, H, N, N) &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt_min&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt_max&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_dt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt_min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_A_real&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A_imag&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cfloat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Complex&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_dt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_A_real&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_A_real&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_imag&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_imag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Complex params must be stored as a real tensor of shape (:, 2) &lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# or as separate components as we did with A above. &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;view_as_real&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Generates S4D kernel of shape (L, H, N, N)&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Construct the SSM parameters &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (H) &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_A_real&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1j&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_imag&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (H N) &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;view_as_complex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (H N) &lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Construct Vandermonde matrix &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dA&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dA&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;There shouldn’t be anything too surprising in this code by now. One quick thing to note is the use of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.view_as_real&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.view_as_complex&lt;/code&gt; to convert between real and complex tensors. For some reason, PyTorch doesn’t like it if we try to pass a complex tensor as an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.Parameter&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;This kernel is constructed via Vandermonde multiplication. A &lt;strong&gt;Vandermonde matrix&lt;/strong&gt; is one whose columns are powers of a vector. For example, we can construct the Vandermonde matrix of degree \(N\) for a vector \(X\) as \(\begin{align} V^{(N)} &amp;amp;= \begin{bmatrix}X^0 &amp;amp; X^1 &amp;amp; X^2 &amp;amp; ... &amp;amp; X^N\end{bmatrix} \\ &amp;amp; \\ &amp;amp;= \begin{bmatrix} 1 &amp;amp; X_1 &amp;amp; X_1^2 &amp;amp; ... &amp;amp; X_1^N \\ 1 &amp;amp; X_2 &amp;amp; X_2^2 &amp;amp; ... &amp;amp; X_2^N \\ 1 &amp;amp; X_3 &amp;amp; X_3^2 &amp;amp; ... &amp;amp; X_3^N \\ \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ 1 &amp;amp; X_N &amp;amp; X_N^2 &amp;amp; ... &amp;amp; X_N^N \end{bmatrix} \end{align}\)&lt;/p&gt; &lt;p&gt;With the kernel in hand, let’s turn our attention to the S4D layer.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;S4D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;S4D (Diagonal Structured State Space Sequence Model) layer. Args: d_model: The number of heads. d_state: The dimension of the state matrix. kernel_args: Arguments to pass to the kernel, i.e. only dt_min, dt_max in this implementation. &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_state&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;S4DKernel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Input and output shape (B, H, L)&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;kernel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Perform convolution in the frequency domain &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u_f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;rfft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;rfft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;irfft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u_f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Truncate to original length and add skip connection &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[...,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Here we see the S4D layer in action. The input &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;u&lt;/code&gt; is a tensor of shape &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(B, H, L)&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B&lt;/code&gt; is the batch size, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;H&lt;/code&gt; is the number of heads, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L&lt;/code&gt; is the sequence length. The output &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y&lt;/code&gt; is also a tensor of shape &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(B, H, L)&lt;/code&gt;. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;D&lt;/code&gt; parameter is a vector of length &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;H&lt;/code&gt; that is used to scale the skip connection.&lt;/p&gt; &lt;p&gt;After computing the kernel, we perform a convolution in the frequency domain. This is done by taking the Fourier transform of the input and kernel, multiplying them, and then taking the inverse Fourier transform. This is a lot faster than computing the convolution in the time domain. We then truncate the output to its original length \(L\) and add the skip connection.&lt;/p&gt; &lt;p&gt;That takes care of all our pre-reqs. Now we can finally implement the H3 layer!&lt;/p&gt; &lt;h2 id=&quot;h3-hungry-hungry-hippos&quot;&gt;H3: Hungry Hungry HiPPOs&lt;/h2&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/h3_2.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/h3_2.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/h3_2.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/h3_2.svg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Rather than diving into the H3 paper and working our way down, we’ll start by writing an H3 layer and then working our work our way up. We have an rather simple implementation of H3, so let’s just jump in.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;H3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;The Hungry Hungry Hippos (H3) layer. Args: d_model: The number of heads. d_state: The dimension of the state matrix. kernel_args: Arguments to pass to the S4D kernel. &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_state&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_proj&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k_proj&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v_proj&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s4d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;S4D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shift&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Shift&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;rearrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;q_proj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;b h l -&amp;gt; h l b&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;rearrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;k_proj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;b h l -&amp;gt; h l b&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;rearrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;v_proj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;b h l -&amp;gt; h l b&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shift_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shift&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s4d_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;s4d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shift_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;rearrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s4d_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;h l b -&amp;gt; b h l&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;The skeleton of the H3 bears some resemblance to the attention mechanism in the Transformer. In fact, H3 is intentionally designed to resemble the mechanism of &lt;strong&gt;linear attention&lt;/strong&gt;&lt;d-cite key=&quot;linattn&quot;&gt;, but with a few key differences. Let&apos;s review:&lt;/d-cite&gt;&lt;/p&gt; &lt;h3 id=&quot;self-attention&quot;&gt;Self-attention&lt;/h3&gt; &lt;p&gt;Self-atention is computed as shown below. For layer input \(X\), the layer projects \(X\) into \(Q, K, V\) matrices. Then, with \(d_k\) as the dimension of the key matrix \(K\), the attention function is defined as follows:&lt;/p&gt; \[\begin{align*} \text{Attention} &amp;amp; = &amp;amp; V&apos;&amp;amp; = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V \end{align*}\] &lt;h3 id=&quot;linear-attention&quot;&gt;Linear Attention&lt;/h3&gt; &lt;p&gt;Linear attention, introduced by () , is a simple extension of self-attention that aims to draw a throughline between the Transformer and the RNN. To obtain the framework for linear attention, we first rewrite the attention function as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Start with standard attention: \(V&apos;= \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V\)&lt;/li&gt; &lt;li&gt;Rewrite in terms of rows/columns instead of matrices: \(V&apos;_i= \frac{\sum_{j=1}^L e^{q^TK}}{\sum_{j=1}^L e^{q^TK}}v_j\)&lt;/li&gt; &lt;li&gt;Instead of the softmax, we can substitute an arbitrary similarity function sim. So all together we have the following: \(V&apos;_i= \frac{\sum_{j=1}^L sim(QK)}{\sum_{j=1}^L sim(q^TK)}v_j\)]&lt;/li&gt; &lt;li&gt;Linear attention makes the assumption that the similarity function &lt;em&gt;itself&lt;/em&gt; is linear, unlike the softmax, but that it operates on the output of a non-linear kernel \(\phi\) applied to each of \(Q\), \(K\), and \(V\):&lt;/li&gt; &lt;/ol&gt; \[sim(q, k) = \phi(q)^T\phi(k)\] &lt;p&gt;The authors then show that this framework connects the Transformer to the RNN. In particular, they show that the output at any given time is a function of two cumulative sums:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The output \(O_i\) at time \(i\) is given by \(O_i = \frac{\sum_{j=1}^i \phi(K_j)V_j^T}{\sum_{j=1}^i \phi(K_j)}\)&lt;/li&gt; &lt;li&gt;Let \(S_i = \sum_{j=1}^i \phi(K_j)V_j^T\)&lt;/li&gt; &lt;li&gt;Let \(z_i = \sum_{j=1}^i \phi(K_j)\)&lt;/li&gt; &lt;li&gt;Let \(d_i = \phi(Q_i)^Tz_i\)&lt;/li&gt; &lt;li&gt;Then \(O_i = \frac{\phi(Q_i)^TS_i}{d_i}\)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In other words, the output at time \(i\) is a function of the cumulative sum of the kernel applied to \(K\) and \(V\), and the cumulative sum of the kernel applied to \(Q\). This is exactly the same as the RNN, where the output at time \(i\) is a function of the cumulative sum of the input \(X\) and the cumulative sum of the hidden state \(H\).&lt;/p&gt; &lt;p&gt;This is a very interesting result, and one which motivated the development of H3.&lt;/p&gt; &lt;h3 id=&quot;h3&quot;&gt;H3&lt;/h3&gt; &lt;p&gt;So how does H3 differ from standard and linear attention? First, we have the presence of two state space models within the H3 layer. One of them is a standard S4D or similar SSM, and the other SSM features a specially constructed transition matrix we will call a &lt;strong&gt;shift matrix&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The shift SSM’s transition matrix \(A\) has the following basic form:&lt;/p&gt; \[\begin{align*} A_{nk} = \begin{cases} 1 &amp;amp; k = n - 1 \\ 0 &amp;amp; otherwise \end{cases} \end{align*}\] &lt;p&gt;Let’s see what happens when we apply our shift matrix to the state \(X = \begin{pmatrix} x_1 &amp;amp; x_2 &amp;amp; x_3 &amp;amp; x_4 \end{pmatrix}^T\):&lt;/p&gt; \[\begin{align*} Ax &amp;amp;= \begin{pmatrix} 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\ 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \end{pmatrix} = \begin{pmatrix} 0 \\ x_1 \\ x_2 \\ x_3 \end{pmatrix} \end{align*}\] &lt;p&gt;So this matrix is shifting the state vector to the right by one. This is exactly what we want, because we want to create interactions between the current token and the previous tokens, and that won’t happen if we only have diagonal transition matrices. Instead, we need to keep the past tokens within reach in some sense, and that’s precisely the role of the shift matrix.&lt;/p&gt; &lt;p&gt;Consider what happens immediately after the shift matrix is applied: we perform an elementwise multiplication between the shifted state and the query vector \(Q\). This is the same as the linear attention mechanism, where we perform an elementwise multiplication between the query and the key. Now, the corresponding elements being multiplied come from different time steps. Again, that’s exactly what we want.&lt;/p&gt; &lt;p&gt;The authors mention that we can let \(B\) be fixed or make it a trainable parameter. In the code, we stick with a fixed value for today. Specifically, the see tht B is set to the matrix consisting of all zeros except for a 1 in the first column. This has the effect of multiplying the first element of the shifted state by the first element of the query, the second element of the shifted state by the second element of the query, and so on. Because this process is ever-repeating, this “shift by one” operation will in fact set up a linear relationship between each token and its entire history.&lt;/p&gt; &lt;p&gt;Let’s see the code:&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Shift&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;The shift state space layer. Args: d_model: The number of heads. d_state: The dimension of the state matrix. The shift layer is a special case of an SSM layer with a fixed A matrix that allows tokens to mix with past tokens. For d_state = 4, the A matrix would be: A = [[0, 0, 0, 0], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]] &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_state&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[...,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Input and output shape (B, H, L)&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Construct kernel &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B_fc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;rfft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;conj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C_f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;rfft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;irfft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B_fc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C_f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;rearrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[...,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;b h l -&amp;gt; b l h&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Perform convolution by kernel &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;rfft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u_f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;rfft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;irfft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u_f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Truncate to original length and add skip connection &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[...,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Nothing too interesting here after the last few layer definitions. Lots of convolutions being carried out in the frequency domain, etc.&lt;/p&gt; &lt;p&gt;Well, that’s it for building our model! We’ve covered the basics of H3 and state space models, and we’ve seen how to implement them in PyTorch. Now let’s see how our model does on some toy language tasks from the H3 paper.&lt;/p&gt; &lt;h2 id=&quot;language-tasks&quot;&gt;Language Tasks&lt;/h2&gt; &lt;p&gt;Dao et al introduce two toy language tasks near the beginning of the H3 paper to demonstrate its distinct capabilities compared to more general SSMs. For these tasks, the authors considered toy-scale models of the H3 family, the S4D family, a simple Transformer, and another SSM variant called the gated state space.&lt;/p&gt; &lt;p&gt;First, a base class, then we’ll introduce the fun stuff. Our LanguageTask class is quite simple. It has a few methods that we’ll use to encode tokens into tensors, and decode them back into sequences of tokens. We’ll also define a method to return an embedding layer for the task’s vocabulary,&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;LanguageTask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Base class for language tasks. Each task should implement the following methods: __init__: Initialize the task. generate_sequence: Generate a sequence of tokens for the task. &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generate_sequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;This is a method must be implemented by each task.&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;NotImplementedError&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_embedding_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;embedding_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Return an embedding layer for the task vocabulary.&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alphabet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;embedding_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;encode_sequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Convert a sequence of tokens to a tensor of indices.&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;decode_sequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Convert a tensor of indices back into a sequence of corresponding tokens.&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h3 id=&quot;induction-head--copying-task&quot;&gt;Induction head / copying task&lt;/h3&gt; &lt;p&gt;The name “induction head” comes from recent literature in &lt;em&gt;mechanistic interpretability&lt;/em&gt;, or the study of how neural networks learn to detect patterns and even execute algorithms via logical and arithmetic circuits. &lt;d-cite key=&quot;induction_heads&quot;&gt;&lt;/d-cite&gt;. Induction heads are formally defined in () as those heads which exhibit the following two properties on a repeated random sequence of tokens:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Prefix matching: The head attends back to previous tokens that were followed by the current and/or recent tokens.&lt;/li&gt; &lt;li&gt;Copying: More attention to a token increases the probability that the head will output that token next.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;H3 is motivated by the fact that the other SSMs in H3’s family do not exhibit these properties, despite their ability to learn to attend to complex patters over long sequences.&lt;/p&gt; &lt;p&gt;This is clear when we look at the original results.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/synthtasks1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/synthtasks1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/synthtasks1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/synthtasks1.png&quot; class=&quot;img&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/synthtasks2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/synthtasks2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/synthtasks2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-10-hippo-to-h3/synthtasks2.png&quot; class=&quot;img&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;The toy attention model performs perfectly on both tasks. H3 does perfectly on one task and right under the Attention model on the other. The other SSMs in H3’s family do not perform well at all. They particularly struggle with the copying task, which matches the intuition that they are not able to “log tokens after particular events”.&lt;/p&gt; &lt;p&gt;Alright, on to the tasks. First, the induction head task.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;InductionHeadTask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LanguageTask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Toy language task to test for &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;copying&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; capabilities. The task is to learn to repeat the token that is shown after the special token &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;. We use an alphabet of 19 standard letters, plus the special token. The model is trained on sequences of 30 tokens. In each sequence, the pair (&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; + letter) is shown twice, and all other tokens are sampled randomly with replacement from the set of standard letters. Example sequences: &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;a _ b c a c _ b&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;b _ a c b c _ a&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;induction_head&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq_length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;abcdefghijklmnopqrs&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;special&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alphabet&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;special&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alphabet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inv_vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generate_sequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;answer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;special&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq_length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;permutation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;answer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;answer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# convert to list of chars &lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;For “Induction head”, we task the model with learning to treat a token as a “copying” token. This task has an alphabet of 19 normal characters &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&apos;abc..s&apos;&lt;/code&gt; and one special character &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_&lt;/code&gt;. The model is trained on sequences of 30 tokens. In each sequence, the pair &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(&apos;_&apos; + letter)&lt;/code&gt; is shown twice, and all other tokens are sampled at random.&lt;/p&gt; &lt;p&gt;To put it differently, in each sequence, we will see the special character “_” twice. The first time, it will be followed by a random letter. The second time, it will be followed by that same letter. The model is tasked with learning to repeat the letter shown earlier in the sequence.&lt;/p&gt; &lt;p&gt;For example, the sequence &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&apos;a _ b c a c _ b&apos;&lt;/code&gt; is a valid sequence for this task. The model should learn to repeat the letter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&apos;b&apos;&lt;/code&gt; after the second &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_&lt;/code&gt; token. This task sounds simple but it does require sophisticated circuits within the model to learn.&lt;/p&gt; &lt;h3 id=&quot;associative-memory-task&quot;&gt;Associative memory task&lt;/h3&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;AssociativeMemoryTask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LanguageTask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Toy language task for associative memory. The task is to learn to associate a key with a value at inference time. The model is trained on sequences of key-value pairs, where the keys and values are shuffled in each sequence. Each key-value pair is shown twice in each sequence, and accuracy is measured only by the final value in the sequence. Example sequences: &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;a 1 b 2 c 3 b 2 c 3 a 1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;b 3 c 1 c 1 a 2 a 2 b 3&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; Although the example above features 3 keys and values, this task is hard-coded with an alphabet of 10 keys and 10 values, as in the original paper. &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;associative_memory&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;abcdefghij&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;0123456789&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alphabet&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vals&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inv_vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generate_sequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shuffled_vals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;permutation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pairs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shuffled_vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pairs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;permutation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pairs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pair&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pairs&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;The second task tests “Associative memory”. The model is trained on sequences of key-value pairs, where the keys and values are shuffled in each sequence. Each key-value pair is shown twice in each sequence, and accuracy is measured only by the final value in the sequence.&lt;/p&gt; &lt;p&gt;For example, the sequence &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&apos;a 1 b 2 c 3 b 2 c 3 a 1&apos;&lt;/code&gt; is a valid sequence for this task. The model should learn to associate the key &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&apos;a&apos;&lt;/code&gt; with the value &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&apos;1&apos;&lt;/code&gt; and the key &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&apos;b&apos;&lt;/code&gt; with the value &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&apos;2&apos;&lt;/code&gt;. The model should then be able to correctly predict the value &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&apos;1&apos;&lt;/code&gt; when shown the key &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&apos;a&apos;&lt;/code&gt; at inference time.&lt;/p&gt; &lt;h2 id=&quot;wrapping-up&quot;&gt;Wrapping up&lt;/h2&gt; &lt;p&gt;We’ve just seen how the introduction of a shift SSM made the H3 model more capable at reasoning with text-based tasks and cognitive operations like copying and associative memory. There is still so much to explore in the H3 paper and related works, but I hope this post has given you a taste of the power and expressivity of state space models.&lt;/p&gt; &lt;p&gt;If you’re interested in learning more about state space models, I highly recommend the following resources in addition to cited papers and references.&lt;/p&gt; &lt;p&gt;The codebase for this blog post: The official H3 codebase: https://github.com/hazyresearch/h3 The official S4 codebase: https://github.com/hazyresearch/s4&lt;/p&gt; </content> </entry> <entry> <title>Building Blocks of Text-to-Video Generation</title> <link href="https://jocelynshen.com/blog/2022/text2vid/"/> <updated>2022-12-01T00:00:00+00:00</updated> <id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/text2vid</id> <content type="html">&lt;h1 id=&quot;googles-imagen-video-and-metas-make-a-video-explained&quot;&gt;Google’s Imagen Video and Meta’s Make-a-Video Explained&lt;/h1&gt; &lt;p&gt; &lt;/p&gt; &lt;callout&gt; Google and Meta have both developed advanced AI networks that can generate new, unseen videos using only simple text prompts. Try clicking through the prompts and compare the results between Google&apos;s Imagen Video and Meta&apos;s Make-a-Video models: &lt;/callout&gt; &lt;figure1&gt; &lt;iframe height=&quot;600px&quot; width=&quot;840px&quot; scrolling=&quot;No&quot; frameborder=&quot;0&quot; hspace=&quot;0&quot; vspace=&quot;0&quot; src=&quot;https://video-gui.onrender.com/&quot;&gt;&lt;/iframe&gt; &lt;/figure1&gt; &lt;p style=&quot;text-align: justify&quot;&gt;In this post, we dissect and explain the mechanics behind the key building blocks for state-of-the-art Text-to-Video generation. We provide interactive examples of these building blocks and demonstrate the key novelties/differences between two Text-to-Video models: Imagen Video and Make-a-Video. Finally, we summarize by showing how the building blocks fit together into a complete Text-to-Video framework as well as noting the current failure modes and limitations of the models today.&lt;/p&gt; &lt;h1 id=&quot;history-of-text-to-video&quot;&gt;History of Text-to-Video&lt;/h1&gt; &lt;p style=&quot;text-align: justify&quot;&gt;Just six months after the release of DALL-E 2, both Meta and Google released novel Text-to-Video generation models that output impressive video-format content. These networks build off of recent advancements in Text-to-Image modeling using stable diffusion (like DALL-E &lt;a href=&quot;https://arxiv.org/pdf/2102.12092&quot;&gt;[1]&lt;/a&gt; and Imagen &lt;a href=&quot;https://arxiv.org/pdf/2205.11487&quot;&gt;[2]&lt;/a&gt;). Meta’s Make-A-Video &lt;a href=&quot;https://arxiv.org/pdf/2209.14792&quot;&gt;[3]&lt;/a&gt; is capable of five second 768x768 clips at variable frame rates while Google’s Imagen Video &lt;a href=&quot;https://arxiv.org/pdf/2210.02303&quot;&gt;[4]&lt;/a&gt; can produce 1280×768 videos at 24 fps. Rather than training strictly on text-video pair datasets, both Imagen Video and Make-a-Video leverage the massive text-image pair databases to construct video from pretrained Text-to-Image generation models. These Text-to-Video generators are capable of creating high-resolution, photorealistic and stylistic content of impossible scenarios. Networks such as these can be powerful tools for artists and creators as well as the basis for predicting future frames of a video.&lt;/p&gt; &lt;p style=&quot;text-align: justify&quot;&gt;Video generation has progressed rapidly in the past decade. Early video generation models focused on simple, specific domains and next frame prediction with &lt;strong&gt;deterministic autoregressive&lt;/strong&gt; methods (CDNA &lt;a href=&quot;https://proceedings.neurips.cc/paper/2016/file/d9d4f495e875a2e075a1a4a6e1b9770f-Paper.pdf&quot;&gt;[5]&lt;/a&gt;, PredRNN &lt;a href=&quot;https://papers.nips.cc/paper/2017/file/e5f6ad6ce374177eef023bf5d0c018b6-Paper.pdf&quot;&gt;[6]&lt;/a&gt;). Later video prediction models incorporated stochasticity (SV2P &lt;a href=&quot;https://openreview.net/pdf?id=rk49Mg-CW&quot;&gt;[7]&lt;/a&gt;). Another line of work uses generative models, namely &lt;strong&gt;GANs&lt;/strong&gt;, to synthesize complex scenes without a first frame (VGAN &lt;a href=&quot;https://arxiv.org/pdf/1611.01799.pdf&quot;&gt;[8]&lt;/a&gt;, TGAN &lt;a href=&quot;https://arxiv.org/pdf/1611.06624&quot;&gt;[9]&lt;/a&gt;). More recently, Text-to-Video has been approached with &lt;strong&gt;VQVAEs&lt;/strong&gt; to learn latent representations of video frames and then &lt;strong&gt;autoregressive transformers&lt;/strong&gt; to generate video samples (GODIVA &lt;a href=&quot;https://arxiv.org/pdf/2104.14806&quot;&gt;[10]&lt;/a&gt;, NUWA &lt;a href=&quot;https://arxiv.org/pdf/2111.12417&quot;&gt;[11]&lt;/a&gt;). This technique allows for open-domain video generation, but frames are still generated one at a time chronologically, resulting in potentially poor text-video alignment. CogVideo &lt;a href=&quot;https://arxiv.org/pdf/2205.15868&quot;&gt;[12]&lt;/a&gt; adjusts the training procedure to fix alignment (discussed below) and uses pre-trained Text-to-Image weights. Make-A-Video and Imagen Video both use &lt;strong&gt;diffusion models&lt;/strong&gt; (VDM &lt;a href=&quot;https://openreview.net/pdf?id=2LdBqxc1Yv&quot;&gt;[13]&lt;/a&gt;), which we will discuss in the next section.&lt;/p&gt; &lt;p style=&quot;text-align: justify&quot;&gt;Make-A-Video and Imagen Video have come out just six months after Open-AI’s DALL-E 2. Text-to-Video is a much harder problem than Text-to-Image because we don’t have access to as many labeled text-video pairs. Therefore, all the models we highlight take advantage of starting from an existing Text-to-Image model with pre-trained or frozen weights. Moreover, beyond just generating pixels, the network has to predict how they will all evolve over time to coherently complete any actions in the text prompt.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/timeline1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/timeline1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/timeline1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/timeline1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 1. Timeline of video generation and prediction techniques. The modern-day Text-to-Video generators each leverage the capabilities of massively pre-trained Text-to-Image networks, such as Imagen and DALL-E 2. &lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p style=&quot;text-align: justify&quot;&gt;We’ll break down the building blocks to make Text-to-Video generation possible, starting from a brief overview of how Text-to-Image generators use stable diffusion, how to make the components 3D to incorporate temporal information for video generation, and how to increase the spatial and temporal resolution. We focus on how these components make up Make-A-Video and Imagen Video, but also touch on CogVideo (an open-source Text-to-Image video generator that uses a VQVAE + autoregressive transformers architecture).&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/T2V1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/T2V1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/T2V1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/T2V1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 2. A simplified foundational network diagram for Text-to-Video generation that leverages pretrained Text-to-Image encodings. These encodings are trained to be decoded into image &quot;batches&quot;, e.g., videos, that get upsampled spatially and temporally to have higher framerates and higher resolution. Scroll to Figure 9 to see the building blocks in more detail and how they fit together. &lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;h1 id=&quot;text-to-image-generation&quot;&gt;Text-to-Image Generation&lt;/h1&gt; &lt;h2 id=&quot;what-is-latent-space&quot;&gt;What is latent space?&lt;/h2&gt; &lt;p style=&quot;text-align: justify&quot;&gt;Text-to-Image generation uses stable diffusion in latent space and a 2D U-Net architecture for image generation (see link for more details). First let’s explain how &lt;strong&gt;auto-encoders&lt;/strong&gt; work:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/autoencoder1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/autoencoder1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/autoencoder1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/autoencoder1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 3. Demonstration of an autoencoder network. Images get compressed into lower-dimensional embeddings that are later decoded and &quot;reconstructed.&quot; Training this autoencoder network learns weights for the decoder, such that when randomly sampled embeddings from Z are passed through the decoder, D, brand new images unseen by the network are generated. &lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p style=&quot;text-align: justify&quot;&gt;Here an input image is encoded into a lower-dimensional latent space representation and a decoder can reconstruct the image. This network is trained on the &lt;a href=&quot;https://cs.nyu.edu/~roweis/data.html&quot;&gt;Frey Faces Dataset&lt;/a&gt; by comparing the input to the reconstructed output. Sampling within the latent space distribution allows us to generate realistic outputs.&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;callout&gt; Try it out yourself! We’ve trained an autoencoder with a two-dimensional latent space embedding. The two latents happen to correspond to expression and pose (the two variables changed in the dataset). Slowly Drag your cursor through latent space to change the values of the latent variable, Z, and see how the reconstructed image from the decoder changes (dragging too quickly will buffer image generation): &lt;/callout&gt; &lt;figure&gt; &lt;iframe height=&quot;420px&quot; width=&quot;720px&quot; scrolling=&quot;No&quot; frameborder=&quot;0&quot; hspace=&quot;0&quot; vspace=&quot;0&quot; src=&quot;https://vae-gui.onrender.com/&quot;&gt;&lt;/iframe&gt; &lt;/figure&gt; &lt;p style=&quot;text-align: justify&quot;&gt;For Text-to-Video generation, the encoding-decoding network is trained using &lt;strong&gt;stable diffusion&lt;/strong&gt; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf&quot;&gt;[15]&lt;/a&gt;.&lt;/p&gt; &lt;h2 id=&quot;how-does-stable-diffusion-work-in-latent-space&quot;&gt;How does stable diffusion work in latent space?&lt;/h2&gt; &lt;p style=&quot;text-align: justify&quot;&gt;During the forward process, we create a dataset by incrementally adding more noise to our latent variables. In the reverse process, we train a model with a &lt;strong&gt;U-Net architecture&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/1505.04597&quot;&gt;[16]&lt;/a&gt; to iteratively denoise these latents. This way, we can efficiently generate new images by starting with random noise and end up with a latent that can be decoded into a real image (while conditioning layers on the input text embedding).&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/latents1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/latents1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/latents1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/latents1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 4. Forward and reverse processes in stable diffusion. In the forward process, random noise is progressively added to the latents in order to create a training set. In the reverse process, the noisy latents are iteratively passed through a U-Net model that learns to denoise the latents. Thus, we can pass random noise into this model and generate a new relevant video with the model’s attention layers conditioned to the input. &lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p style=&quot;text-align: justify&quot;&gt;The U-Net architecture (which we use as a noise detector) is an &lt;strong&gt;autoencoder&lt;/strong&gt;. Downsampling and upsampling is done with convolutional layers. However, because the latent space is lower-dimensional, it’s possible to lose information, meaning that spatial recreation can be imprecise during upsampling. To deal with this, U-Net has &lt;strong&gt;skip connections&lt;/strong&gt; that transfer information across the network, bypassing the downsampling and compression.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/unet2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/unet2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/unet2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/unet2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 5. U-Net architecture consists of a convolutional encoder and decoder. Skip connections copy and crop information from downsampling. Attention layers at skip connections help by weighting relevant information. Referenced from U-Net [16]. &lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p style=&quot;text-align: justify&quot;&gt;However, the poor feature representation in the initial layers result in redundant information. To deal with this, we can add &lt;strong&gt;attention layers&lt;/strong&gt; at the skip connections to suppress activation in irrelevant regions, reducing the number of redundant features brought across by focusing “attention” only on the most important image features. For Text-to-Image generation, these attention networks also have access to the text embeddings to help condition the attention.&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;callout&gt; Try it out yourself! Click on different points in the first image and see how this would change the weights in the attention layer. The attention highlights similar features to the clicked region. For Text-to-Image and Text-to-Video generation, we include text embeddings to condition attention: &lt;/callout&gt; &lt;figure&gt; &lt;iframe height=&quot;400px&quot; width=&quot;600px&quot; scrolling=&quot;No&quot; frameborder=&quot;0&quot; hspace=&quot;0&quot; vspace=&quot;0&quot; src=&quot;https://attn-gui.onrender.com/&quot;&gt;&lt;/iframe&gt; &lt;/figure&gt; &lt;p&gt;In the next section we discuss how to modify our convolutional and attention layers to move from image &lt;em&gt;(2D spatial)&lt;/em&gt; representations to video &lt;em&gt;(3D)&lt;/em&gt; representations, composed of individual frames &lt;em&gt;(2D spatial)&lt;/em&gt; + time &lt;em&gt;(1D temporal)&lt;/em&gt;.&lt;/p&gt; &lt;h1 id=&quot;text-to-video-generation&quot;&gt;Text-to-Video Generation&lt;/h1&gt; &lt;h2 id=&quot;how-do-we-extend-text-to-image-to-text-to-video&quot;&gt;How do we extend Text-to-Image to Text-to-Video?&lt;/h2&gt; &lt;p style=&quot;text-align: justify&quot;&gt;Text-to-Image generation uses U-Net architecture with 2D spatial convolution and attention layers. For video generation, we need to add a third temporal dimension to the two spatial ones. 3D convolution layers are computationally expensive and 3D attention layers are computationally intractable. Therefore, these papers have their own approaches.&lt;/p&gt; &lt;p style=&quot;text-align: justify&quot;&gt;Make-A-Video creates pseudo 3D convolution and attention layers by stacking a 1D temporal layer over a 2D spatial layer. Imagen Video does spatial convolution and attention for each individual frame, then does temporal attention or convolution across all frames.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/attention1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/attention1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/attention1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/attention1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 6. 3D U-Net components stacking 1D temporal layers over 2D spatial layers. Make-A-Video has individual pseudo 3D convolutional and attention layers. Imagen video first does spatial processing on each individual frame and then has a temporal attention layer across frames. The spatial layers can all be pre-trained from Text-to-Image models. &lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Separating the spatial and temporal operations allows for &lt;strong&gt;building off of existing Text-to-Image models.&lt;/strong&gt;&lt;/p&gt; &lt;ul style=&quot;text-align: justify&quot;&gt; &lt;li&gt;&lt;strong&gt;CogVideo&lt;/strong&gt; freezes all the weights of the spatial layers&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Make-A-Video&lt;/strong&gt; uses pretrained weights for the spatial layers but initializes the temporal layer weights to the identity matrix. This way they can continue tuning all weights with new video data&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Imagen Video&lt;/strong&gt; can jointly train their model with video and image data, doing the latter by masking the temporal connections&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;spatial-and-temporal-super-resolution&quot;&gt;Spatial and Temporal Super Resolution&lt;/h2&gt; &lt;p style=&quot;text-align: justify&quot;&gt;The base video decoder creates a fixed number of frames (5 frames for CogVideo, 16 frames for Make-A-Video, and 15 frames for Imagen Video) that need to be upsampled temporally and spatially.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/super_resolution1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/super_resolution1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/super_resolution1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/super_resolution1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 7. Text-to-Video architectures of Imagen Video (top) and Make-A-Video (bottom). Light blue boxes represent temporal upsampling steps and darker blue boxes are for spatial upsampling. &lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p style=&quot;text-align: justify&quot;&gt;Make-A-Video uses &lt;strong&gt;frame rate conditioning&lt;/strong&gt;, meaning they have an additional input that determines the fps in the generated video (unlike how Imagen Video has a fixed frame rate in each stage). During training, this is useful as a form of data augmentation due to the limited dataset of videos. CogVideo also highlights the importance of changing the frame rate in order to retime videos such that an entire action can be encompassed in a fixed video length. For example the action “drinking” is composed of the sub-actions “pick up glass,” “drink,” and “place glass” which need to be performed in that order. If training on videos of a fixed length, changing the frame rate can help ensure text-video alignment.&lt;/p&gt; &lt;p style=&quot;text-align: justify&quot;&gt;&lt;strong&gt;Frame interpolation&lt;/strong&gt; for Make-A-Video is done in an autoregressive manner. They fine-tune a spatio-temporal decoder by masking certain frames of a training video and learning to predict them. They train with variable frame-skips and fps conditioning to enable different temporal upsampling rates. The framework is also able to interpolate and extrapolate (extend the beginning or end of a video).&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/masking2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/masking2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/masking2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/masking2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 8. Frame masking for frame interpolation (middle) and extrapolation (bottom). For interpolation, variable frames are skipped. For extrapolation, frames at the beginning or end of the video can be masked. &lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p style=&quot;text-align: justify&quot;&gt;Imagen Video’s approach relies on &lt;strong&gt;cascaded video diffusion models&lt;/strong&gt;. They generate entire blocks of frames simultaneously for each network to avoid the artifacts that would result from running super-resolution on independent frames. Each of the 6 super-resolution sub-models after the base video diffusion model, shown in &lt;em&gt;Figure 7 (top)&lt;/em&gt;, focuses on either temporal or spatial upsampling. While the base model (the video decoder at the lowest frame rate/resolution) uses a temporal attention layer to model long-term temporal dependencies, the super-resolution models only use temporal convolution layers for computational efficiency while still maintaining local temporal consistency. Similarly, spatial attention is only used in the base and first two spatial super-resolution models, while the rest only use convolution.&lt;/p&gt; &lt;p style=&quot;text-align: justify&quot;&gt;Make-A-Video’s approach initially interpolates frames and then increases the spatial resolution with two super-resolution layers, shown in &lt;em&gt;Figure 7 (bottom)&lt;/em&gt;. The first super-resolution layer operates across spatial and temporal dimensions. The second super-resolution layer only operates across the spatial dimension because of memory and space constraints. However, spatial upsampling requires &lt;strong&gt;detail hallucination&lt;/strong&gt; which needs to be consistent across frames (hence the use of the temporal dimension in the previous layer). To deal with this, they use the same noise initialization for each frame to encourage consistent detail hallucination across frames.&lt;/p&gt; &lt;callout&gt; Here we show a low-resolution video upsampled using bilinear interpolation (left) and the same video upsampled using a super-resolution neural networks applied to each individual frame (middle). We see flickering artifacts because upsampling was performed separately per frame, rather than hallucinating detail across frames. The difference map (right) highlights the differences between the left and middle videos to demonstrate the flickering effect that occurs in a video when upsampled without proper detail hallucination to maintain temporal coherency. The video generation models we discuss actively consider temporal coherency while hallucinating details during upsampling to avoid these artifacts &lt;/callout&gt; &lt;center&gt; &lt;video autoplay=&quot;&quot; muted=&quot;&quot; loop=&quot;&quot; controls=&quot;&quot; src=&quot;https://jocelynshen.com/deep-learning-blog.github.io/assets/video/2023-02-01-text2vid/artifacts_stack1.mp4&quot; style=&quot;width:500px&quot; type=&quot;video/mp4&quot;&gt; &lt;/video&gt; &lt;figcaption&gt;(left) Video frames upsampled using bilinear interpolation. (middle) Video frames upsampled using a super-resolution neural network. (right) Difference map between each frame of the left and middle videos, showing flickering artifacts.&lt;/figcaption&gt; &lt;/center&gt; &lt;p&gt; &lt;/p&gt; &lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt; &lt;h2 id=&quot;putting-together-all-the-building-blocks&quot;&gt;Putting Together All The Building Blocks&lt;/h2&gt; &lt;p&gt;In this post, we have described the foundational building blocks of Text-to-Video generation of two popular models, Google’s Imagen Video and Meta’s Make-a-Video. Although these two methods have various differences, they build off of similar theory and similar building blocks. In &lt;em&gt;Figure 9&lt;/em&gt;, we visually demonstrate how each of the consituent building blocks discussed in this post fit together to construct the larger Text-to-Video model.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/T2V_building_blocks3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/T2V_building_blocks3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/T2V_building_blocks3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2023-02-01-text2vid/T2V_building_blocks3.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 9. The building blocks of Text-to-Video generation. &lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;The foundational building blocks of &lt;em&gt;Figure 9&lt;/em&gt; and their utility in Text-to-Video generation are summarized:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Latent Decoder:&lt;/strong&gt; An autoencoder encoding-decoding network demonstrates how data can be compressed into a low-dimensional latent representation and then by selecting different points in this latent space, new outputs are generated.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Latent Diffusion:&lt;/strong&gt; Training an encoding-decoding network with the stable diffusion process allows completely new images to be generated from latents with added noise. In a Text-to-Video model, image “batches” are generated from a single noisy latent put through the trained decoder to create a new, unseen video at low FPS and low resolution.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;2D U-Net:&lt;/strong&gt; To retain import information during the latent space compression encoding-decoding process, skip connections are added to tether the encoder with the decoder. This is a data-efficient architecture called a U-Net that is used in the encoding-decoding process of Text-to-Image.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;3D U-Net:&lt;/strong&gt; The 3D U-Net is an extension of the Text-to-Image 2D U-Net for Text-to-Video generation. Since it is computationally expensive to expand 2D convolution and attention directly into 3D, hence, psuedo 3D convolution and attention are constructed by concatenating 2D spatial and 1D temporal layers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Attention:&lt;/strong&gt; Attention layers help determine which spatial and temporal features to pay attention to, according to the input text conditioning. In a Text-to-Video model, attention helps create realistic videos by connecting important features in each frame and across frames with less required information transfer than a standard fully-connected layer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Super-Resolution:&lt;/strong&gt; Upsampling in both the spatial and temporal dimensions increases the video resolution and frame rate, respectively. In Text-to-Video, upsampling is done across image batches simultaneously or with other considerations to ensure temporal consistency.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;By combining these six building blocks together, a &lt;strong&gt;complete Text-to-Video generation model&lt;/strong&gt; can be constructed. Google and Meta demonstrate technically unique yet methodically similar approaches for expanding 2D Text-to-Image generation into the 3D realm while significantly improving the resolution, framerate, and temporal coherency of videos generated from text-based prompts.&lt;/p&gt; &lt;h2 id=&quot;limitations-of-text-to-video&quot;&gt;Limitations of Text-to-Video&lt;/h2&gt; &lt;p style=&quot;text-align: justify&quot;&gt;As beautiful as many of these videos are . . .&lt;/p&gt; &lt;figure&gt; &lt;video autoplay=&quot;&quot; muted=&quot;&quot; loop=&quot;&quot; controls=&quot;&quot; src=&quot;https://imagen.research.google/video/hdvideos/51.mp4&quot; width=&quot;600&quot; type=&quot;video/mp4&quot;&gt; &lt;/video&gt; &lt;/figure&gt; &lt;p&gt; &lt;/p&gt; &lt;p style=&quot;text-align: justify&quot;&gt;Not all of them are perfect . . . &lt;em&gt;(pay close attention to the legs of the elephant walking)&lt;/em&gt;&lt;/p&gt; &lt;figure&gt; &lt;video autoplay=&quot;&quot; muted=&quot;&quot; loop=&quot;&quot; controls=&quot;&quot; src=&quot;https://imagen.research.google/video/hdvideos/14.mp4&quot; width=&quot;600&quot; type=&quot;video/mp4&quot;&gt; &lt;/video&gt; &lt;/figure&gt; &lt;p&gt;Although Imagen Video and Make-a-Video have made significant progress in temporal coherency to remove flickering effects, complex videos generated where image data is sparse, have poor realism across the temporal dimension. In the elephant walking underwater example, a lack of training data of elephants walking or perhaps training sets with insufficient frame rates results in latent diffusion having to work harder to interpolate the missing frames, resulting in &lt;strong&gt;poor temporal realism&lt;/strong&gt;. However, as both datasets and models continue to grow in size, the videos generated by the methods discussed in this post will improve in realism and these failure modes will become less common.&lt;/p&gt; &lt;p&gt;Furthermore, both models are optimized for producing shorter (5-second) videos. Since Make-A-Video directly builds on Text-to-Image, it cannot learn associations that can only be learned from videos. Longer videos containing multiple scenes and actions are challenging to generate with both of these models.&lt;/p&gt; &lt;p style=&quot;text-align: justify&quot;&gt;Undoubtedly, these Text-to-Video generation methods can substantially expand the creative toolbox available to artists and creators, however, key issues should be addressed before these networks become publicly available. For example, misuse of the models can result in fake, explicit, hateful, or otherwise generally &lt;strong&gt;harmful content&lt;/strong&gt;. To help address this, additional classifiers can be trained to filter text inputs and video outputs. Moreover, the outputs reflect the composition of the training dataset, which include some problematic data, social biases, and stereotypes.&lt;/p&gt; &lt;h1 id=&quot;related-works&quot;&gt;Related Works&lt;/h1&gt; &lt;p style=&quot;text-align: justify&quot;&gt;Several advancements have been achieved with the methods described in this post, however, video generation is not a new concept, nor do the methods described in this post solve all video generation challenges. So, here is a selection of some other interesting video generation variations/applications developed by other researchers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://phenaki.video/&quot;&gt;Phenaki&lt;/a&gt; is another video generation tool that can generate videos of several minutes in length from story-like text prompts, compared to 5 second videos generated by Imagen Video and Make-a-Video.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://kuai-lab.github.io/eccv2022sound/&quot;&gt;Lee &lt;em&gt;et al.&lt;/em&gt;&lt;/a&gt; and &lt;a href=&quot;https://medhini.github.io/audio_video_textures/&quot;&gt;Narashimhan &lt;em&gt;et al.&lt;/em&gt;&lt;/a&gt; generated video synced with audio inputs.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://sites.google.com/view/visualforesight?pli=1&quot;&gt;Visual Foresight&lt;/a&gt; predicts how an object will move given an action in pixel space for more practical robotics planning and control applications.&lt;/li&gt; &lt;/ul&gt; &lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt; &lt;p style=&quot;font-size: smaller&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2102.12092&quot;&gt;[1] Ramesh, A. et al. Zero-Shot Text-to-Image Generation, 2021. &lt;em&gt;arXiv Preprint&lt;/em&gt;.&lt;/a&gt;&lt;/p&gt; &lt;p style=&quot;font-size: smaller&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2205.11487&quot;&gt;[2] Saharia, C. et al. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding, 2022. &lt;em&gt;arXiv Preprint&lt;/em&gt;.&lt;/a&gt;&lt;/p&gt; &lt;p style=&quot;font-size: smaller&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2209.14792&quot;&gt;[3] Singer, U. et al. Make-a-Video: Text-to-Video Generation Without Text-Video Data, 2022. &lt;em&gt;arXiv Preprint&lt;/em&gt;.&lt;/a&gt;&lt;/p&gt; &lt;p style=&quot;font-size: smaller&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2210.02303&quot;&gt;[4] Ho, J. et al. Imagen Video: High Definition Video Generation with Diffusion Models, 2022. &lt;em&gt;arXiv Preprint&lt;/em&gt;.&lt;/a&gt;&lt;/p&gt; &lt;p style=&quot;font-size: smaller&quot;&gt;&lt;a href=&quot;https://proceedings.neurips.cc/paper/2016/file/d9d4f495e875a2e075a1a4a6e1b9770f-Paper.pdf&quot;&gt;[5] Finn, C. et al. Unsupervised Learning for Physical Interaction through Video Prediction, 2016. &lt;em&gt;30th Conference on Neural Information Processing Systems (NeurIPS)&lt;/em&gt;.&lt;/a&gt;&lt;/p&gt; &lt;p style=&quot;font-size: smaller&quot;&gt;&lt;a href=&quot;https://papers.nips.cc/paper/2017/file/e5f6ad6ce374177eef023bf5d0c018b6-Paper.pdf&quot;&gt;[6] Wang, Y. et al. PredRNN: Recurrent Neural Networks for Predictive Learning using Spatiotemporal LSTMs, 2017. &lt;em&gt;30th Conference on Neural Information Processing Systems (NeurIPS)&lt;/em&gt;.&lt;/a&gt;&lt;/p&gt; &lt;p style=&quot;font-size: smaller&quot;&gt;&lt;a href=&quot;https://openreview.net/pdf?id=rk49Mg-CW&quot;&gt;[7] Babaeizadeh, M. et al. Stochastic Variational Video Prediction, 2018. &lt;em&gt;International Conference on Learning Representations (ICLR)&lt;/em&gt;.&lt;/a&gt;&lt;/p&gt; &lt;p style=&quot;font-size: smaller&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1611.01799.pdf&quot;&gt;[8] Zhai, S. et al. Generative Adversarial Networks as Variational Training of Energy Based Models, 2017. &lt;em&gt;arXiv Preprint&lt;/em&gt;.&lt;/a&gt;&lt;/p&gt; &lt;p style=&quot;font-size: smaller&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1611.06624&quot;&gt;[9] Saito, M. et al. Temporal Generative Adversarial Nets with Singular Value Clipping, 2016. &lt;em&gt;arXiv Preprint&lt;/em&gt;.&lt;/a&gt;&lt;/p&gt; &lt;p style=&quot;font-size: smaller&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2104.14806&quot;&gt;[10] Wu, C. et al. GODIVA: Generating Open-DomaIn Videos from nAtural Descriptions, 2021. &lt;em&gt;arXiv Preprint&lt;/em&gt;.&lt;/a&gt;&lt;/p&gt; &lt;p style=&quot;font-size: smaller&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2111.12417&quot;&gt;[11] Wu, C. et al. NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion, 2021. &lt;em&gt;arXiv Preprint&lt;/em&gt;.&lt;/a&gt;&lt;/p&gt; &lt;p style=&quot;font-size: smaller&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2205.15868&quot;&gt;[12] Hong, W. et al. CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers, 2022. &lt;em&gt;arXiv Preprint&lt;/em&gt;.&lt;/a&gt;&lt;/p&gt; &lt;p style=&quot;font-size: smaller&quot;&gt;&lt;a href=&quot;https://openreview.net/pdf?id=2LdBqxc1Yv&quot;&gt;[13] Kingma, D. P. et al. Variational Diffusion Models, 2021. &lt;em&gt;35th Conference on Neural Information Processing Systems (NeurIPS)&lt;/em&gt;.&lt;/a&gt;&lt;/p&gt; &lt;p style=&quot;font-size: smaller&quot;&gt;&lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/file/a4d92e2cd541fca87e4620aba658316d-Paper.pdf&quot;&gt;[14] Ding, M. et al. CogView: Mastering Text-to-Image Generation via Transformers, 2021. &lt;em&gt;35th Conference on Neural Information Processing Systems (NeurIPS)&lt;/em&gt;.&lt;/a&gt;&lt;/p&gt; &lt;p style=&quot;font-size: smaller&quot;&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf&quot;&gt;[15] Rombach, R. et al. High-Resolution Image Synthesis with Latent Diffusion Models, 2022. &lt;em&gt;IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)&lt;/em&gt;.&lt;/a&gt;&lt;/p&gt; &lt;p style=&quot;font-size: smaller&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1505.04597&quot;&gt;[16] Ronneberger, O. et al. U-Net: Convolutional Networks for Biomedical Image Segmentation, 2015. &lt;em&gt;arXiv Preprint&lt;/em&gt;.&lt;/a&gt;&lt;/p&gt; </content> </entry> <entry> <title>Universality of Neural Networks on Sets vs. Graphs</title> <link href="https://jocelynshen.com/blog/2022/sets-and-graphs/"/> <updated>2022-12-01T00:00:00+00:00</updated> <id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/sets-and-graphs</id> <content type="html">&lt;h2 id=&quot;sets-and-graphs&quot;&gt;Sets and Graphs&lt;/h2&gt; &lt;p&gt;Before we dive into&lt;d-footnote&gt;It is important to briefly focus on declaring the *conflict of interest* we had while writing this blog. We are actively working on set and graph representation learning. Accordingly, several paragraphs of this writeup focus on papers that we have co-written. That being said, and in the context of ICLR, we declare that the majority of the ICLR papers referenced in this blog post do _not_ present a conflict of interest for us. Hence, we believe we have, to the best of our efforts, provided an objective and impartial view of learning universal representations over graphs and sets.&lt;/d-footnote&gt; universal function approximation, let’s start with the basics. What do we mean by learning on set- graphs-based data? In both cases, we assume no ordering, i.e. the task is permutation invariant (or equivariant). A graph is typically thought of as a set of nodes with edges between the nodes. A set doesn’t have edges, it just has the nodes, although we often don’t call them nodes, rather set elements. Both the nodes and the edges can have feature vectors attached to them. The figure below (originally from Wagstaff et al. 2021&lt;d-cite key=&quot;wagstaff21&quot;&gt;&lt;/d-cite&gt;) visualises this relationship:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_graphsandsets-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_graphsandsets-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_graphsandsets-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_graphsandsets.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Examples for machine learning tasks on this type of data include 3D point cloud classification (a function mapping a set of coordinates to an object class) and molecular property prediction (a function mapping a molecular graph to, e.g., a free energy value).&lt;/p&gt; &lt;h2 id=&quot;why-do-we-care-about-universal-function-approximation&quot;&gt;Why do we care about universal function approximation?&lt;/h2&gt; &lt;p&gt;First of all, why do we need to be able to approximate all functions? After all, having &lt;em&gt;one&lt;/em&gt; function that performs well on the train set and generalises to the test set is all we need in most cases. Well, the issue is that we have no idea what such a function looks like, otherwise we would implement it directly and wouldn’t need to train a neural network. Hence, the network not being a universal function approximator &lt;em&gt;may&lt;/em&gt; hurt its performance. &lt;!-- So the logic is, we don&apos;t want to restrict the network unless the restrictions only refer to functions we know that we don&apos;t care about. --&gt;&lt;/p&gt; &lt;p&gt;Graph Isomorphism Networks (GINs) by Xu et al.&lt;d-cite key=&quot;GIN&quot;&gt;&lt;/d-cite&gt;) provide the quintessential example for the merit of universality research: the authors analysed Graph Convolutional Networks (a very popular class of graph neural networks by Kipf et al. 2016&lt;d-cite key=&quot;GCN&quot;&gt;&lt;/d-cite&gt;), pointed out that GCNs are not universal, created a varation of the algorithm that &lt;em&gt;is&lt;/em&gt; universal (or at least closer to), and achieved better results. &lt;!-- So, in this case, the non-universality of the GCNs really did hurt their performance. --&gt;&lt;/p&gt; &lt;p&gt;However, this is not always the case. Sometimes, architecture changes motivated by universal function approximation arguments lead to &lt;em&gt;worse&lt;/em&gt; results. Even in such unfortunate cases, however, we argue that thinking about universality is no waste of time. Firstly, it brings structure into the literature and into the wide range of models available. We need to group approaches together to see the similarities and differences. Universality research can and has served as a helpful tool for that.&lt;/p&gt; &lt;p&gt;Moreover, proving that a certain architecture is or is not universal is an inherently interesting task and teaches us mathematical thinking and argumentation. In a deep learning world, where there is a general sense of randomness and magic in building high-performing neural networks and where it’s hard to interpret what’s going on, one might argue that an additional mathematical analysis is probably good for the balance, even if it turns out to not always directly result in better performance.&lt;/p&gt; &lt;h2 id=&quot;learning-on-sets--universality&quot;&gt;Learning on Sets &amp;amp; Universality&lt;/h2&gt; &lt;p&gt;To prove universal function approximation&lt;d-footnote&gt;*Approximation* is actually not precisely what we will discuss in the rest of this text. Rather, we will consider universal function *representation*. That&apos;s also what we mean by phrases like *is able to learn* (even though that would also depend on the optimiser you are using and the amount of data you have and where you put the layernorms and so on...). The difference between approximation and representation is discussed in detail in Wagstaff et al. 2021 &lt;d-cite key=&quot;wagstaff21&quot;&gt;&lt;/d-cite&gt;. Interestingly, the findings for universal function representation largely also hold for universal function approximation.&lt;/d-footnote&gt;, we typically make two assumptions: 1) the MLP components of the neural networks are infinitely large. 2) the functions that we want to be able to learn are continuous on $\mathbb{R}$.&lt;/p&gt; &lt;p&gt;The first part says: any concrete implementation of a ‘universal’ network architecture might not be able to learn the function of interest, but, if you make it &lt;a href=&quot;https://i.redd.it/n9fgba8b0qr01.png&quot;&gt;bigger&lt;/a&gt;, eventually it will—and that is &lt;em&gt;guaranteed&lt;/em&gt;&lt;d-footnote&gt;Conversely, if the network is provably non-universal (like Graph Convolutional Networks), then there are functions it can *never* learn, no matter how many layers you stack.&lt;/d-footnote&gt;. The second part is a non-intuitive mathematical technicality we will leave uncommented for now and get back to later (because it’s actually a really interesting and important techinicality).&lt;/p&gt; &lt;p&gt;One of the seminal papers discussing both permutation invariant neural networks and universal function approximation was DeepSets by Zaheer et al. in 2017&lt;d-cite key=&quot;Zaheer2017&quot;&gt;&lt;/d-cite&gt;. The idea is simple: apply the same neural network $\phi$ to several inputs, sum up their results, and apply a final neural network $\rho$.&lt;d-footnote&gt;Figure from Wagstaff et al. 2021.&lt;/d-footnote&gt;&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_deepsets-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_deepsets-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_deepsets-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_deepsets.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Because the sum operation is permutation invariant, the final output is invariant with respect to the ordering of the inptus. In other words, the sum quite obviously restricts the space of learnable functions to permutation invariant ones. The question is, can a neural network with this architecture, in principle, learn &lt;em&gt;all&lt;/em&gt; (continuous) permutation invariant functions. Perhaps surprisingly, the authors show that all functions can indeed be represented with this architecture. The idea is a form of binary bit-encoding in the latent space. Concretely, they argue that there is a bijective mapping from rational to natural numbers. Assuming that each input is a rational number, they first map each rational number $x$ to a natural number $c(x)$, and then each natural number to $\phi(x) = 4^{-c(x)}$. It is now easy to see that $\sum_i \phi(x_i) \neq \sum_i \phi(y_i)$ unless the finite sets $ \{ x_0, x_1, … \} $ and $\{y_0, y_1, …\}$ are the same. Now that we uniquely encoded each input, a universal decoder can map this to any output we want. This concludes the proof that the DeepSets architecture is, in theory, a universal function approximator, despite its simplicity.&lt;/p&gt; &lt;p&gt;However, there is an issue with this proof: it builds on the assumption that the MLP components themselves are universal function approximators, in the limit of infinite width. However, the universal function approximation theorem says that this is the case only for continuous functions, where continuity is defined on the real numbers. That conitnuity is important is sort of intuitive: continuity means that a small change in the input implies a small change in the output. And because the building blocks of neural networks (specifically linear combinations and non-linearities) are continuous, it makes sense that the overall function we want the network to learn should be continuous.&lt;/p&gt; &lt;p&gt;But why continuity on the real numbers? Because continuity on the rational numbers is not a very useful property as shown in Wagstaff et al. 2019&lt;d-cite key=&quot;wagstaff19&quot;&gt;&lt;/d-cite&gt;. The mapping we described above is clearly highly discontinuous, and anyone could attest that it is completely unrealistic to assume that a neural network could learn such a metric. That doesn’t mean all is lost. Wagstaff et al. show that the DeepSets architecture is still a universal function approximator when requiring continuity, but only if the latent space (the range of $\phi$) has a dimensionality at least as large as the number of inputs, which is an important restriction.&lt;/p&gt; &lt;p&gt;What about more complicated architectures? Murphy et al.&lt;d-cite key=&quot;Janossy&quot;&gt;&lt;/d-cite&gt; generalise the idea of Deep Sets to applying networks to all possible $k$-tuples of inputs, where $k=1$ recovers the Deep Sets case. This can be seen as unifying other architecture classes such as self-attention. However, this is not known to alleviate the constraint on the latent space mentioned above, as explained in Wagstaff et al. 2021&lt;d-cite key=&quot;wagstaff21&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h2 id=&quot;what-about-graph-representation-learning&quot;&gt;What about &lt;em&gt;graph&lt;/em&gt; representation learning?&lt;/h2&gt; &lt;p&gt;So, this was universality in the context of machine learning on sets, but what about graphs? Interestingly, the graph representation learning community experienced a near-identical journey, evolving entirely in parallel! Perhaps this observation comes as little surprise: to meaningfully propagate information in a graph neural network (GNN), a local, permutation invariant operation is commonplace.&lt;/p&gt; &lt;p&gt;Specifically, a GNN typically operates by computing representations (&lt;em&gt;“messages”&lt;/em&gt;) sent from each node to its neighbours, followed by an &lt;em&gt;aggregation function&lt;/em&gt; which, for every node, combines all of its incoming messages in a way that is &lt;em&gt;invariant to permutations&lt;/em&gt;. Opinions are still divided on whether &lt;em&gt;every&lt;/em&gt; permutation equivariant GNN can be expressed with such pairwise messaging, with a recent position paper by Velickovic&lt;d-cite key=&quot;Velickovic22&quot;&gt;&lt;/d-cite&gt; claiming they &lt;strong&gt;can&lt;/strong&gt;. Regardless of which way the debate goes in the future, aggregating messages over 1-hop neighbours gives rise to a highly elegant implementation of GNNs which is likely here to stay. This comes with very solid community backing, with &lt;a href=&quot;https://www.pyg.org/&quot;&gt;PyG&lt;/a&gt;—one of the most popular GNN frameworks—&lt;a href=&quot;https://github.com/pyg-team/pytorch_geometric/releases/tag/2.1.0&quot;&gt;recently making aggregators a “first-class citizen”&lt;/a&gt; in their GNN pipelining.&lt;/p&gt; &lt;p&gt;Therefore, to build a GNN, it suffices to build a &lt;em&gt;permutation-invariant, local&lt;/em&gt; layer which combines data coming from each node’s neighbours. This feels nearly identical to our previous discussion; what’s changed, really? Well, we need to take care of one seemingly minor detail: it is possible for &lt;strong&gt;two or more neighbours to send &lt;em&gt;exactly the same message&lt;/em&gt;&lt;/strong&gt;. The theoretical framework of Deep Sets and/or Wagstaff et al. wouldn’t entirely suffice in this case, as they assumed a &lt;em&gt;set&lt;/em&gt; input, whereas now we have a &lt;em&gt;multiset&lt;/em&gt;.&lt;/p&gt; &lt;h2 id=&quot;learning-on-graphs--universality&quot;&gt;Learning on Graphs &amp;amp; Universality&lt;/h2&gt; &lt;p&gt;Several influential GNN papers were able to overcome this limitation. The first key development came from the &lt;em&gt;graph isomorphism network&lt;/em&gt; (&lt;strong&gt;GIN&lt;/strong&gt;)&lt;d-cite key=&quot;GIN&quot;&gt;&lt;/d-cite&gt;. GIN is an elegant example of how, over countable features, the maximally-powerful GNN can be built up using similar ideas as in Deep Sets; so long as the local layer we use is &lt;em&gt;injective&lt;/em&gt; over multisets. Similarly to before, we must choose our encoder $\phi$ and aggregator $\bigoplus$, such that $\bigoplus\limits_i \phi(x_i) \neq \bigoplus\limits_i \phi(y_i)$ unless the finite &lt;em&gt;multisets&lt;/em&gt; $\{ \mkern-4mu \{x_0, x_1, …\} \mkern-4mu \}$ and $\{\mkern-4mu\{y_0, y_1, …\} \mkern-4mu \}$ are the same ($x_i, y_i\in\mathbb{Q}$).&lt;/p&gt; &lt;p&gt;In the multiset case, the framework from Deep Sets induces an additional constraint over $\bigoplus$—it needs to preserve the &lt;em&gt;cardinality&lt;/em&gt; information about the repeated elements in a multiset. This immediately implies that some choices of $\bigoplus$, such as $\max$ or averaging, will not yield maximally powerful GNNs.&lt;/p&gt; &lt;p&gt;For example, consider the multisets $\{\mkern-4mu\{1, 1, 2, 2\} \mkern-4mu \}$ and $\{\mkern-4mu\{1, 2\}\mkern-4mu\}$. As we assume the features to be countable, we specify the numbers as &lt;em&gt;one-hot&lt;/em&gt; integers; that is, $1 = [1\ \ 0]$ and $2=[0\ \ 1]$. The maximum of these features, taken over the multiset, is $[1\ \ 1]$, and their average is $\left[\frac{1}{2}\ \ \frac{1}{2}\right]$. This is the case for both of these multisets, meaning that both maximising and averaging are &lt;em&gt;incapable&lt;/em&gt; of telling them apart.&lt;/p&gt; &lt;p&gt;Summations $\left(\bigoplus=\sum\right)$, however, are an example of a suitable injective operator.&lt;/p&gt; &lt;p&gt;Very similarly to the analysis from Wagstaff et al. in the domain of sets, a similar extension in the domain of graphs came through the work on &lt;a href=&quot;**PNA**&quot;&gt;&lt;em&gt;principal neighbourhood aggregation&lt;/em&gt;&lt;/a&gt; by Corso, Cavalleri et al.&lt;d-cite key=&quot;Corso&quot;&gt;&lt;/d-cite&gt;. We already discussed why it is a good idea to focus on features coming from $\mathbb{R}$ rather than $\mathbb{Q}$—the universal approximation theorem is only supported over continuous functions. However, it turns out that, when we let $x_i, y_i\in\mathbb{R}$, it is easily possible to construct neighbourhood multisets for which setting $\bigoplus=\sum$ would &lt;strong&gt;not&lt;/strong&gt; preserve injectivity:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_examples-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_examples-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_examples-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_examples.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;In fact, PNA itself is based on a proof that it is &lt;em&gt;impossible&lt;/em&gt; to build an injective function over multisets with real-valued features using &lt;em&gt;any&lt;/em&gt; &lt;strong&gt;single&lt;/strong&gt; aggregator. In general, for an injective functon over $n$ neighbours, we need &lt;em&gt;at least&lt;/em&gt; $n$ aggregation functions (applied in parallel). PNA then builds an empirically powerful aggregator combination, leveraging this insight while trying to preserve numerical stability.&lt;/p&gt; &lt;p&gt;Note that there is an apparent &lt;strong&gt;similarity&lt;/strong&gt; between the results of Wagstaff et al. and the analysis of PNA. Wagstaff et al. shows that, over real-valued sets of $n$ elements, it is necessary to have an encoder representation &lt;em&gt;width&lt;/em&gt; at least $n$. Corso, Cavalleri et al. showed that, over real-valued multisets of $n$ elements, it is necessary to aggregate them with at least $n$ aggregators. It appears that potent processing of real-valued collections &lt;em&gt;necessitates&lt;/em&gt; representational capacity proportional to the collection’s size, in order to guarantee injectivity. Discovering this correspondence is what brought the two of us together to publish this blog post in the first place, but we do not offer any in-depth analysis of this correspondence here. We do hope it inspires future connections between these two fields, however!&lt;/p&gt; &lt;p&gt;We have established what is necessary to create a maximally-powerful GNN over both &lt;em&gt;countable&lt;/em&gt; and &lt;em&gt;uncountable&lt;/em&gt; input features. So, &lt;em&gt;how powerful are they&lt;/em&gt;, exactly?&lt;/p&gt; &lt;h2 id=&quot;the-weisfeiler-lehman-test&quot;&gt;The Weisfeiler-Lehman Test&lt;/h2&gt; &lt;p&gt;While GNNs are often a powerful tool for processing graph data in the real world, they also won’t solve &lt;em&gt;all&lt;/em&gt; tasks specified on a graph accurately! As a simple counterexample, consider any NP-hard problem, such as the Travelling Salesperson Problem. If we had a fixed-depth GNN that perfectly solves such a problem, we would have shown P=NP! Expectedly, not all GNNs will be equally good at solving various problems, and we may be highly interested in characterising their &lt;em&gt;expressive power&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;The canonical example for characterising expressive power is &lt;em&gt;deciding graph isomorphism&lt;/em&gt;; that is, can our GNN distinguish two non-isomorphic graphs? Specifically, if our GNN is capable of computing graph-level representations \(\mathbf{h}_{\mathcal{G}}\), we are interested whether \(\mathbf{h}_{\mathcal{G_{1}}} \neq\mathbf{h}_{\mathcal{G_{2}}}\) for non-isomorphic graphs \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\). If we cannot attach different representations to these two graphs, any kind of task requiring us to classify them differently is &lt;em&gt;hopeless&lt;/em&gt;! This motivates assessing the power of GNNs by which graphs they are able to &lt;em&gt;distinguish&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;A typical way in which this is formalised is by using the &lt;em&gt;Weisfeiler-Lehman&lt;/em&gt; (&lt;strong&gt;WL&lt;/strong&gt;) graph isomorphism test. To formalise this, we will study a popular algorithm for approximately deciding graph isomorphism.&lt;/p&gt; &lt;p&gt;The WL algorithm featurises a graph $\mathcal{G}=(\mathcal{V},\mathcal{E})$ as follows. First, we set the representation of each node $i\in\mathcal{V}$ as $x_i^{(0)} = 1$. Then, it proceeds as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Let $\mathcal{X}_i^{(t+1)} = \{\mkern-4mu\{x_j^{(t)} :(i,j)\in\mathcal{E}\}\mkern-4mu\}$ be the multiset of features of all neighbours of $i$.&lt;/li&gt; &lt;li&gt;Then, let \(x_i^{(t+1)}=\sum\limits_{y_j\in\mathcal{X}_i^{(t+1)}}\phi(y_j)\), where \(\phi : \mathbb{Q}\rightarrow\mathbb{Q}\) is an &lt;em&gt;injective&lt;/em&gt; hash function.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This process continues as long as the &lt;em&gt;histogram&lt;/em&gt; of $x_i^{(t)}$ changes—initially, all nodes have the same representation. As steps 1–2 are iterated, certain $x_i^{(t)}$ values may become different. Finally, the WL test checks whether two graphs are (possibly) isomorphic by checking whether their histograms have the same (sorted) shape upon convergence.&lt;/p&gt; &lt;p&gt;While remarkably simple, the WL test can accurately distinguish most graphs of real-world interest. It does have some rather painful failure modes, though; for example, it cannot distinguish a 6-cycle from two triangles!&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_wlfail-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_wlfail-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_wlfail-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_wlfail.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;This is because, locally, &lt;em&gt;all nodes look the same&lt;/em&gt; in these two graphs, and the histogram never changes.&lt;/p&gt; &lt;p&gt;The key behind the power of the WL test is the &lt;em&gt;injectivity&lt;/em&gt; of the hash function $\phi$—it may be interpreted as assigning each node a different &lt;em&gt;colour&lt;/em&gt; if it has a different &lt;em&gt;local context&lt;/em&gt;. Similarly, we saw that GNNs are maximally powerful when their propagation models are &lt;em&gt;injective&lt;/em&gt;. It should come as little surprise then that, in terms of distinguishing graph structures over &lt;em&gt;countable&lt;/em&gt; input features, GNNs can &lt;strong&gt;never be more powerful than the WL test&lt;/strong&gt;! And, in fact, this level of power is achieved &lt;em&gt;exactly&lt;/em&gt; when the aggregator is injective. This fact was first discovered by Morris et al.&lt;d-cite key=&quot;Morris&quot;&gt;&lt;/d-cite&gt;, and reinterpreted from the perspective of multiset aggregation by the GIN paper.&lt;/p&gt; &lt;p&gt;While the WL connection has certainly spurred a vast amount of works on improving GNN expressivity, it is also worth recalling the initial assumption: $x_i^{(0)} = 1$. That is, we assume that the input node features are &lt;em&gt;completely uninformative&lt;/em&gt;! Very often, this is not a good idea! It can be proven that even placing &lt;em&gt;random numbers&lt;/em&gt; in the nodes can yield to a provable improvement in expressive power (Sato et al.&lt;d-cite key=&quot;Sato&quot;&gt;&lt;/d-cite&gt;). Further, many recent works (Loukas et al.&lt;d-cite key=&quot;Loukas&quot;&gt;&lt;/d-cite&gt;); Kanatsoulis and Ribeiro&lt;d-cite key=&quot;Ribeiro&quot;&gt;&lt;/d-cite&gt; make it very explicit that, if we allow GNNs access to “appropriate” input features, this leads to a vast improvement in their expressive power.&lt;/p&gt; &lt;p&gt;Even beyond the limitation of the uninformative input features, recent influential works (published at ICLR’22 and ‘23 as orals) have demonstrated that the WL framework itself is worth extending. Geerts and Reutter&lt;d-cite key=&quot;Geerts&quot;&gt;&lt;/d-cite&gt; demonstrate clear theoretical value to expressing GNN computations using a &lt;em&gt;tensor language&lt;/em&gt; (TL), allowing for drawing significant connections to &lt;em&gt;color refinement&lt;/em&gt; algorithms. And Zhang et al.&lt;d-cite key=&quot;Zhang&quot;&gt;&lt;/d-cite&gt; demonstrate that the WL framework may be &lt;em&gt;weak&lt;/em&gt; in terms of its architectural distinguishing power, showing that many higher-order GNNs that surpass the limitations of the 1-WL test, are in fact still incapable of computing many standard polynomial-time-computable metrics over graphs, such as ones relating to the graph’s &lt;em&gt;biconnected components&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;Lastly, linking back to our central discussion, we argue that focussing the theoretical analysis only on discrete features may not lead to highly learnable target mappings. From the perspective of the WL test (and basically any discrete-valued procedure), the models presented in Deep Sets and PNA are no more powerful than 1-WL. However, moving into continuous feature support, PNA is indeed more powerful at distinguishing graphs than models like GIN.&lt;/p&gt; </content> </entry> <entry> <title>Thinking Like Transformers</title> <link href="https://jocelynshen.com/blog/2022/raspy/"/> <updated>2022-12-01T00:00:00+00:00</updated> <id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/raspy</id> <content type="html">&lt;link rel=&quot;stylesheet&quot; href=&quot;custom.css&quot; /&gt; &lt;h1 id=&quot;thinking-like-transformers&quot;&gt;Thinking Like Transformers&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2106.06981.pdf&quot;&gt;Paper&lt;/a&gt; by Gail Weiss, Yoav Goldberg, Eran Yahav&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Transformer models are foundational to AI systems. There are now countless explanations of “how transformers work?” in the sense of the architecture diagram at the heart of transformers.&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_5_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;However this diagram does not provide any intuition into the computational model of this framework. As researchers become interested in how Transformers work, gaining intuition into their mechanisms becomes increasingly useful.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2106.06981.pdf&quot;&gt;Thinking like Transformers&lt;/a&gt; proposes a computational framework for Transformer-like calculations. The framework uses discrete computation to simulate Transformer computations. The resulting language &lt;a href=&quot;https://github.com/tech-srl/RASP&quot;&gt;RASP&lt;/a&gt; is a programming language where, ideally, every program can compile down to a specific Transformer (indeed, David Lindner and colleagues have recently released a &lt;a href=&quot;https://arxiv.org/abs/2301.05062&quot;&gt;compiler&lt;/a&gt; for a large subset of RASP!).&amp;lt;/p&amp;gt;&lt;/p&gt; &lt;p&gt;In this blog post, I reimplemented a variant of RASP in Python (RASPy). The language is roughly compatible with the original version, but with some syntactic changes that I thought were fun. With this language, we have a challenging set of puzzles to walk through and understand how it works.&lt;/p&gt; &lt;p&gt;Before jumping into the language itself, let’s look at an example of what coding with Transformers looks like. Here is some code that computes the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flip&lt;/code&gt;, i.e. reversing an input sequence. The code itself uses two Transformer layers to apply attention and mathematical computations to achieve the result.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;flip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flip&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flip&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;flip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_11_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;h2 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Part 1: &lt;a href=&quot;#transformers-as-code&quot;&gt;Transformers as Code&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 2: &lt;a href=&quot;#coding-with-transformers&quot;&gt;Coding with Transformers&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;transformers-as-code&quot;&gt;Transformers as Code&lt;/h2&gt; &lt;p&gt;Our goal is to define a computational formalism that mimics the expressivity of Transformers. We will go through this process by analogy, describing each language construct next to the aspect of the Transformer it represents. (See the full &lt;a href=&quot;https://arxiv.org/pdf/2106.06981.pdf&quot;&gt;paper&lt;/a&gt; for the formal language specification).&lt;/p&gt; &lt;p&gt;The core unit of the language is a &lt;em&gt;sequence operation&lt;/em&gt; that transforms a sequence to another sequence of the same length. I will refer to these throughout as &lt;em&gt;transforms&lt;/em&gt;.&lt;/p&gt; &lt;h3 id=&quot;inputs&quot;&gt;Inputs&lt;/h3&gt; &lt;p&gt;In a Transformer, the base layer is the input fed to the model. This input usually contains the raw tokens as well as positional information.&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_15_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;In code, the symbol &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tokens&lt;/code&gt; represents the simplest transform. It returns the tokens passed to the model. The default input is the sequence “hello”.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_17_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;If we want to change the input to the transform, we use the input method to pass in an alternative.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_19_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;As with Transformers, we cannot access the positions of these sequences directly. However, to mimic position embeddings, we have access to a sequence of indices.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_21_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sop&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;goodbye&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_22_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;h3 id=&quot;feed-forward-network&quot;&gt;Feed Forward Network&lt;/h3&gt; &lt;p&gt;After the input layer, we reach the feed-forward network. In a Transformer, this stage can apply mathematical operations to each element of the sequence independently.&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_24_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;In code, we represent this stage by computation on transforms. Mathematical operations are overloaded to represent independent computation on each element of the sequence .&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_26_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;The result is a new transform. Once constructed it can be applied to new input.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_28_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;Operations can combine multiple transforms. For example, functions of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tokens&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;indices&lt;/code&gt;. The analogy here is that the Transformer activations can keep track of multiple pieces of information simultaneously.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_30_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_31_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;We provide a few helper functions to make it easier to write transforms. For example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;where&lt;/code&gt; provides an “if” statement like construct&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_33_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;And &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;map&lt;/code&gt; lets us define our own operators, for instance a string to int transform. (Users should be careful to only use operations here that could be computed with a simple neural network).&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;atoi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;ord&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;ord&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;atoi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;31234&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_35_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;When chaining these transforms, it is often easier to work with functions. For example the following applies where and then &lt;code&gt;atoi&lt;/code&gt; and then adds 2.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;atoi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;ord&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;ord&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;atoi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;02-13&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_37_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;From here on, unless we use a different input sequence, we will assume that the input is ‘hello’ and omit the input display in the illustrations.&lt;/p&gt; &lt;h3 id=&quot;attention-selectors&quot;&gt;Attention Selectors&lt;/h3&gt; &lt;p&gt;Things get more interesting when we start to apply attention. This allows routing of information between the different elements of the sequence.&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_39_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;We begin by defining notation for the keys and queries of the model. Keys and queries are effectively transforms that we will broadcast and compare to each other to create &lt;em&gt;selectors&lt;/em&gt;, our parallel to attention patterns. We create them directly from transforms. For example, if we want to define a key, we call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key&lt;/code&gt; on a transform.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_41_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;Similarly for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query&lt;/code&gt;. (Queries are presented as columns to reflect their relation to the selectors we will create from them.)&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_43_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;Scalars can be used as keys or queries. They broadcast out to the length of the underlying sequence.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_45_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;By applying a comparison operation between a key and a query we create a &lt;em&gt;selector&lt;/em&gt;, our parallel to an attention matrix - though this one is unweighted.&lt;/p&gt; &lt;p&gt;A selector is a binary matrix indicating which input position (column) each output position (row) will attend to in an eventual attention computation. In the comparison creating it, the key values describe the input (column) positions, and the query values describe the output (row) positions.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;eq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eq&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_47_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;Some examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A selector that matches each output position to the previous input position.&lt;/li&gt; &lt;/ul&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_49_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A selector that matches each output position to all earlier input positions.&lt;/li&gt; &lt;/ul&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;before&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;before&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_51_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A selector that matches each output position to all later input positions.&lt;/li&gt; &lt;/ul&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;after&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;after&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_53_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;Selectors can be merged using boolean operations. For example, this selector focuses each output position on 1) earlier positions that 2) contain the same original input token as its own. We show this by including both pairs of keys and queries in the matrix.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;before&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eq&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_55_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;h2 id=&quot;using-attention&quot;&gt;Using Attention&lt;/h2&gt; &lt;p&gt;Given an attention selector we can provide a value sequence to aggregate. We represent aggregation by &lt;strong&gt;summing&lt;/strong&gt; up over the values that have a true value for their selector.&lt;/p&gt; &lt;p&gt;(Note: in the original paper, they use a &lt;strong&gt;mean&lt;/strong&gt; aggregation and show a clever construction where mean aggregation is able to represent a sum calculation. RASPy uses sum by default for simplicity and to avoid fractions. In practicce this means that RASPy may underestimate the number of layers needed to convert to a mean based model by a factor of 2.)&lt;/p&gt; &lt;p&gt;Attention aggregation gives us the ability to compute functions like histograms.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_59_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;Visually we follow the architecture diagram. Queries are to the left, Keys at the top, Values at the bottom, and the Output is to the right.&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_61_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;Some attention operations may not even use the input tokens. For instance to compute the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;length&lt;/code&gt; of a sequence, we create a “select all” attention selector and then add 1 from each position.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_63_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;Here’s a more complex example, shown step-by-step. (This is the kind of thing they ask in interviews!)&lt;/p&gt; &lt;p&gt;Say we want to compute the sum of neighboring values in a sequence, along a sliding window. First we apply the forward cutoff, attending only to positions that are not too far in the past.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;WINDOW&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WINDOW&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s1&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_65_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;Then the backward cutoff, attending only to positions up to and including our own.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;s2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s2&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_67_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;Intersect.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sel&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_69_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;And finally aggregate.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sum2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sum2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_71_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;Here is a simple example that produces a 2-layer transform. The first corresponds to computing length and the second the cumulative sum. The cumulative sum has to go into a second layer because it is applied to a transform which uses length, and so it can only be computed after the computation of length is complete.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cumsum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;before&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cumsum&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cumsum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_73_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;h2 id=&quot;layers&quot;&gt;Layers&lt;/h2&gt; &lt;p&gt;The language supports building up more complex transforms. It keeps track of the &lt;em&gt;layers&lt;/em&gt; by tracking the operations computed so far.&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_76_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;Here is a simple example that produces a 2-layer transform. The first corresponds to computing length and the second the cumulative sum.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cumsum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_78_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;h2 id=&quot;coding-with-transformers&quot;&gt;Coding with Transformers&lt;/h2&gt; &lt;p&gt;Given this library of functions, we can write operations to accomplish surprisingly complex tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Can we produce a Transformer that does basic addition of two arbitrary length numbers?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;i.e. given a string “19492+23919” can we produce the correct output?&lt;/p&gt; &lt;p&gt;We will go through these steps, and their solutions, here. If you would rather do them on your own, we provide a version where you can try them yourself!&lt;/p&gt; &lt;p&gt;Before we dive in to the main task, we will do some challenges of increasing difficulty to help us build some intuitions.&lt;/p&gt; &lt;h3 id=&quot;challenge-1-select-a-given-index&quot;&gt;Challenge 1: Select a given index&lt;/h3&gt; &lt;p&gt;Produce a sequence where all the elements have the value at index i.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_83_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;h3 id=&quot;challenge-2-shift&quot;&gt;Challenge 2: Shift&lt;/h3&gt; &lt;p&gt;Shift all of the tokens in a sequence to the right by i positions. (Here we introduce an optional parameter in the aggregation: the default value to be used when no input positions are selected. If not defined, this value is 0.)&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;shift&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;shift&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;shift&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_85_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;h3 id=&quot;challenge-3-minimum&quot;&gt;Challenge 3: Minimum&lt;/h3&gt; &lt;p&gt;Compute the minimum values of the sequence. (This one starts to get harder. Our version uses 2 layers of attention.)&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;minimum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sel1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;before&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sel2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;less&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sel1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sel2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;less&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;minimum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_87_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;p&gt;The idea behind our solution is an implicit full ordering of the input positions: we (implicitly) order the positions according to input token value, with input position as tie breaker. Our first act is to have each position attend to all positions before it in the ordering: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sel1&lt;/code&gt; focuses on earlier input positions with the same input token value, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sel2&lt;/code&gt; focuses on input positions with lower input token value. We then aggregate a 1 from all positions to get where each position is located in this ordering (i.e., how many other positions precede it). The minimum value is the input value at the first position according to this ordering (i.e., the one which had no other positions precede it).&lt;/p&gt; &lt;h3 id=&quot;challenge-4-first-index&quot;&gt;Challenge 4: First Index&lt;/h3&gt; &lt;p&gt;Compute the first index that has token q, assuming the sequence always has length shorter than 100. (2 layers)&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;minimum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;99&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_90_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;h3 id=&quot;challenge-5-right-align&quot;&gt;Challenge 5: Right Align&lt;/h3&gt; &lt;p&gt;Right align a padded sequence e.g. ralign().inputs(‘xyz___’) = ‘—xyz’” (2 layers)&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;ralign&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;ralign&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;ralign&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;xyz__&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_92_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;h3 id=&quot;challenge-6-split&quot;&gt;Challenge 6: Split&lt;/h3&gt; &lt;p&gt;Split a sequence into two parts at value v and then right align. You can assume there is exactly one appearance of v in the sequence. (3 layers to get and align the first part of the sequence, but only 1 for the second.)&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_first_part&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;split_point&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_first_part&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;ralign&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;split_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;split_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;xyz+zyr&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_94_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;xyz+zyr&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_95_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;h3 id=&quot;challenge-6-slide&quot;&gt;Challenge 6: Slide&lt;/h3&gt; &lt;p&gt;Replace special tokens “&amp;lt;” with the closest non “&amp;lt;” value to their right. (2 layers)&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;slide&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cumsum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;slide&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;slide&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;xxxh&amp;lt;&amp;lt;&amp;lt;l&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_97_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;h3 id=&quot;challenge-7-add&quot;&gt;Challenge 7: Add&lt;/h3&gt; &lt;p&gt;For this one you want to perform addition of two numbers. Here are the steps.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;683+345&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;ol&gt; &lt;li&gt;Split into parts (challenge 6). Convert to ints. Add.&lt;/li&gt; &lt;/ol&gt; &lt;blockquote&gt; &lt;p&gt;“683+345” =&amp;gt; [0, 0, 0, 9, 12, 8]&lt;/p&gt; &lt;/blockquote&gt; &lt;ol&gt; &lt;li&gt;Compute the carry terms. Three possibilities: definitely receives carry (“1”), definitely doesn’t receive carry (“0”), maybe receives carry (“&amp;lt;”).Because we are only adding two numbers, the only case in which a position might receive a carry is if the position after it sums to 9. In that case, it will receive a carry if and only if the position after &lt;em&gt;that&lt;/em&gt; receives a carry.&lt;/li&gt; &lt;/ol&gt; &lt;blockquote&gt; &lt;p&gt;[0, 0, 0, 9, 12, 8] =&amp;gt; “00&amp;lt;100”&lt;/p&gt; &lt;/blockquote&gt; &lt;ol&gt; &lt;li&gt;Slide the carry coefficients. A position that might receive a carry will get one if and only if the next position receives a carry - and so on down the chain until the next definite carry/no carry.&lt;/li&gt; &lt;/ol&gt; &lt;blockquote&gt; &lt;p&gt;“00&amp;lt;100” =&amp;gt; 001100”&lt;/p&gt; &lt;/blockquote&gt; &lt;ol&gt; &lt;li&gt;Complete the addition.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Each of these is 1 line of code. The full system is 6 layers. (if you are careful you can do it in 5!).&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 0) Parse and add &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;atoi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; \ &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;atoi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 1) Check for carries &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gets_carry&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;shift&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 2) Slide carries to their columns - all in one parallel go! &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gets_carry&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;atoi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;slide&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gets_carry&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gets_carry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 3) Add in carries, and remove overflow from original addition. &lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;return &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gets_carry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;683+345&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_99_0.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;mi&quot;&gt;683&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;345&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1028 &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Pretty neat stuff. If you are interested more in this topic, be sure to check at the paper:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2106.06981.pdf&quot;&gt;Thinking like Transformers&lt;/a&gt; and the &lt;a href=&quot;https://github.com/tech-srl/RASP&quot;&gt;RASP language&lt;/a&gt;.&lt;/p&gt; </content> </entry> <entry> <title>How much meta-learning is in image-to-image translation?</title> <link href="https://jocelynshen.com/blog/2022/how-much-meta-learning-is-in-image-to-image-translation/"/> <updated>2022-12-01T00:00:00+00:00</updated> <id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/how-much-meta-learning-is-in-image-to-image-translation</id> <content type="html">&lt;p&gt;At the last ICLR conference, Zhou et al. [2022] &lt;d-cite key=&quot;DBLP:conf/iclr/ZhouTRKPHF22&quot;&gt;&lt;/d-cite&gt; presented work showing that CNNs do not transfer information between classes of a classification task.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Allan Zhou, Fahim Tajwar, Alexander Robey, Tom Knowles, George J. Pappas, Hamed Hassani, Chelsea Finn [ICLR, 2022] Do Deep Networks Transfer Invariances Across Classes?&lt;d-cite key=&quot;DBLP:conf/iclr/ZhouTRKPHF22&quot;&gt;&lt;/d-cite&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here is a quick summary of their findings: If we train a Convolutional Neural Net (CNN) to classify fruit on a set of randomly brightened and darkened images of apples and oranges, it will learn to ignore the scene’s brightness. We say that the CNN learned that classification is &lt;strong&gt;invariant&lt;/strong&gt; to the &lt;strong&gt;nuisance transformation&lt;/strong&gt; of randomly changing the brightness of an image. We now add a set of plums to the training data, but fewer examples of them than we have apples and oranges. However, we keep using the same random transformations. The training set thus becomes &lt;strong&gt;class-imbalanced&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;We might expect a sophisticated learner to look at the entire dataset, recognize the random brightness modifications across all types of fruit and henceforth ignore brightness when making predictions. If this applied to our fruit experiment, the CNN would be similarly good at ignoring lighting variations on all types of fruit. Furthermore, we would expect the CNN to become more competent at ignoring lighting variations in proportion to &lt;strong&gt;the total amount of images&lt;/strong&gt;, irrespective of which fruit they depict.&lt;/p&gt; &lt;p&gt;Zhou et al. [2022] &lt;d-cite key=&quot;DBLP:conf/iclr/ZhouTRKPHF22&quot;&gt;&lt;/d-cite&gt; show that a CNN does not behave like this: When using a CNN on a &lt;strong&gt;class-imbalanced&lt;/strong&gt; classification task with random nuisance transformations, the CNNs invariance to the transformation is proportional to the size of the training set &lt;strong&gt;for each class&lt;/strong&gt;. This finding suggests CNNs don’t &lt;strong&gt;transfer invariance&lt;/strong&gt; between classes when learning such a classification task.&lt;/p&gt; &lt;p&gt;However, there is a solution: Zhou et al. [2022] &lt;d-cite key=&quot;DBLP:conf/iclr/ZhouTRKPHF22&quot;&gt;&lt;/d-cite&gt; use an Image to Image translation architecture called MUNIT&lt;d-cite key=&quot;DBLP:conf/eccv/HuangLBK18&quot;&gt;&lt;/d-cite&gt; to learn the transformations and generate additional data from which the CNN can learn the invariance separately for each class. Thus, the invariance to nuisance transformations is transferred &lt;strong&gt;generatively&lt;/strong&gt;. They call this method &lt;strong&gt;Generative Invariance Transfer (GIT)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;In this blog post, we are going to argue that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The experiment described above is a meta-learning experiment.&lt;/li&gt; &lt;li&gt;MUNIT is related to meta-learning methods.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Before we proceed to the main post, let’s clarify some definitions. If you are already familiar with the subject, you may skip this part:&lt;/p&gt; &lt;details&gt; &lt;summary&gt;&lt;b&gt; Definition: Class-Imbalanced Classification&lt;/b&gt;&lt;/summary&gt; &lt;br /&gt; &lt;p&gt; In many real-world classification datasets, the number of examples for each class varies. Class-imbalanced classification refers to classification on datasets where the frequencies of class labels vary significantly. &lt;/p&gt; &lt;p&gt; It is generally more difficult for a neural network to learn to classify classes with fewer examples. However, it is often important to perform well on all classes, regardless of their frequency in the dataset. If we train a model to classify a dataset of different skin tumors, most examples may be benign. Still, it is crucial to identify the rare, malignant ones. Experiment design, including training and evaluation methods must therefore be adjusted when using class-imbalanced data. &lt;/p&gt; &lt;br /&gt; &lt;/details&gt; &lt;details&gt; &lt;summary&gt;&lt;b&gt; Definition: Nuisance Transformation &amp;amp; Transformation Invariance&lt;/b&gt;&lt;/summary&gt; &lt;br /&gt; &lt;p&gt; Transformations are alterations of data. In the context of image classification, nuisance transformations are alterations that do not affect the class labels of the data. A model is said to be invariant to a nuisance transformation if it can successfully ignore the transformation when predicting a class label. &lt;/p&gt; We can formally define a nuisance transformation &lt;p&gt; $$T(\cdot |x)$$ &lt;/p&gt; &lt;p&gt; as a distribution over transformation functions. An example of a nuisance transformation might be a distribution over rotation matrices of different angles, or lighting transformations with different exposure values. By definition, nuisance transformations have no impact on class labels $y$, only on data $x$. A perfectly transformation-invariant classifier would thus completely ignore them, i.e., &lt;/p&gt; &lt;p&gt; $$ \hat{P}_w(y = j|x) = \hat{P}_w(y = j|x&apos;), \; x&apos; \sim T(\cdot |x). $$ &lt;/p&gt; &lt;/details&gt; &lt;h2 id=&quot;a-closer-look-at-the-experiment&quot;&gt;A closer look at the experiment&lt;/h2&gt; &lt;p&gt;Let’s take a more detailed look at the experiment Zhou et al. [2022] &lt;d-cite key=&quot;DBLP:conf/iclr/ZhouTRKPHF22&quot;&gt;&lt;/d-cite&gt; conducted:&lt;/p&gt; &lt;p&gt;Zhou et al. [2022] &lt;d-cite key=&quot;DBLP:conf/iclr/ZhouTRKPHF22&quot;&gt;&lt;/d-cite&gt; take a dataset, e.g., CIFAR-100, then apply a nuisance transformation, for example, random rotation, background intensity, or dilation and erosion. They then remove samples from some classes until the distribution of class sizes follows Zipf’s law with parameter 2.0 and a minimum class size of 5. The test set remains balanced, i.e., all test classes have the same number of samples. They then train a CNN model - for example, a ResNet - on this imbalanced and transformed training data.&lt;/p&gt; &lt;p&gt;To measure the invariance of the trained model to the applied transformation Zhou et al. [2022] &lt;d-cite key=&quot;DBLP:conf/iclr/ZhouTRKPHF22&quot;&gt;&lt;/d-cite&gt; use the empirical Kullback-Leibler divergence between the untransformed test set and the transformed test set of each class.&lt;/p&gt; &lt;p&gt; $$ eKLD(\hat{P}_w) = \mathbb{E}_{x \sim \mathbb{P}_{test}, x&apos; \sim T(\cdot|x)} [D_{KL}(\hat{P}_w(y = j|x) || \hat{P}_w(y = j|x&apos;))] $$ &lt;/p&gt; &lt;p&gt;If the learner is invariant to the transformation, the predicted probability distribution over class labels should be identical for the transformed and untransformed images. In that case, the KLD should be zero and greater than zero otherwise. The higher the expected KL-divergence, the more the applied transformation impacts the network’s predictions.&lt;/p&gt; &lt;p&gt;The result: eKLD falls with class size. This implies that the CNN does not learn that there are the same nuisance transformations on all images and therefore does not transfer this knowledge to the classes with less training data. A CNN learns invariance &lt;strong&gt;separately for each class&lt;/strong&gt;.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/EKLD.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/EKLD.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/EKLD.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/EKLD.svg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;how-is-this-a-meta-learning-experiment&quot;&gt;How is this a meta-learning experiment?&lt;/h2&gt; &lt;p&gt;You might think this is a cool experiment, but how is it related to meta-learning?&lt;/p&gt; &lt;p&gt;Let’s look at one of the original papers on meta-learning. In the 1998 book “Learning to learn” Sebastian Thrun &amp;amp; Lorien Pratt define an algorithm as capable of “Learning to learn” if it improves its performance in proportion to the number of tasks it is exposed to:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;an algorithm is said to learn to learn if its performance at each task improves with experience and with the number of tasks. Put differently, a learning algorithm whose performance does not depend on the number of learning tasks, which hence would not benefit from the presence of other learning tasks, is not said to learn to learn &lt;d-cite key=&quot;DBLP:books/sp/98/ThrunP98&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Now how does this apply to the experiment just outlined? In the introduction, we thought about how a sophisticated learner might handle a dataset like the one described in the last section. We said that a sophisticated learner would learn that the nuisance transformations are applied uniformly &lt;strong&gt;to all classes&lt;/strong&gt;. Therefore, if we added more classes to the dataset, the learner would become &lt;strong&gt;more invariant&lt;/strong&gt; to the transformations because we expose it to more examples of them. Since this is part of the classification task &lt;strong&gt;for each class&lt;/strong&gt;, the learner should, everything else being equal, become better at classification, especially on classes with few training examples. To see this, we must think of the multi-classification task not as a single task but as multiple mappings from image features to activations that must be learned, as a set of binary classification tasks. Thrun and Pratt continue:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;For an algorithm to fit this definition, some kind of &lt;em&gt;transfer&lt;/em&gt; must occur between multiple tasks that must have a positive impact on expected task-performance &lt;d-cite key=&quot;DBLP:books/sp/98/ThrunP98&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This transfer is what Zhou et al. [2022] &lt;d-cite key=&quot;DBLP:conf/iclr/ZhouTRKPHF22&quot;&gt;&lt;/d-cite&gt; tried to measure. There is some meta-information learnable across several tasks, in our case, the transformation distribution across many binary classification tasks. If a learner can learn this meta-information and transfer it to each new task it has “learned to learn”; it is a meta-learner. The goal of Zhou et al.’s &lt;d-cite key=&quot;DBLP:conf/iclr/ZhouTRKPHF22&quot;&gt;&lt;/d-cite&gt; experiment was to see whether this transfer takes place. Thus, arguably, it is a meta-learning experiment.&lt;/p&gt; &lt;h2 id=&quot;generative-invariance-transfer&quot;&gt;Generative Invariance Transfer&lt;/h2&gt; &lt;p&gt;Zhou et al. [2022] &lt;d-cite key=&quot;DBLP:conf/iclr/ZhouTRKPHF22&quot;&gt;&lt;/d-cite&gt; don’t stop there. They show that, using the MUNIT (Multimodal Unsupervised image-to-image Translation)&lt;d-cite key=&quot;DBLP:conf/eccv/HuangLBK18&quot;&gt;&lt;/d-cite&gt; architecture, they can learn the nuisance transformations applied to the dataset and generate additional training samples for the classes with few samples, improving transformation invariance there. They call this Generative invariance transfer (GIT). Let’s take a closer look:&lt;/p&gt; &lt;p&gt;MUNIT networks are capable of performing image-to-image translation, which means that they can translate an image from one domain, such as pictures of leopards, into another domain, such as pictures of house cats. The translated image should look like a real house cat while still resembling the original leopard image. For instance, if the leopard in the original image has its eyes closed, the translated image should contain a house cat with closed eyes. Eye state is a feature present in both domains, so a good translator should not alter it. On the other hand, a leopard’s fur is yellow and spotted, while a house cat’s fur can be white, black, grey, or brown. To make the translated images indistinguishable from real house cats, the translator must thus replace leopard fur with house cat fur.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_ENCODING.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_ENCODING.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_ENCODING.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_ENCODING.svg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;MUNIT networks learn to perform translations by correctly distinguishing the domain-agnostic features (such as eye state) from the domain-specific features (such as the distribution of fur color). They embed an image into two latent spaces: a content space that encodes the domain-agnostic features and a style space that encodes the domain-specific features (see figure above).&lt;/p&gt; &lt;p&gt;To transform a leopard into a house cat, we can encode the leopard into a content and a style code, discard the leopard-specific style code, randomly select a cat-specific style code, and assemble a house cat image that looks similar by combining the leopard’s content code with the randomly chosen cat style code (see figure below).&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_TRANSLATION.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_TRANSLATION.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_TRANSLATION.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_TRANSLATION.svg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Zhou et al. [2022] &lt;d-cite key=&quot;DBLP:conf/iclr/ZhouTRKPHF22&quot;&gt;&lt;/d-cite&gt; modify the process of using MUNIT to transfer images between domains. They do not use MUNIT to translate images &lt;strong&gt;between&lt;/strong&gt; domains but &lt;strong&gt;within&lt;/strong&gt; a domain. The MUNIT network exchanges the style code of an image with another style code of the same domain. For example, if the domain is house cats, the MUNIT network might translate a grey house cat into a black one. The learning task in this single-domain application of MUNIT is to decompose example-agnostic content features from example-specific style features so that the translated images still look like house cats. For example, fur color is a valid style feature for translating within the ‘house cat’ domain because every house cat has a fur color. A translator only switching fur color is hard to detect.&lt;/p&gt; &lt;p&gt;However, if the domain included house cats &lt;strong&gt;and apples&lt;/strong&gt;, fur color is not a valid style feature. If it was, the translator might translate fur color on an apple and give it black fur, which would look suspiciously out of place. Whatever house cats and apples have in common - maybe their position or size in the frame - would be a valid style feature. We would expect an intra-domain translator on an apples-and-cats dataset to change the position and size of an apple but not to turn it into a cat (not even partially).&lt;/p&gt; &lt;p&gt;It turns out that on a dataset with uniformly applied nuisance transformations, the nuisance transformations are valid style features: The result of randomly rotating an apple cannot be discerned as artificial when images of all classes, house cats and apples, were previously randomly rotated.&lt;/p&gt; &lt;p&gt;Zhou et al. [2022] &lt;d-cite key=&quot;DBLP:conf/iclr/ZhouTRKPHF22&quot;&gt;&lt;/d-cite&gt; find that when they train a MUNIT network on a dataset with nuisance transformations and class imbalances, the MUNIT network decomposes the class and transformation distributions. The style latent space of the MUNIT network approximates the transformation distribution $T(\cdot |x)$. The content space preserves the remaining features of the image, such as its class. Thus, when translating an image, i.e., exchanging its style code, MUNIT applies a random nuisance transformation while preserving content. Zhou et al. [2022] &lt;d-cite key=&quot;DBLP:conf/iclr/ZhouTRKPHF22&quot;&gt;&lt;/d-cite&gt; use this method to generate data for classes with few examples. While the CNN is still unable to transfer invariance to $T(\cdot |x)$ between classes, it can now learn it for each class separately using the data generated by MUNIT, which has acquired knowledge of $T(\cdot |x)$ from the entire dataset.&lt;/p&gt; &lt;p&gt;So MUNIT can decompose the example-specific information, e.g., whether something is an apple or a house cat, from the meta-information, the applied nuisance transformations. When we add more classes, it has more data and can better learn the transformation distribution T(\cdot |x)$. Does solving a meta-learning problem make MUNIT a meta-learner? Let’s look at the relationship MUNIT has with traditional meta-learners.&lt;/p&gt; &lt;h2 id=&quot;how-much-meta-learning-is-in-munit&quot;&gt;How much meta-learning is in MUNIT?&lt;/h2&gt; &lt;p&gt;To see how well MUNIT fits the definition of meta-learning, let’s define meta-learning more concretely. We define contemporary neural-network-based meta-learners in terms of a learning procedure: An outer training loop with a set of trainable parameters iterates over tasks in a distribution of tasks. Formally a task is comprised of a dataset and a loss function $ \mathcal{T} = \{ \mathcal{D}, \mathcal{L} \} $. In an inner loop, a learning algorithm based on the outer loop’s parameters is instantiated for each task. We train it on a training set (&lt;em&gt;meta-training&lt;/em&gt;) and test it on a validation set (&lt;em&gt;meta-validation&lt;/em&gt;). We then use loss on this validation set to update the outer loop’s parameters. In this task-centered view of meta-learning, we can express the objective function as&lt;/p&gt; &lt;p&gt; $$ \underset{\omega}{\mathrm{min}} \; \mathbb{E}_{\mathcal{T} \sim p(\mathcal{T})} \; \mathcal{L}(\mathcal{D}, \omega), $$ &lt;/p&gt; &lt;p&gt;where $ \omega $ is parameters trained exclusively on the meta-level, i.e., the &lt;em&gt;meta-knowledge&lt;/em&gt; learnable from the task distribution &lt;d-cite key=&quot;DBLP:journals/pami/HospedalesAMS22&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;This &lt;em&gt;meta-knowledge&lt;/em&gt; is what the meta-learner accumulates and transfers across the tasks in Thrun and Pratt’s definition above. Collecting meta-knowledge allows the meta-learner to improve its expected task performance with the number of tasks. The meta-knowledge in the experiment of Zhou et al. [2022] &lt;d-cite key=&quot;DBLP:conf/iclr/ZhouTRKPHF22&quot;&gt;&lt;/d-cite&gt; is the invariance to the nuisance transformations as the transformations are identical and need to be ignored for images of all classes. By creating additional transformed samples, the MUNIT network makes the meta-knowledge learnable for the CNN.&lt;/p&gt; &lt;p&gt;The task-centered view of meta-learning brings us to a related issue: A meta-learner must discern and decompose task-specific knowledge from meta-knowledge. Contemporary meta-learners decompose meta-knowledge through the different objectives of their inner and outer loops and their respective loss terms. They store meta-knowledge in the outer loop’s parameter set $ \omega $ but must not learn task-specific information there. Any unlearned meta-features lead to slower adaptation, negatively impacting performance, &lt;em&gt;meta-underfitting&lt;/em&gt;. On the other hand, any learned task-specific features will not generalize to unseen tasks in the distribution, thus also negatively impacting performance, &lt;em&gt;meta-overfitting&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;We recall that, similarly, MUNIT &lt;d-cite key=&quot;DBLP:conf/eccv/HuangLBK18&quot;&gt;&lt;/d-cite&gt; decomposes domain-specific style information and domain-agnostic content information. Applied to two domains, leopards and house cats, a MUNIT network will encode the domain-agnostic information, e.g., posture, scale, background, in its content latent space, and the domain-specific information, e.g., how a cat’s hair looks, in its style latent space. If the MUNIT network encoded the domain-agnostic information in the style latent space, the resulting image would not appear to be a good translation since the style information is discarded and replaced. It might turn a closed-eyed leopard into a staring cat. If the MUNIT network encoded the domain-specific transformation in the content latent space, the network would have difficulty translating between domains. A house cat might still have its original leopard fur.&lt;/p&gt; &lt;p&gt;Both meta-learning and multi-domain unsupervised image-to-image translation are thus learning problems that require a separation of the general from the specific. This is even visible when comparing their formalizations as optimization problems.&lt;/p&gt; &lt;p&gt;Francheschi et al. [2018] &lt;d-cite key=&quot;DBLP:conf/icml/FranceschiFSGP18&quot;&gt;&lt;/d-cite&gt; show that all contemporary neural-network-based meta-learning approaches can be expressed as bi-level optimization problems. We can formally write the optimization objective of a general meta-learner as:&lt;/p&gt; &lt;p&gt; $$ \bbox[5pt, border: 2px solid blue]{ \begin{align*} \omega^{*} = \underset{\omega}{\mathrm{argmin}} \sum_{i=1}^{M} \mathcal{L}^{meta}(\theta^{* \; (i)}(\omega), D^{val}_i), \end{align*} } $$ &lt;/p&gt; &lt;p&gt;where $M$ describes the number of tasks in a batch, $\mathcal{L}^{meta}$ is the meta-loss function, and $ D^{val}_i $ is the validation set of the task $ i $. $\omega$ represents the parameters exclusively updated in the outer loop. $ \theta^{* \; (i)} $ represents an inner loop learning a task that we can formally express as a sub-objective constraining the primary objective&lt;/p&gt; &lt;p&gt; $$ \bbox[5pt, border: 2px solid red]{ \begin{align*} s.t. \; \theta^{* \; (i)} = \underset{\theta}{\mathrm{argmin}} \; \mathcal{L^{task}}(\theta, \omega, D^{tr}_i), \end{align*} } $$ &lt;/p&gt; &lt;p&gt;where $ \theta $ are the model parameters updated in the inner loop, $ \mathcal{L}^{task} $ is the loss function by which they are updated and $ D^{tr}_i $ is the training set of the task $ i $ &lt;d-cite key=&quot;DBLP:journals/pami/HospedalesAMS22&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;It turns out that the loss functions of MUNIT can be similarly decomposed:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_LOSS.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_LOSS.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_LOSS.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_LOSS.svg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;MUNIT’s loss function consists of two adversarial (GAN) &lt;d-cite key=&quot;DBLP:conf/nips/GoodfellowPMXWOCB14&quot;&gt;&lt;/d-cite&gt; loss terms (see figure above) with several auxiliary reconstruction loss terms. To keep the notation simple, we combine all reconstruction terms into a joined reconstruction loss $ \mathcal{L}_{recon}(\theta_c, \theta_s) $, where $ \theta_c $ are the parameters of the &lt;em&gt;content&lt;/em&gt; encoding/decoding networks and $ \theta_s $ are the parameters of the &lt;em&gt;style&lt;/em&gt; encoding/decoding networks. We will only look at one of the two GAN losses in detail since they are symmetric, and one is discarded entirely when MUNIT is used on a single domain in the fashion of Zhou et al. [2022] &lt;d-cite key=&quot;DBLP:conf/iclr/ZhouTRKPHF22&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;MUNIT’s GAN loss term is&lt;/p&gt; &lt;p&gt; $$ \begin{align*} &amp;amp;\mathcal{L}^{x_{2}}_{GAN}(\theta_d, \theta_c, \theta_s) \\\\ =&amp;amp; \;\mathbb{E}_{c_{1} \sim p(c_{1}), s_{2} \sim p(s_{2})} \left[ \log (1 -D_ {2} (G_{2} (c_{1}, s_{2}, \theta_c, \theta_s), \theta_d)) \right] \\ +&amp;amp; \;\mathbb{E}_{x_{2} \sim p(x_{2})} \left[ \log(D_{2} (x_{2}, \theta_d)) \right], \end{align*} $$ &lt;/p&gt; &lt;p&gt;where the $ \theta_d $ represents the parameters of the discriminator network, $p(x_2)$ is the data of the second domain, $ c_1 $ is the content embedding of an image from the first domain to be translated. $ s_2 $ is a random style code of the second domain. $ D_2 $ is the discriminator of the second domain, and $ G_2 $ is its generator. MUNIT’s full objective function is:&lt;/p&gt; &lt;p&gt; $$ \begin{align*} \underset{\theta_c, \theta_s}{\mathrm{argmin}} \; \underset{\theta_d}{\mathrm{argmax}}&amp;amp; \;\mathbb{E}_{c_{1} \sim p(c_{1}), s_{2} \sim p(s_{2})} \left[ \log (1 -D_ {2} (G_{2} (c_{1}, s_{2}, \theta_c, \theta_s), \theta_d)) \right] \\ +&amp;amp; \; \mathbb{E}_{x_{2} \sim p(x_{2})} \left[ \log(D_{2} (x_{2}, \theta_d)) \right], + \; \mathcal{L}^{x_{1}}_{GAN}(\theta_d, \theta_c, \theta_s) \\ +&amp;amp; \;\mathcal{L}_{recon}(\theta_c, \theta_s) \end{align*} $$ &lt;/p&gt; &lt;p&gt;(compare &lt;d-cite key=&quot;DBLP:conf/eccv/HuangLBK18, DBLP:conf/nips/GoodfellowPMXWOCB14&quot;&gt;&lt;/d-cite&gt;). We can reformulate this into a bi-level optimization problem by extracting a minimization problem describing the update of the generative networks. We also drop the second GAN loss term as it is not relevant to our analysis.&lt;/p&gt; &lt;p&gt; $$ \bbox[5px, border: 2px solid blue]{ \begin{align*} \omega^{*} &amp;amp; = \{ \theta_c^*, \theta_s^* \} \\\\ &amp;amp; = \underset{\theta_c, \theta_s}{\mathrm{argmin}} \; \mathbb{E}_{c_{1} \sim p(c_{1}), s_{2} \sim p(s_{2})} \left[ \log (1 -D_ {2} (G_{2} (c_{1}, s_{2}, \theta_c, \theta_s), \theta_d^{*})) \right] \\ &amp;amp; + \mathcal{L}_{recon}(\theta_c, \theta_s), \end{align*} } $$ &lt;/p&gt; &lt;p&gt;We then add a single constraint, a subsidiary maximization problem for the discriminator function:&lt;/p&gt; &lt;p&gt; $$ \bbox[5px, border: 2px solid red]{ \begin{align*} &amp;amp;s.t. \;\theta_d^{*} \\\\ &amp;amp; = \underset{\theta_d}{\mathrm{argmax}} \; \mathbb{E}_{c_{1} \sim p(c_{1}), s_{2} \sim p(s_{2})} \left[ \log (1 -D_ {2} (G_{2} (c_{1}, s_{2}, \theta_c, \theta_s), \theta_d)) \right] \\ &amp;amp; + \mathbb{E}_{x_{2} \sim p(x_{2})} \left[ \log(D_{2} (x_{2}, \theta_d)) \right] \end{align*} } $$ &lt;/p&gt; &lt;p&gt;Interestingly, this bi-level view does not only resemble a meta-learning procedure as expressed above, but the bi-level optimization also facilitates a similar effect. Maximizing the discriminator’s performance in the constraint punishes style information encoded as content information. If style information is encoded as content information, the discriminator detects artifacts of the original domain in the translated image. Similarly, a meta-learner prevents &lt;em&gt;meta-overfitting&lt;/em&gt; via an outer optimization loop.&lt;/p&gt; &lt;p&gt;The two procedures also share “indirect” parameter updates. During GAN training, the discriminator’s parameters are updated through the changes in the generator’s parameters, which derive from the discriminator’s previous parameters, and so forth; The training of the discriminator and generator are separate but dependent processes. Similarly, in a meta-learner, the outer loop impacts the inner loop by determining its initial learner. The results of the inner loop, meanwhile, impact the outer loop via the meta-validation loss. While often overlapping in practice, the inner and outer loop parameter sets could, in principle, be completely disjunct, as they are in GAN training.&lt;/p&gt; &lt;p&gt;Concluding, we discern between MUNIT applied to two domains versus a single domain. When applied to two domains, MUNIT is a &lt;em&gt;binary&lt;/em&gt; image-to-image translation architecture, i.e., we cannot add domains. For multi-domain translation, we need to train many MUNIT networks to translate between pairs of domains. Thus, although it uses mechanisms similar to a meta-learner, it is &lt;em&gt;not&lt;/em&gt; a meta-learner in an image-to-image translation context.&lt;/p&gt; &lt;p&gt;However, when applied to a single domain MUNIT &lt;em&gt;does&lt;/em&gt; “learn to learn” (if you agree with the conclusion of the previous chapter) as it combines information from all classes to extract the transformation distribution. While it does not &lt;em&gt;perform&lt;/em&gt; classification, the class information of an image is encoded in MUNIT’s content space. Since MUNIT is trained unsupervised, it is probably closer to a distance metric than an actual class label. We might thus classify single-domain MUNIT as an unsupervised, generative meta-learner. It performs meta-learning in a general sense of “discerning the general from the task-specific” using a related two-loop training approach with two sets of parameters and a similar loss function. Thus MUNIT is related to meta-learning methods.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The ICLR paper “Do Deep Networks Transfer Invariances Across Classes?” &lt;d-cite key=&quot;DBLP:conf/iclr/ZhouTRKPHF22&quot;&gt;&lt;/d-cite&gt; shows that image-to-image translation methods can be used to learn and apply nuisance transformations, enabling a CNN to become invariant to them via data augmentation. This blog post argued that this is a meta-learning setting. In the view of this author, Zhou et al. [2022] &lt;d-cite key=&quot;DBLP:conf/iclr/ZhouTRKPHF22&quot;&gt;&lt;/d-cite&gt; solve a meta-learning problem using an unsupervised, generative method. A closer examination reveals parallels between both types of architecture.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Learning the meta-information of image-to-image translation and meta-learning might enable researchers to design better architectures in both domains.&lt;/em&gt;&lt;/p&gt; </content> </entry> <entry> <title>How does the inductive bias influence the generalization capability of neural networks?</title> <link href="https://jocelynshen.com/blog/2022/how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/"/> <updated>2022-12-01T00:00:00+00:00</updated> <id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks</id> <content type="html">&lt;p&gt;Deep neural networks are a commonly used machine learning technique that has proven to be effective for many different use cases. However, their ability to generalize from training data is not well understood. In this blog post, we will explore the paper “Identity Crisis: Memorization and Generalization under Extreme Overparameterization” by Zhang et al. [2020] &lt;d-cite key=&quot;DBLP:conf/iclr/ZhangBHMS20&quot;&gt;&lt;/d-cite&gt;, which aims to shed light on the question of why neural networks are able to generalize, and how inductive biases influence their generalization capabilities.&lt;/p&gt; &lt;h2 id=&quot;overfitting-puzzle&quot;&gt;Overfitting Puzzle&lt;/h2&gt; &lt;p&gt;One open question in the field of machine learning is the &lt;strong&gt;overfitting puzzle&lt;/strong&gt;, which describes the paradox that neural networks are often used in an overparameterized state (i.e., with more parameters than training examples), yet they are still able to generalize well to new, unseen data. This contradicts classical learning theory, which states that a model with too many parameters will simply memorize the training data and perform poorly on new data.&lt;/p&gt; &lt;p&gt;Neural networks, particularly deep networks, are typically used in the overparameterized regime, where the number of parameters exceeds the number of training examples. In this case, common generalization bounds do not apply &lt;d-cite key=&quot;DBLP:journals/corr/abs-1801-00173&quot;&gt;&lt;/d-cite&gt;. According to classical learning theory, the generalization behavior of a learning system should depend on the number of training examples (n), and the complexity of the model should be balanced with its fit to the data &lt;d-cite key=&quot;DBLP:journals/corr/abs-1801-00173&quot;&gt;&lt;/d-cite&gt;. However, neural networks have shown that this is not always the case, as they can perform well even in cases of extreme overparameterization (e.g., a 5-layer CNN with 80 million parameters &lt;d-cite key=&quot;DBLP:conf/iclr/ZhangBHMS20&quot;&gt;&lt;/d-cite&gt;).&lt;/p&gt; &lt;p&gt;To better understand this phenomenon, Zhang et al. [2020] &lt;d-cite key=&quot;DBLP:conf/iclr/ZhangBHMS20&quot;&gt;&lt;/d-cite&gt; examined the role of inductive bias in neural networks. Inductive bias, or learning bias, refers to the assumptions a network makes about the nature of the target function and is determined by the network’s architecture. Zhang et al. [2020] &lt;d-cite key=&quot;DBLP:conf/iclr/ZhangBHMS20&quot;&gt;&lt;/d-cite&gt; conducted experiments with different types of fully connected networks (FCN) and convolutional neural networks (CNN) to investigate which biases are effective for these network architectures.&lt;/p&gt; &lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt; &lt;!--- paar Ergebnisse vorstellen (z.B. Figure 2,3; Was sehen wir? Graphiken, GIF?, wechsle sieht) Erklärung, warum wenige Layer besser funktionieren als viele 2 Theoreme erklären? nicht zu speziell Vllt noch Figure 4 erklären Bilder reduzieren auf Wichtiges! ---&gt; &lt;p&gt;In the paper “Identity Crisis: Memorization and Generalization under Extreme Overparameterization” by Zhang et al. [2020] &lt;d-cite key=&quot;DBLP:conf/iclr/ZhangBHMS20&quot;&gt;&lt;/d-cite&gt;, the authors use &lt;strong&gt;empirical studies&lt;/strong&gt; to better understand the &lt;em&gt;overfitting puzzle&lt;/em&gt; and how inductive bias affects the behavior of overparameterized neural networks. The authors specifically aim to investigate the role of inductive bias under &lt;strong&gt;different architectural choices&lt;/strong&gt; by comparing fully connected and convolutional neural networks.&lt;/p&gt; &lt;p&gt;The task used in the study is to learn an identity map through a single data point, which is an artificial setup that demonstrates the most extreme case of overparameterization. The goal of the study is to determine whether a network tends towards memorization (learning a constant function) or generalization (learning the identity function).&lt;/p&gt; &lt;p&gt;To enable the &lt;strong&gt;identity task&lt;/strong&gt; &lt;d-cite key=&quot;DBLP:conf/eccv/HeZRS16&quot;&gt;&lt;/d-cite&gt; for linear models, the authors ensure that hidden dimensions are not smaller than the input and set the weights to the identity matrix in every layer. For convolutional layers, only the center of the kernel is used and all other values are set to zero, simulating a 1 x 1 convolution which acts as a local identity function. For deeper models that use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&quot;&gt;ReLU&lt;/a&gt; activation function, it is necessary to encode and recover negative values, as they are discarded by the ReLU function. This can be achieved by using hidden dimensions that are twice the size of the input and storing negative and positive values separately.&lt;/p&gt; &lt;p&gt;The study uses the &lt;strong&gt;&lt;a href=&quot;https://paperswithcode.com/dataset/mnist&quot;&gt;MNIST dataset&lt;/a&gt;&lt;/strong&gt; and tests the networks on various types of data, including a linear combination of two digits, random digits from the MNIST test set, random images from the Fashion MNIST dataset, and algorithmically generated image patterns.&lt;/p&gt; &lt;p&gt;So let us look at some of the results:&lt;/p&gt; &lt;div class=&quot;l-page&quot;&gt; &lt;iframe src=&quot;/deep-learning-blog.github.io/assets/html/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_3.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; width=&quot;100%&quot; height=&quot;806px&quot;&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;h3 id=&quot;fully-connected-networks-fcn&quot;&gt;Fully connected networks (FCN)&lt;/h3&gt; &lt;p&gt;The figure above shows that for fully connected networks, the outputs differ depending on the depth of the network and the type of testing data. Shallower networks seem to incorporate random white noise into the output, while deeper networks tend to learn the constant function. The similarity of the test data to the training example also affects the behavior of the model. When the test data is from the MNIST digit sets, all network architectures perform quite well. However, for test data that is more dissimilar to the training data, the output tends to include more random white noise. The authors prove this finding with a &lt;em&gt;theorem&lt;/em&gt; for 1-layer FCNs,&lt;/p&gt; \[f(x) = \Pi(x) + R \Pi(x),\] &lt;p&gt;which decomposes the test data point $x$ into components that are parallel and perpendicular to the training example. This theorem shows that the output becomes more random as the test data becomes less similar to the training data.&lt;/p&gt; &lt;p&gt;This behavior can be confirmed by visualizing the results of the 1-layer FCN:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_1layer-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_1layer-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_1layer-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_1layer.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;The inductive bias does not lead to either good generalization or memorization. Instead, the predictions become more random as the test data becomes less similar to the training data.&lt;/p&gt; &lt;p&gt;Deeper networks tend to learn the constant function, resulting in a strong inductive bias towards the training output regardless of the specific input. This behavior is similar to that of a deep ReLU network, as shown in the figure comparing deep FCN and deep ReLU networks.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_compareFCNReLU-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_compareFCNReLU-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_compareFCNReLU-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_compareFCNReLU.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Zhang et al. [2020] &lt;d-cite key=&quot;DBLP:conf/iclr/ZhangBHMS20&quot;&gt;&lt;/d-cite&gt; conclude that more complex network architectures are more prone to memorization. This finding aligns with statistical learning theory, as a more complex architecture has more parameters and, therefore, more overparameterization.&lt;/p&gt; &lt;h3 id=&quot;convolutional-neural-networks-cnns&quot;&gt;Convolutional neural networks (CNNs)&lt;/h3&gt; &lt;p&gt;For convolutional neural networks, the inductive bias was analyzed using the ReLU activation function and testing networks with different depths. The hidden layers of the CNN consist of 5 × 5 convolution filters organized into 128 channels. The networks have two constraints to match the structure of the identity target function.&lt;/p&gt; &lt;p&gt;If you choose the button ‘CNN’ in the first figure, it shows the resulting visualizations. It can be seen that shallow networks are able to learn the identity function, while intermediate-depth networks function as edge detectors and deep networks learn the constant function. Whether the model learns the identity or the constant function, both outcomes reflect inductive biases since no specific structure was given by the task.&lt;/p&gt; &lt;p&gt;A better understanding of the evolution of the output can be obtained by examining the status of the prediction in the hidden layers of the CNN. Since CNNs, unlike FCNs, preserve the spatial relations between neurons in the intermediate layers, these layers can be visualized. The figure below shows the results for a randomly initialized 20-layer CNN compared to different depths of trained CNNs.”&lt;/p&gt; &lt;div class=&quot;l-page&quot;&gt; &lt;iframe src=&quot;/deep-learning-blog.github.io/assets/html/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/CNNs_intermedLayers.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; width=&quot;100%&quot; height=&quot;900px&quot;&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;p&gt;Random convolution gradually smooths out the input data, and after around eight layers, the shapes are lost. When the networks are trained, the results differ. The 7-layer CNN performs well and ends up with an identity function of the input images, while the results of the 14-layer CNN are more blurry. For the 20-layer trained CNN, it initially behaves similarly to the randomly initialized CNN by wiping out the input data, but it preserves the shapes for a longer period. In the last three layers, it renders the constant function of the training data and outputs 7 for any input.&lt;/p&gt; &lt;p&gt;These results align with the findings of Radhakrishnan et al. [2018] &lt;d-cite key=&quot;radhakrishnan2019memorization&quot;&gt;&lt;/d-cite&gt; in ‘Memorization in overparametrized autoencoders’, which used a similar empirical framework on fully-connected autoencoders. They found that deep neural networks learn locally contractive maps around the training examples, leading to learning the constant function.&lt;/p&gt; &lt;p&gt;As for FCNs, the experiments show that the similarity of the test data to the training data point increases task success. Zhang et al. [2020] &lt;d-cite key=&quot;DBLP:conf/iclr/ZhangBHMS20&quot;&gt;&lt;/d-cite&gt; conducted further experiments with different &lt;strong&gt;feature channel numbers and dimensions&lt;/strong&gt;. They found that increasing the hidden dimensions/adding channels is much less prone to overfitting than adding depth. This should be considered when designing new models: if the goal is to increase the number of parameters of an existing model (perhaps to improve optimization dynamics or prepare for more training data), it is better to try increasing the hidden dimension before tuning the depth, unless the nature of the data changes.&lt;/p&gt; &lt;p&gt;Another factor that influences inductive bias is **model initialization++. For networks with few channels, the difference between random initialization and the converged network is extreme &lt;d-cite key=&quot;DBLP:conf/iclr/FrankleC19&quot;&gt;&lt;/d-cite&gt;. This can be explained as follows: in the regime of random initialization with only a few channels, the initialization does not have enough flexibility to compensate for incorrect choices. As a result, the networks are more likely to converge to non-optimal extrema. Having more channels helps to smooth out this problem, as more parameters can compensate for ‘unlucky’ cases.&lt;/p&gt; &lt;h2 id=&quot;general-findings&quot;&gt;General findings&lt;/h2&gt; &lt;p&gt;The figures below show that CNNs have better generalization capability than FCNs. However, it is important to note that the experiments primarily aim to compare different neural networks &lt;strong&gt;within their architecture type&lt;/strong&gt;, so a comparison between FCNs and CNNs cannot be considered fair. CNNs have natural advantages due to sparser networks and structural biases, such as local receptive fields and parameter sharing, that are consistent with the identity task. Additionally, CNNs have more parameters, as seen in the underlying figure: a 6-layer FCN contains 3.6M parameters, while a 5-layer CNN (with 5x5 filters of 1024 channels) has 78M parameters. These differences should be taken into account when evaluating the results of the experiments.&lt;/p&gt; &lt;div class=&quot;l-page&quot; style=&quot;width: 704px; margin: auto;&quot;&gt; &lt;iframe src=&quot;/deep-learning-blog.github.io/assets/html/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/plot.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; width=&quot;100%&quot; height=&quot;480px&quot;&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;p&gt;To conclude, CNNs generalize better than FCNs, even though they have more parameters. This is consistent with the observed phenomenon that neural networks do not follow the statistical learning theory.&lt;/p&gt; &lt;p&gt;The experiments described above lead to the following main findings of the paper:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The number of parameters does not strongly correlate with generalization performance, but the structural bias of the model does.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For example, when equally overparameterized,&lt;/p&gt; &lt;ul&gt; &lt;li&gt;training a very deep model without residual connections is prone to memorization, while&lt;/li&gt; &lt;li&gt;adding more feature channels/dimensions is much less likely to cause overfitting.&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;After reading this blog post, we hope that the concept of the overfitting puzzle is understood and that you appreciate the significance of the study conducted by Zhang et al. [2020] &lt;d-cite key=&quot;DBLP:conf/iclr/ZhangBHMS20&quot;&gt;&lt;/d-cite&gt;. The artificial setup used in the study is a smart way to approach this topic and allows for an intuitive interpretation of the results. The authors found that CNNs tend to “generalize” by actually learning the concept of identity, while FCNs are prone to memorization. Within these networks, it can be said that the simpler the network architecture is, the better the task results. Another observation is that deep CNNs exhibit extreme memorization. It would have been interesting to analyze the inductive bias for other types of data (e.g., sequence data like speech) and compare whether the stated theorems also hold in those cases.&lt;/p&gt; &lt;p&gt;In summary, Zhang et al. [2020] &lt;d-cite key=&quot;DBLP:conf/iclr/ZhangBHMS20&quot;&gt;&lt;/d-cite&gt; conducted interesting studies that have helped the machine learning community to gain a deeper understanding of inductive bias. Their results provide concrete guidance for practitioners that can help design models for new tasks.&lt;/p&gt; </content> </entry> <entry> <title>A Hitchhiker's Guide to Momentum</title> <link href="https://jocelynshen.com/blog/2022/hitchhikers-momentum/"/> <updated>2022-12-01T00:00:00+00:00</updated> <id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/hitchhikers-momentum</id> <content type="html">&lt;!-- some latex shortcuts --&gt; &lt;div style=&quot;display: none&quot;&gt; $$ \def\argmin{\mathop{\mathrm{arg\,min}}} \def\xx{\pmb{x}} \def\HH{\pmb{H}} \def\bb{\pmb{b}} \def\EE{ \mathbb{E} } \def\RR{ \mathbb{R} } \def\lmax{L} \def\lmin{\mu} \def\defas{\stackrel{\text{def}}{=}} \definecolor{colormomentum}{RGB}{27, 158, 119} \definecolor{colorstepsize}{RGB}{217, 95, 2} \def\mom{ {\color{colormomentum}{m}} } \def\step{ {\color{colorstepsize}h} } $$ &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/rate_convergence_momentum-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/rate_convergence_momentum-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/rate_convergence_momentum-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/rate_convergence_momentum.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;gradient-descent-with-momentum&quot;&gt;Gradient Descent with Momentum&lt;/h2&gt; &lt;p&gt;Gradient descent with momentum,&lt;d-cite key=&quot;polyak1964some&quot;&gt;&lt;/d-cite&gt; also known as heavy ball or momentum for short, is an optimization method designed to solve unconstrained minimization problems of the form \begin{equation} \argmin_{\xx \in \RR^d} \, f(\xx)\,, \end{equation} where the objective function \(f\) is differentiable and we have access to its gradient \(\nabla f\). In this method the update is a sum of two terms. The first term is the difference between the current and the previous iterate \((\xx_{t} - \xx_{t-1})\), also known as &lt;em&gt;momentum term&lt;/em&gt;. The second term is the gradient \(\nabla f(\xx_t)\) of the objective function.&lt;/p&gt; &lt;p class=&quot;framed&quot;&gt; &lt;b class=&quot;underline&quot;&gt;Gradient Descent with Momentum&lt;/b&gt;&lt;br /&gt; &lt;b&gt;Input&lt;/b&gt;: starting guess \(\xx_0\), step-size \(\step &amp;gt; 0\) and momentum parameter \(\mom \in (0, 1)\).&lt;br /&gt; \(\xx_1 = \xx_0 - \dfrac{\step}{\mom+1} \nabla f(\xx_0)\) &lt;br /&gt; &lt;b&gt;For&lt;/b&gt; \(t=1, 2, \ldots\) compute \begin{equation}\label{eq:momentum_update} \xx_{t+1} = \xx_t + \mom(\xx_{t} - \xx_{t-1}) - \step\nabla f(\xx_t) \end{equation} &lt;/p&gt; &lt;p&gt;Despite its simplicity, gradient descent with momentum exhibits unexpectedly rich dynamics that we’ll explore on this post.&lt;/p&gt; &lt;p&gt;The origins of momentum can be traced back to Frankel’s method in the 1950s for solving linear system of equations.&lt;d-cite key=&quot;frankel1950convergence&quot;&gt;&lt;/d-cite&gt; It was later generalized by Boris Polayk to non-quadratic objectives&lt;d-cite key=&quot;polyak1964some&quot;&gt;&lt;/d-cite&gt;. In recent years there has been a resurgence in interest in this venerable method, as a stochastic variant of this method, where the gradient is replaced by a stochastic estimate, is one of the most popular methods for deep learning. This has led in recent years to a flurry fo research –and improved understanding – of this stochastic variant. Although this blog posts limits itself with the deterministic variant, the interested reader is encouraged to explore following references. A good starting point is the paper by &lt;a href=&quot;https://arxiv.org/abs/1712.07628&quot;&gt;Sutskever et al.&lt;/a&gt;,&lt;d-cite key=&quot;sutskever2013importance&quot;&gt;&lt;/d-cite&gt; which was among the firsts to highlight the importance of momentum for deep learning optimization. More recent progress include an analysis of the last iteration of the method by &lt;a href=&quot;https://arxiv.org/abs/2104.09864&quot;&gt;Tao et al.&lt;/a&gt;&lt;d-cite key=&quot;tao2021the&quot;&gt;&lt;/d-cite&gt; and a paper by &lt;a href=&quot;https://arxiv.org/abs/2106.07587&quot;&gt;Liu et al.&lt;/a&gt;&lt;d-cite key=&quot;Liu2020Accelerating&quot;&gt;&lt;/d-cite&gt; that develops accelerated variants for over-parameterized models.&lt;/p&gt; &lt;p&gt;Coming back to the subject of our post, the (non-stochastic) gradient descendent with momentum method, a paper that also explores the dynamics of momentum is Gabriel Goh’s &lt;a href=&quot;https://distill.pub/2017/momentum/&quot;&gt;Why Momentum Really Works&lt;/a&gt;.&lt;d-cite key=&quot;goh2017momentum&quot;&gt;&lt;/d-cite&gt; There are subtle but important differences between both analysis. The landscape described in the section &lt;a href=&quot;https://distill.pub/2017/momentum/#momentum2D&quot;&gt;“The Dynamics of Momentum”&lt;/a&gt; describe the improvement along the direction &lt;em&gt;of a single eigenvector&lt;/em&gt;. This partial view produces some misleading conclusions. For example, along the direction of a single eigenvector, the largest improvement is achieved with zero momentum and a step-size of 1 over the associated eigenvalue. This conclusion however doesn’t hold in higher dimensions, where as we will see, the momentum term that yields the fastest convergence is non-zero.&lt;/p&gt; &lt;h2 id=&quot;how-fast-is-momentum&quot;&gt;How fast is Momentum?&lt;/h2&gt; &lt;p&gt;Momentum is &lt;em&gt;fast&lt;/em&gt;. So fast that it’s often the default choice of machine learning practitioners. But can we quantify this more precisely?&lt;/p&gt; &lt;p&gt;Throughout the post we’ll assume that our objective function \(f\) is a quadratic objective of the form \begin{equation}\label{eq:opt} f(\xx) \defas \frac{1}{2}(\xx - \xx^\star) \HH (\xx - \xx^\star)~, \end{equation} where \(\HH\) is a symmetric positive definite matrix and \(\xx^\star\) is the minimizer of the objective. We’ll assume that the eigenvalues of \(\HH\) are in the interval \([\mu, L]\).&lt;/p&gt; &lt;p&gt;The measure we’ll use to quantify the speed of convergence is the rate of convergence. This is the worst-case relative improvement in the iterate suboptimality at iteration \(t\), defined as \begin{equation}\label{eq:convergence_rate} r_t \defas \sup_{\xx_0, \text{eigs}(\HH) \in [\mu, L]} \frac{\|\xx_{t} - \xx^\star\|}{\|\xx_{0} - \xx^\star\|}\,. \end{equation} This is a worst-case measure because of all problem instances, we take worst possible initialization \(\xx_0\) and matrix \(\HH\) with eigenvalues in the interval \([\mu, L]\).&lt;/p&gt; &lt;p&gt;This is a measure of how much progress is made (in the worst-case) at iteration \(t\). The smaller the value of \(r_t\), the faster the algorithm converges. Since all algorithms that we consider converge exponentially fast, for large enough \(t\) the error is of the order of \(\mathcal{O}{(\text{constant}^t)}\). Hence the most informative quantity is the value of \(\text{constant}\) in this expression. We’ll call this quantity the &lt;i&gt;asymptotic rate of convergence&lt;/i&gt;, and denote it: \begin{equation} r_{\infty} \defas \limsup_{t \to \infty} \sqrt[t]{r_t}\,. \end{equation} This is the quantity we’ll be discussing throughout the post and what we’ll use to compare the speed of momentum for different values of its hyperparameters.&lt;/p&gt; &lt;h3 id=&quot;a-connection-between-optimization-methods-and-polynomials&quot;&gt;A connection between optimization methods and polynomials&lt;/h3&gt; &lt;p&gt;To compute easily the asymptotic rate of convergence for all admissible values of step-size and momentum, we’ll use a connection between optimization of quadratic functions and the theory of orthogonal polynomials. This theory was extensively used in the early days of numerical analysis &lt;d-cite key=&quot;Rutishauser1959&quot;&gt;&lt;/d-cite&gt; and provides an elegant and simple way to compute asymptotic rates (and non-asymptotic ones, althought not the topic of this blog post) from known results in the theory of orthogonal polynomials. We favor this technique for its simplicity and elegance, although ones ones that also be used with identical results. Other techniques include the linear operator technique used by Polyak,&lt;d-cite key=&quot;polyak1964some&quot;&gt;&lt;/d-cite&gt; the estimate sequences technique pioneered by Nesterov&lt;d-cite key=&quot;nesterov1983method&quot;&gt;&lt;/d-cite&gt; or the use of Lyapunov functions.&lt;d-cite key=&quot;JMLR:v22:20-195&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;The main result that will allow us to make the link between optimization and orthogonal polynomials is the following result. It’s origins seem unclear, although a proof can be found in the 1959 monograph of Rutishauser.&lt;d-cite key=&quot;Rutishauser1959&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p class=&quot;lemma&quot;&gt; Consider the following polynomial \(P_t\) of degree \(t\), defined recursively as: \begin{equation} \begin{split} &amp;amp;P_{t+1}(\lambda) = (1 + \mom - \step \lambda ) P_{t}(\lambda) - \mom P_{t-1}(\lambda)\\ &amp;amp;P_1(\lambda) = 1 - \frac{\step}{1 + \mom} \lambda\,, ~ P_0(\lambda) = 1\,,~ \end{split}\label{eq:def_residual_polynomial2} \end{equation} Then we can write the suboptimality at iteration \(t\) as \begin{equation} \xx_t - \xx^\star = P_t(\HH) \left( \xx_0 - \xx^\star \right) \,, \end{equation} where \(P_t(\HH)\) is the matrix obtained from evaluating the (originally rel-valued) polynomial \(P_t\) at the matrix \(\HH\). &lt;/p&gt; &lt;p&gt;This last identity will allow us to easily compute convergence rates. In particular, plugging it into the definition of the convergence rate \eqref{eq:convergence_rate} we get that the rate is determined by the absolute value of the residual polynomial over the \([\mu, L]\) interval: \begin{align} r_t &amp;amp;= \sup_{\xx_0, \text{eigs}(\HH) \in [\mu, L]} \frac{\|P_t(\HH) \left( \xx_0 - \xx^\star \right)\|}{\|\xx_{0} - \xx^\star\|} \\ &amp;amp; = \sup_{\text{eigs}(\HH) \in [\mu, L]} \|P_t(\HH)\| \\ &amp;amp; = \sup_{\lambda \in [\mu, L]} \lvert P_t(\lambda) \rvert\,. \end{align} We’ve now reduced the problem of computing the convergence rate to the problem of computing the absolute value of a polynomial over a given interval. This is a problem that has been extensively studied in the theory of orthogonal polynomials. In particular, we’ll use known bounds on Chebyshev polynomials of the first and second kind, as the residual polynomial of momentum can be written as a convex combination of these two polynomials. This fact is proven in the next result, which is a generalization of equation (II.29) in (Rutishauser 1959).&lt;d-cite key=&quot;Rutishauser1959&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p class=&quot;lemma&quot;&gt; The residual polynomial of momentum can be written in terms of Chebyshev polynomials of the first and second kind as \begin{align} P_t(\lambda) = \mom^{t/2} \left( {\small\frac{2\mom}{1+\mom}}\, T_t(\sigma(\lambda)) + {\small\frac{1 - \mom}{1 + \mom}}\,U_t(\sigma(\lambda))\right)\,. \end{align} where \(\sigma(\lambda) = {\small\dfrac{1}{2\sqrt{\mom}}}(1 + \mom - \step\,\lambda)\,\) is a linear function that we&apos;ll refer to as the &lt;span class=&quot;underline&quot;&gt;link function&lt;/span&gt; and \(T_t\) and \(U_t\) are the Chebyshev polynomials of the first and second kind respectively. &lt;/p&gt; &lt;div class=&quot;wrap-collapsible-XXX&quot;&gt; &lt;input id=&quot;collapsible3&quot; class=&quot;toggle&quot; type=&quot;checkbox&quot; /&gt; &lt;label for=&quot;collapsible3&quot; class=&quot;lbl-toggle&quot; tabindex=&quot;0&quot;&gt;&lt;b&gt;Show proof&lt;/b&gt;&lt;/label&gt;&lt;div class=&quot;collapsible-content&quot;&gt;&lt;div class=&quot;content-inner&quot;&gt;&lt;div class=&quot;proof&quot; id=&quot;proof-variance&quot;&gt; &lt;p&gt; Let&apos;s denote by \(\widetilde{P}_t\) the right hand side of the above equation, that is, \begin{equation} \widetilde{P}_{t}(\lambda) \defas \mom^{t/2} \left( {\small\frac{2 \mom}{1 + \mom}}\, T_t(\sigma(\lambda)) + {\small\frac{1 - \mom}{1 + \mom}}\, U_t(\sigma(\lambda))\right)\,. \end{equation} Our goal is to show that \(P_t = \widetilde{P}_t\) for all \(t\). &lt;/p&gt; &lt;p&gt; For \(t=1\), \(T_1(\lambda) = \lambda\) and \(U_1(\lambda) = 2\lambda\), so we have \begin{align} \widetilde{P}_1(\lambda) &amp;amp;= \sqrt{\mom} \left(\tfrac{2 \mom}{1 + \mom} \sigma(\lambda) + \tfrac{1 - \mom}{1 + \mom} 2 \sigma(\lambda)\right)\\ &amp;amp;= \frac{2 \sqrt{\mom}}{1 + \mom} \sigma(\lambda) = 1 - \frac{\step}{1 + \mom} \lambda\,, \end{align} which corresponds to the definition of \(P_1\) in \eqref{eq:def_residual_polynomial2}. &lt;/p&gt; &lt;p&gt; Assume it&apos;s true for any iteration up to \(t\), we will show it&apos;s true for \(t+1\). Using the three-term recurrence of Chebyshev polynomials we have \begin{align} &amp;amp;\widetilde{P}_{t+1}(\lambda) = \mom^{(t+1)/2} \left( {\small\frac{2 \mom}{1 + \mom}}\, T_{t+1}(\sigma(\lambda)) + {\small\frac{1 - \mom}{1 + \mom}}\, U_{t+1}(\sigma(\lambda))\right) \\ &amp;amp;= \mom^{(t+1)/2} \Big( {\small\frac{2 \mom}{1 + \mom}}\, (2 \sigma(\lambda) T_{t}(\sigma(\lambda)) - T_{t-1}(\sigma(\lambda))) \nonumber\\ &amp;amp;\qquad\qquad + {\small\frac{1 - \mom}{1 + \mom}}\, (2 \sigma(\lambda) U_{t}(\sigma(\lambda)) - U_{t-1}(\sigma(\lambda)))\Big)\\ &amp;amp;= 2 \sigma(\lambda) \sqrt{\mom} P_t(\lambda) - \mom P_{t-1}(\lambda)\\ &amp;amp;= (1 + \mom - \step \lambda) P_t(\lambda) - \mom P_{t-1}(\lambda) \end{align} where the third identity follows from grouping polynomials of same degree and the induction hypothesis. The last expression is the recursive definition of \(P_{t+1}\) in \eqref{eq:def_residual_polynomial2}, which proves the desired \(\widetilde{P}_{t+1} = {P}_{t+1}\). &lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt; &lt;h3 id=&quot;tools-of-the-trade-the-two-faces-of-chebyshev-polynomials&quot;&gt;Tools of the trade: the two faces of Chebyshev polynomials&lt;/h3&gt; &lt;p&gt;A key feature that we’ll use extensively about Chebyshev polynomials is that they behave very differently inside and outside the interval \([-1, 1]\). Inside this interval (shaded blue region) the magnitude of these polynomials stays close to zero, while outside it explodes:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/two_phases_chebyshev.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/two_phases_chebyshev.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/two_phases_chebyshev.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/two_phases_chebyshev.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Let’s make this observation more precise.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Inside&lt;/strong&gt; the \([-1, 1]\) interval, Chebyshev polynomials admit the &lt;a href=&quot;https://en.wikipedia.org/wiki/Chebyshev_polynomials#Trigonometric_definition&quot;&gt;trigonometric definitions&lt;/a&gt; \(T_t(\cos(\theta)) = \cos(t \theta)\) and \(U_{t}(\cos(\theta)) = \sin((t+1)\theta) / \sin(\theta)\) and so they have an oscillatory behavior with values bounded in absolute value by 1 and \(t+1\) respectively.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Outside&lt;/strong&gt; of this interval the Chebyshev polynomials of the first kind admit the &lt;a href=&quot;https://en.wikipedia.org/wiki/Chebyshev_polynomials#Explicit_expressions&quot;&gt;explicit form&lt;/a&gt; for \(|\xi| \ge 1\): \begin{align} T_t(\xi) &amp;amp;= \dfrac{1}{2} \Big(\xi-\sqrt{\xi^2-1} \Big)^t + \dfrac{1}{2} \Big(\xi+\sqrt{\xi^2-1} \Big)^t \\ U_t(\xi) &amp;amp;= \frac{(\xi + \sqrt{\xi^2 - 1})^{t+1} - (\xi - \sqrt{\xi^2 - 1})^{t+1}}{2 \sqrt{\xi^2 - 1}}\,. \end{align} We’re interested in convergence rates, so we’ll look into \(t\)-th root asymptotics of the quantities.&lt;d-footnote&gt;With little extra effort, it would be possible to derive non-asymptotic convergence rates, although I won&apos;t pursue this analysis here.&lt;/d-footnote&gt; Luckily, these asymptotics are the same for both polynomials&lt;d-footnote&gt;Although we won&apos;t use it here, this \(t\)-th root asymptotic holds for (almost) all orthogonal polynomials, not just Chebyshev polynomials. See for instance reference below&lt;/d-footnote&gt; &lt;d-cite key=&quot;stahl1990nth&quot;&gt;&lt;/d-cite&gt; and taking limits we have that \begin{equation} \lim_{t \to \infty} \sqrt[t]{|T_t(\xi)|} = \lim_{t \to \infty} \sqrt[t]{|U_t(\xi)|} = |\xi| + \sqrt{\xi^2 - 1}\,. \end{equation}&lt;/p&gt; &lt;h2 id=&quot;the-robust-region&quot;&gt;The Robust Region&lt;/h2&gt; &lt;p&gt;Let’s start first by considering the case in which the image of \(\sigma\) is in the \([-1, 1]\) interval. This is the most favorable case. In this case, the Chebyshev polynomials are bounded in absolute value by 1 and \(t+1\) respectively. Since the Chebsyshev polynomials are evaluated at \(\sigma(\cdot)\), this implies that \(\lvert \sigma(\lambda)\rvert \leq 1\). We’ll call the set of step-size and momentum parameters for which the previous inequality is verified the &lt;em&gt;robust region&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;Let’s visualize this region in a map. Since \(\sigma\) is a linear function, its extremal values are reached at the edges: \begin{equation} \max_{\lambda \in [\lmin, \lmax]} |\sigma(\lambda)| = \max{|\sigma(\lmin)|, |\sigma(\lmax)|}\,. \end{equation} Using \(\lmin \leq \lmax\) and that \(\sigma(\lambda)\) is decreasing in \(\lambda\), we can simplify the condition \(\lvert \sigma(\lambda)\rvert \leq 1\) to \(\sigma(\lmin) \leq 1\) and \(\sigma(L) \geq -1\), which in terms of the step-size and momentum correspond to: \begin{equation}\label{eq:robust_region} \frac{(1 - \sqrt{\mom})^2}{\lmin} \leq \step \leq \frac{(1 + \sqrt{\mom})^2}{L} \,. \end{equation} These two conditions provide the upper and lower bound of the robust region.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/sketch_robust_region-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/sketch_robust_region-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/sketch_robust_region-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/sketch_robust_region.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;asymptotic-rate&quot;&gt;Asymptotic rate&lt;/h3&gt; &lt;p&gt;Let \(\sigma(\lambda) = \cos(\theta)\) for some \(\theta\), which is always possible since \(\sigma(\lambda) \in [-1, 1]\). In this regime, Chebyshev polynomials verify the identities \(T_t(\cos(\theta)) = \cos(t \theta)\) and \(U_t(\cos(\theta)) = \sin((t+1)\theta)/\sin(\theta)\) , which replacing in the definition of the residual polynomial gives \begin{equation} P_t(\sigma^{-1}(\cos(\theta))) = \mom^{t/2} \left[ {\small\frac{2\mom}{1+\mom}}\, \cos(t\theta) + {\small\frac{1 - \mom}{1 + \mom}}\,\frac{\sin((t+1)\theta)}{\sin(\theta)}\right]\,. \end{equation}&lt;/p&gt; &lt;p&gt;Since the expression inside the square brackets is bounded in absolute value by \(t+2\), taking \(t\)-th root and then limits we have \(\limsup_{t \to \infty} \sqrt[t]{\lvert P_t(\sigma^{-1}(\cos(\theta)))\rvert} = \sqrt{\mom}\) for &lt;i&gt;any&lt;/i&gt; \(\theta\). This gives our first asymptotic rate:&lt;/p&gt; &lt;p class=&quot;framed&quot; style=&quot;text-align: center&quot;&gt; The asymptotic rate in the robust region is \(r_{\infty} = \sqrt{\mom}\). &lt;/p&gt; &lt;p&gt;This is nothing short of magical. It would seem natural –and this will be the case in other regions– that the speed of convergence should depend on both the step-size and the momentum parameter. Yet, this result implies that it’s not the case in the robust region. In this region, the convergence &lt;i&gt;only&lt;/i&gt; depends on the momentum parameter $\mom$. Amazing.&lt;d-footnote&gt;This insensitivity to step-size has been leveraged by Zhang et al. 2018 to develop a momentum tuner &lt;/d-footnote&gt; &lt;d-cite key=&quot;zhang2017yellowfin&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;This also illustrates why we call this the &lt;i&gt;robust&lt;/i&gt; region. In its interior, perturbing the step-size in a way that we stay within the region has no effect on the convergence rate. The next figure displays the asymptotic rate (darker is faster) in the robust region.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/rate_robust_region-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/rate_robust_region-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/rate_robust_region-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/rate_robust_region.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;the-lazy-region&quot;&gt;The Lazy Region&lt;/h2&gt; &lt;p&gt;Let’s consider now what happens outside of the robust region. In this case, the convergence will depend on the largest of \(\{\lvert\sigma(\lmin)\rvert, \lvert\sigma(L)\rvert\}\). We’ll consider first the case in which the maximum is \(\lvert\sigma(\lmin)\rvert\) and leave the other one for next section.&lt;/p&gt; &lt;p&gt;This region is determined by the inequalities \(\lvert\sigma(\lmin)\rvert &amp;gt; 1\) and \(\lvert\sigma(\lmin)\rvert \geq \lvert\sigma(L)\rvert\). Using the definition of \(\sigma\) and solving for \(\step\) gives the equivalent conditions \begin{equation} \step \leq \frac{2(1 + \mom)}{L + \lmin} \quad \text{ and }\quad \step \leq \frac{(1 - \sqrt{\mom})^2}{\lmin}\,. \end{equation} Note the second inequality is the same one as for the robust region \eqref{eq:robust_region} but with the inequality sign reversed, and so the region will be on the oposite side of that curve. We’ll call this the &lt;i&gt;lazy region&lt;/i&gt;, as in increasing the momentum will take us out of it and into the robust region.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/sketch_lazy_region-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/sketch_lazy_region-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/sketch_lazy_region-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/sketch_lazy_region.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;asymptotic-rate-1&quot;&gt;Asymptotic rate&lt;/h3&gt; &lt;p&gt;As we saw earlier, outside of the \([-1, 1]\) interval both Chebyshev have simple \(t\)-th root asymptotics. Using this and that both kinds of Chebyshev polynomials agree in sign outside of the \([-1, 1]\) interval we can compute the asymptotic rate as \begin{align} \lim_{t \to \infty} \sqrt[t]{r_t} &amp;amp;= \sqrt{\mom} \lim_{t \to \infty} \sqrt[t]{ {\small\frac{2\mom}{\mom+1}}\, T_t(\sigma(\lmin)) + {\small\frac{1 - \mom}{1 + \mom}}\,U_t(\sigma(\lmin))} \\ &amp;amp;= \sqrt{\mom}\left(|\sigma(\lmin)| + \sqrt{\sigma(\lmin)^2 - 1} \right) \\ \end{align} This gives the asymptotic rate for this region&lt;/p&gt; &lt;p class=&quot;framed&quot; style=&quot;text-align: center&quot;&gt; In the lazy region the asymptotic rate is \(r_{\infty} = \sqrt{\mom}\left(|\sigma(\lmin)| + \sqrt{\sigma(\lmin)^2 - 1} \right)\). &lt;/p&gt; &lt;p&gt;Unlike in the robust region, this rate depends on both the step-size and the momentum parameter, which enters in the rate through the link function \(\sigma\). This can be observed in the color plot of the asymptotic rate&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/rate_lazy_region-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/rate_lazy_region-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/rate_lazy_region-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/rate_lazy_region.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;knifes-edge&quot;&gt;Knife’s Edge&lt;/h2&gt; &lt;p&gt;The robust and lazy region occupy most (but not all!) of the region for which momentum converges. There’s a small region that sits between the lazy and robust regions and the region where momentum diverges. We call this region the &lt;i&gt;Knife’s edge&lt;/i&gt;&lt;/p&gt; &lt;p&gt;For parameters not in the robust or lazy region, we have that \(|\sigma(L)| &amp;gt; 1\) and \(|\sigma(L)| &amp;gt; |\sigma(\lmin)|\). Using the asymptotics of Chebyshev polynomials as we did in the previous section, we have that the asymptotic rate is \(\sqrt{\mom}\left(|\sigma(L)| + \sqrt{\sigma(L)^2 - 1} \right)\). The method will only converge when this asymptotic rate is below 1. Enforcing this results in \(\step \lt 2 (1 + \mom) / L\). Combining this condition with the one of not being in the robust or lazy region gives the characterization: \begin{equation} \step \lt \frac{2 (1 + \mom)}{L} \quad \text{ and } \quad \step \geq \max\Big\{\tfrac{2(1 + \mom)}{L + \lmin}, \tfrac{(1 + \sqrt{\mom})^2}{L}\Big\}\,. \end{equation}&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/sketch_knife_edge-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/sketch_knife_edge-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/sketch_knife_edge-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/sketch_knife_edge.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;asymptotic-rate-2&quot;&gt;Asymptotic rate&lt;/h3&gt; &lt;p&gt;The asymptotic rate can be computed using the same technique as in the lazy region. The resulting rate is the same as in that region but with \(\sigma(L)\) replacing \(\sigma(\lmin)\):&lt;/p&gt; &lt;p class=&quot;framed&quot; style=&quot;text-align: center&quot;&gt; In the Knife&apos;s edge region the asymptotic rate is \(\sqrt{\mom}\left(|\sigma(L)| + \sqrt{\sigma(L)^2 - 1} \right)\). &lt;/p&gt; &lt;p&gt;Pictorially, this corresponds to&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/rate_knife_edge-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/rate_knife_edge-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/rate_knife_edge-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/rate_knife_edge.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;putting-it-all-together&quot;&gt;Putting it All Together&lt;/h2&gt; &lt;p&gt;This is the end of our journey. We’ve visited all the regions on which momentum converges.&lt;d-footnote&gt;There&apos;s a small convergent region with &lt;i&gt;negative&lt;/i&gt; momentum parameter that we haven&apos;t visited. Although not typically used for minimization, negative momentum has found applications in smooth games &lt;a href=&quot;https://arxiv.org/abs/1807.04740&quot;&gt;(Gidel et al., 2020)&lt;/a&gt;.&lt;/d-footnote&gt; The only thing left to do is to combine all the asymptotic rates we’ve gathered along the way.&lt;/p&gt; &lt;p class=&quot;theorem&quot;&gt; The asymptotic rate \(\limsup_{t \to \infty} \sqrt[t]{r_t}\) of momentum is \begin{alignat}{2} &amp;amp;\sqrt{\mom} &amp;amp;&amp;amp;\text{ if }\step \in \big[\frac{(1 - \sqrt{\mom})^2}{\lmin}, \frac{(1+\sqrt{\mom})^2}{L}\big]\\ &amp;amp;\sqrt{\mom}(|\sigma(\lmin)| + \sqrt{\sigma(\lmin)^2 - 1}) &amp;amp;&amp;amp;\text{ if } \step \in \big[0, \min\{\tfrac{2(1 + \mom)}{L + \lmin}, \tfrac{(1 - \sqrt{\mom})^2}{\lmin}\}\big]\\ &amp;amp;\sqrt{\mom}(|\sigma(L)| + \sqrt{\sigma(L)^2 - 1})&amp;amp;&amp;amp;\text{ if } \step \in \big[\max\big\{\tfrac{2(1 + \mom)}{L + \lmin}, \tfrac{(1 + \sqrt{\mom})^2}{L}\big\}, \tfrac{2 (1 + \mom) }{L} \big)\\ &amp;amp;\geq 1 \text{ (divergence)} &amp;amp;&amp;amp; \text{ otherwise.} \end{alignat} &lt;/p&gt; &lt;p&gt;Plotting the asymptotic rates for all regions we can see that Polyak momentum (the method with momentum $\mom = \left(\frac{\sqrt{L} - \sqrt{\lmin}}{\sqrt{L} + \sqrt{\lmin}}\right)^2$ and step-size $\step = \left(\frac{2}{\sqrt{L} + \sqrt{\lmin}}\right)^2$ which is asymptotically optimal among the momentum methods with constant coefficients) is at the intersection of the three regions.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/rate_convergence_momentum-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/rate_convergence_momentum-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/rate_convergence_momentum-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-hitchhikers-momentum/rate_convergence_momentum.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;reproducibility&quot;&gt;Reproducibility&lt;/h2&gt; &lt;p&gt;All plots in this post were generated using the following Jupyer notebook: &lt;a href=&quot;/deep-learning-blog.github.io/assets/html/2022-12-01-hitchhikers-momentum/hitchhikers-momentum.html&quot;&gt;[HTML]&lt;/a&gt; &lt;a href=&quot;/deep-learning-blog.github.io/assets/html/2022-12-01-hitchhikers-momentum/hitchhikers-momentum.ipynb&quot;&gt;[IPYNB]&lt;/a&gt;&lt;/p&gt; </content> </entry> <entry> <title>Data Poisoning is Hitting a Wall</title> <link href="https://jocelynshen.com/blog/2022/facial-poisoning/"/> <updated>2022-12-01T00:00:00+00:00</updated> <id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/facial-poisoning</id> <content type="html">&lt;h2 id=&quot;overview-and-motivation&quot;&gt;Overview and Motivation&lt;/h2&gt; &lt;p&gt;To illustrate the data poisoning process, and to tie in with the paper below, let’s describe data poisoning against the backdrop of the facial recognition problem.&lt;/p&gt; &lt;p&gt;Facial recognition systems have been known to pose a severe threat to society. With unprecedented advancements in AI research, it is evident that this threat will be around for a while. There has been a steady increase in vendors offering facial recognition services for downstream applications — ranging from customer onboarding tools to criminal identification for police forces. The systems provided by these vendors are usually trained on images of users’ faces scraped from the Web. Ethical and moral concerns aside, this poses a considerable risk to the privacy of individuals.&lt;/p&gt; &lt;h4 id=&quot;what-is-data-poisoning&quot;&gt;What is Data Poisoning?&lt;/h4&gt; &lt;p&gt;Keeping this in mind, a growing body of work has emerged that allows users to fight back using principles from adversarial machine learning. Primary among these is the technique of data poisoning - where users can perturb pictures that they post online so that models that train on these become &lt;em&gt;poisoned&lt;/em&gt;. In other words, once a model has been introduced to a perturbed image of a user, it misidentifies any future instances of that person.&lt;/p&gt; &lt;p&gt;Services like &lt;em&gt;Fawkes&lt;/em&gt; popularized this approach by offering a service promising “strong protection against unauthorized {facial recognition} models.” Users could pass their images through Fawkes and receive poisoned photos - virtually identical to the naked eye, which were then posted to social media, alleviating any worries that they might be used to identify them in the future. It quickly gained popularity, was covered by the New York Times &lt;d-footnote&gt;[This tool could protect your photos from Facial Recognition](https://www.nytimes.com/2020/08/03/technology/fawkes-tool-protects-photos-from-facial-recognition.html)&lt;/d-footnote&gt; and received over 500,000 downloads. Following Fawkes’ success, similar systems were proposed in academic and commercial settings.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-facial-poisoning/facial_poisoning-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-facial-poisoning/facial_poisoning-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-facial-poisoning/facial_poisoning-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-facial-poisoning/facial_poisoning.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;hr /&gt; &lt;p&gt;The authors of the paper, however, look at these systems from a different perspective. They argue that services like Fawkes (and poisoning strategies in general) cannot protect users’ privacy when it comes to facial recognition systems. In fact, it usually exacerbates the situation by providing them with a false sense of security. For instance, there might have previously been a privacy-focused user who would have refrained from uploading their photos to the Internet. However, they might do so now under the false belief that their poisoned photos would work towards protecting their privacy. Thus, these users are now &lt;em&gt;less private&lt;/em&gt; than they were before.&lt;/p&gt; &lt;h2 id=&quot;why-doesnt-data-poisoning-work&quot;&gt;Why doesn’t data poisoning work?&lt;/h2&gt; &lt;p&gt;While data poisoning may have uses in other fields, such as healthcare, this post shows that it would not protect against facial recognition models. The main reason for this is due to a fundamental asymmetry between the users and the model trainers. Let us take the scenario described in the above figure. A user commits to an attack and uploads a perturbed image of themselves to the Web. This image eventually gets scraped by the model as part of its data collection strategy. In this case, the model trainer, or the vendors offering facial recognition services, now benefit from acting second. This provides them with two significant advantages:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;Since image poisoning systems cater to large user bases, these systems are usually made publicly accessible. This allows the model trainers to become aware of the technique used, which, in turn, helps them apply techniques to resist the poisoning attacks. This strategy of using alternate training techniques is known as an &lt;strong&gt;adaptive defense&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;As current poisoning attacks are designed to prevent &lt;em&gt;existing&lt;/em&gt; facial recognition tools from working, there is no reason to assume that future models will also be poisoned. So, trainers can simply wait a while and use newer models to keep identifying users, which would be invulnerable to poisoning attacks. This technique can (aptly) be named an &lt;strong&gt;oblivious defense&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Observant readers might equate this setting of continually evolving attack and defense tactics to an &lt;em&gt;arms race&lt;/em&gt;. However, since a perturbation applied to an image cannot be changed once scraped by the model, a successful attack has to remain effective against &lt;em&gt;all&lt;/em&gt; future models, even those trained adaptively against the attack. A better alternative to this would be pushing for legislation that restricts the use of privacy-invasive facial recognition systems.&lt;/p&gt; &lt;h2 id=&quot;high-level-idea&quot;&gt;High Level Idea&lt;/h2&gt; &lt;p&gt;We now look at the conclusions put forward in the excellent paper written by Radiya-Dixit &lt;em&gt;et al&lt;/em&gt;.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;An adaptive model trainer with black-box access to facial recognition systems like Fawkes can train a robust model that resists poisoning attacks and correctly identifies all users with high accuracy.&lt;/li&gt; &lt;li&gt;An adaptive model trainer can also repurpose this model to &lt;em&gt;detect&lt;/em&gt; perturbed pictures with near-perfect accuracy.&lt;/li&gt; &lt;li&gt;Image poisoning systems have already been broken by newer facial recognition that appeared less than a year after the attacks were introduced and employed superior training strategies.&lt;/li&gt; &lt;li&gt;It is possible to increase the robustness of a model (against poisoning attacks) without degrading its accuracy in identifying ‘clean’ images.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Let us take a closer look and deconstruct how the authors arrived at these conclusions.&lt;/p&gt; &lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt; &lt;p&gt;For clarity, before we arrive at the individual conclusions, we look at the setup used by the authors to carry out their experiments.&lt;/p&gt; &lt;p&gt;The authors evaluate three distinct poisoning attacks: &lt;strong&gt;Fawkes v0.3&lt;/strong&gt;, &lt;strong&gt;Fawkes v1.0&lt;/strong&gt;&lt;d-cite key=&quot;shan2020fawkes&quot;&gt;&lt;/d-cite&gt;, and a separate attack published at ICLR 2021 called &lt;strong&gt;LowKey&lt;/strong&gt;&lt;d-cite key=&quot;cherepanova2021lowkey&quot;&gt;&lt;/d-cite&gt;. All of these function on the same underlying principle of data poisoning. Their goal is to force the facial recognition model to associate an image with spurious features absent in unperturbed images.&lt;/p&gt; &lt;p&gt;The experiments are performed with the &lt;em&gt;FaceScrub&lt;/em&gt; dataset&lt;d-cite key=&quot;ng2014data&quot;&gt;&lt;/d-cite&gt;, which contains over 50,000 pictures of 530 celebrities. A sample run of an experimental procedure can be described as follows: A user, in this case, one of the celebrities in the &lt;em&gt;FaceScrub&lt;/em&gt; dataset, perturbs all of their images with &lt;em&gt;Fawkes&lt;/em&gt; or &lt;em&gt;LowKey&lt;/em&gt; in their strongest settings. These images then end up as the training data used by the model trainer. The model trainer uses the standard approach for training their facial recognition system by employing a pre-trained feature extractor to convert pictures into embeddings. Given a test image, the model tries to find a training example that minimizes the distance between them in the embedding space and returns the identity associated with the training example.&lt;/p&gt; &lt;p&gt;The authors use various models as feature extractors from &lt;em&gt;FaceNet&lt;/em&gt;&lt;d-cite key=&quot;schroff2015facenet&quot;&gt;&lt;/d-cite&gt; to OpenAI’s &lt;em&gt;CLIP&lt;/em&gt;&lt;d-cite key=&quot;radford2021learning&quot;&gt;&lt;/d-cite&gt;. This is an important step that helps quantify the effectiveness of the &lt;strong&gt;oblivious defense&lt;/strong&gt; strategy. ***&lt;/p&gt; &lt;h4 id=&quot;adaptive-defenses-break-facial-poisoning-attacks&quot;&gt;Adaptive defenses break facial poisoning attacks&lt;/h4&gt; &lt;p&gt;This section describes how the model trainer can adaptively train a generic feature extractor that can resist poisoning attacks.&lt;/p&gt; &lt;p&gt;The model trainer begins by collecting a public dataset of unperturbed images. In this case, that would be a canonical dataset of celebrities that are a part of the &lt;em&gt;FaceScrub&lt;/em&gt; dataset. With black-box access to the poisoning tool, the trainer calls it to obtain perturbed samples of the same images.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-facial-poisoning/adaptive-attack.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-facial-poisoning/adaptive-attack.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-facial-poisoning/adaptive-attack.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-facial-poisoning/adaptive-attack.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;With access to both unperturbed images and their corresponding poisoned counterparts, the trainer can teach a model to produce similar embeddings for both sets of pictures, encouraging the model to adaptively learn robust features. This is done hoping that this robustness would eventually generalize to perturbations’ applied to other images.&lt;/p&gt; &lt;blockquote&gt; While the above strategy works in theory, it requires direct intervention from model trainers by using the &apos;clean&apos; images provided by them. This would not scale well, especially for large-scale facial recognition systems that look at millions of photographs. However, this attack could also occur without the trainers&apos; explicit involvement. There is a high possibility that some users already have unperturbed images of themselves on the Web; either they forgot to perturb some pictures, or they were uploaded by someone else. Feature extractors trained on these pictures would then be encouraged to learn robust features. &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; All three attacks were evaluated against a non-robust &lt;em&gt;WebFace&lt;/em&gt; model to establish a baseline. They were found to have a misclassification rate of 55-77% for users who poisoned their pictures online. This compares starkly to a rate of 8% for unprotected users. However, when trained adaptively, the misclassification rate for all users - protected or unprotected - dropped to 5-8%, and all poisoning attacks were rendered ineffective. ***&lt;/p&gt; &lt;h4 id=&quot;attack-detection&quot;&gt;Attack Detection&lt;/h4&gt; &lt;p&gt;Since the model trainers have black-box access to the facial poisoning tools (&lt;em&gt;Fawkes&lt;/em&gt; and &lt;em&gt;LowKey&lt;/em&gt;), they can also turn the tables and build a detector to determine whether a specific image has been perturbed. Such a detector can dynamically filter out perturbed photos, allowing the model to retain only unperturbed pictures of a user. Moreover, detecting an attack could be a privacy concern (for instance, law enforcement might actively target users whose attack attempts are detected).&lt;/p&gt; &lt;p&gt;To verify this, the authors were able to fine-tune a standard pre-trained &lt;em&gt;ImageNet&lt;/em&gt; model to distinguish between perturbed and clean images of 25 random celebrities in the dataset. The model detected the poisoned images with near-perfect precision (99.8%) and recall (99.8%). ***&lt;/p&gt; &lt;h4 id=&quot;time-is-all-you-need&quot;&gt;Time is all you need&lt;/h4&gt; &lt;p&gt;Rather than creating poisoned counterparts to clean images and adaptively training a model, trainers have a much simpler alternative. They can simply wait for better facial recognition systems to be developed and then retroactively apply such a system to pictures they scraped in the past. &lt;em&gt;&lt;strong&gt;Simply put, facial poisoning attacks cannot withstand the test of time&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;To bypass this &lt;em&gt;oblivious&lt;/em&gt; defense strategy, an attack must not only be able to fool all present models but also be effective against future iterations without changing its perturbation. Asymetrically (to the benefit of the model trainer) newer techniques need not be robust to all attacks; instead, they just have to resist the specific method used in previous pictures.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-facial-poisoning/oblivious-attack.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-facial-poisoning/oblivious-attack.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-facial-poisoning/oblivious-attack.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-facial-poisoning/oblivious-attack.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;To confirm this, the paper included a study where &lt;em&gt;Fawkes&lt;/em&gt; was pitted against various feature extractors ordered chronologically. While the original &lt;em&gt;Fawkes v0.3&lt;/em&gt; was utterly ineffective against any model apart from &lt;em&gt;WebFace&lt;/em&gt;, the updated v1.0 could transfer its attack to other extractors like &lt;em&gt;VGGFace&lt;/em&gt;, &lt;em&gt;FaceNet&lt;/em&gt;, and &lt;em&gt;ArcFace&lt;/em&gt;. However, while &lt;em&gt;Fawkes v1.0&lt;/em&gt; provided a perfect (100%) error rate on the &lt;em&gt;Celeb1M&lt;/em&gt; model (the one it was trained to target), it failed miserably against more recent extractors like &lt;em&gt;MagFace&lt;/em&gt;&lt;d-cite key=&quot;meng2021magface&quot;&gt;&lt;/d-cite&gt; or &lt;em&gt;CLIP&lt;/em&gt;. A similar trend was also observed when using &lt;em&gt;LowKey&lt;/em&gt;. While it fared better than &lt;em&gt;Fawkes&lt;/em&gt; and could transfer its attack to MagSafe, LowKey failed to break the fine-tuned &lt;em&gt;CLIP&lt;/em&gt; model trained by the authors.&lt;/p&gt; &lt;p&gt;To provide more credence to their findings, the authors also illustrated how users who downloaded an older model (&lt;em&gt;Fawkes v0.3&lt;/em&gt;, for example) could not ‘regain’ their privacy by switching to an updated attack. For brevity, this post does not go into the specifics, but we encourage interested readers to look at the paper and additional supplementary material. ***&lt;/p&gt; &lt;h4 id=&quot;robustness-shouldnt-come-at-the-cost-of-accuracy&quot;&gt;Robustness shouldn’t come at the cost of accuracy&lt;/h4&gt; &lt;p&gt;A potential caveat for the &lt;em&gt;adaptive&lt;/em&gt; and &lt;em&gt;oblivious&lt;/em&gt; defenses is that increased robustness may come at the cost of decreased accuracy. For example, the CLIP model is much more robust than all the other feature extractors, but its clean accuracy falls slightly below the best models. In most cases, a trainer might be hesitant to deploy a &lt;em&gt;CLIP&lt;/em&gt;-based model if only a small minority of users try to attack the system.&lt;/p&gt; &lt;p&gt;Keeping this in mind, the authors demonstrated two approaches that allow model trainers to incorporate the best qualities of both worlds:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Top2:&lt;/strong&gt; This approach involved having a human in the loop. The authors propose that the system simply run the image through both models and return two candidate labels. To further streamline the process, the system could pass the image to the robust model only when the more accurate model cannot get a result. Humans could visually inspect these images to check for inconsistencies or determine if they were poisoned.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Confidence Thresholding:&lt;/strong&gt; To automate the above process, the system could begin by passing the image through the most accurate model and checking the prediction’s confidence. This can be quantitatively defined as the distance between the target picture and its nearest neighbor in the embedding space. If the system finds the confidence below a certain threshold, the image is passed through the robust model instead.&lt;/p&gt; &lt;p&gt;The paper demonstrates a facial recognition system that uses &lt;em&gt;MagFace&lt;/em&gt; for an accurate model and combines that with a more robust model like the fine-tuned &lt;em&gt;CLIP&lt;/em&gt; or an adaptively trained model. In both cases, the clean accuracy of the system matches or exceeds that of &lt;em&gt;MagFace&lt;/em&gt;, while retaining high robustness to attacks.&lt;/p&gt; &lt;hr /&gt; &lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt; &lt;p&gt;The main takeaway from this post is that data poisoning is no longer an effective method to protect users from facial recognition systems. The original premise for developing poisoning attacks was to facilitate an ‘arms race,’ where better attacks could counteract improved defenses. However, the people who deploy facial recognition models would always have the upper hand.&lt;/p&gt; &lt;p&gt;The paper shows that facial recognition models can be trained to detect and overcome poisoning attacks by simply having black-box access to a public-facing tool or just waiting for newer models and retroactively using them. To compete even against the latter category of systems, users would have to presume that minimal changes will be made to facial recognition models in the upcoming years. Given the state and pace of research in the field, that seems highly unlikely. ***&lt;/p&gt; &lt;h3 id=&quot;outlook&quot;&gt;Outlook&lt;/h3&gt; &lt;p&gt;This blog post provides a better understanding of the techniques used to neutralize the effects of data poisoning from the ICLR 2022 paper &lt;em&gt;Data Poisoning Won’t Save You from Facial Recognition.&lt;/em&gt; We hope that this has been of help to researchers and practitioners in the fields of adversarial ML.&lt;/p&gt; &lt;p&gt;We now look to provide some clarifications and how we think this work would fit in the current age of machine learning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The work is a net positive&lt;/strong&gt; This paper takes a gloomy stance on the current state of protection against facial recognition models. By stating that model trainers would always have the upper hand in the race by simply switching to a more advanced framework, the authors quash any possibility of a technological solution. Instead, they argue that a legislative approach might hold the key to solving the problem. Looking at the discussion between the authors and the reviewers before the acceptance of the paper &lt;d-footnote&gt;[ICLR OpenReview](https://openreview.net/forum?id=B5XahNLmna)&lt;/d-footnote&gt;, it was clear that the reviewers were reluctant to accept the finality of the solution - a sentiment we’re sure would be shared by many others. However, if nothing else, this paper warns users against the futility of using commercial products like Fawkes to protect their identities. In alleviating the false sense of security provided by data poisoning attacks, this paper - and, by extension, this post - serves as a net positive for users’ privacy.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Is legislation the answer?&lt;/strong&gt; With artificial intelligence embedding itself into society at an unprecedented rate, it is clear that a complete overhaul of legislative frameworks is urgently required. As AI becomes more mainstream, privacy-invasive systems could graduate from storing information to using them for financial incentives. While we have seen this happen with users’ browsing data, the repercussions of using biometrics would be much more severe. In fact, there have already been cases where facial recognition has been used by companies on users without their prior explicit consent. &lt;d-footnote&gt; [Madison Square Garden has put lawyers who represent people suing it on an &apos;exclusion list&apos; to keep them out of concerts and sporting events](https://www.nytimes.com/2022/12/22/nyregion/madison-square-garden-facial-recognition.html)&lt;/d-footnote&gt;&lt;/p&gt; &lt;p&gt;While we agree with the authors for a push towards proper legislation, given the rate of progress, we believe the community can do more. Legislation is a process that moves slowly and usually needs uniform implementation. Literature on the subject has shown that each country has its own views on the emerging landscape of AI &lt;d-footnote&gt;[How different countries view artificial intelligence](https://www.brookings.edu/research/how-different-countries-view-artificial-intelligence/)&lt;/d-footnote&gt; and bases its rules on those views. These may or may not always work. We believe a temporary stopgap in the form of a technological solution would be helpful, while a legislative solution holds maximum promise in the long run.&lt;/p&gt; &lt;hr /&gt; &lt;h3 id=&quot;tldr&quot;&gt;TL;DR&lt;/h3&gt; &lt;p&gt;This post broadly explores the ineffectiveness of data poisoning strategies against facial recognition models. It shows that commercial solutions like Fawkes and LowKey, which allow users to perturb their photos before posting them to social media, offer no protection to the users once their pictures are scraped.&lt;/p&gt; &lt;p&gt;It reveals that an ‘oblivious’ model trainer can simply wait long enough for future developments to nullify the effects of the perturbation. Or, since the people developing the facial recognition systems also have access to poisoning tools, they can simply develop strategies to detect and adapt to the perturbations.&lt;/p&gt; &lt;p&gt;Finally, given that there are no technical solutions to the problem, the best approach would be to push for legislation to counteract privacy-invasive facial recognition systems.&lt;/p&gt; &lt;hr /&gt; </content> </entry> <entry> <title>Sample Blog Post</title> <link href="https://jocelynshen.com/blog/2022/distill-example/"/> <updated>2022-12-01T00:00:00+00:00</updated> <id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/distill-example</id> <content type="html">&lt;h2 id=&quot;equations&quot;&gt;Equations&lt;/h2&gt; &lt;p&gt;This theme supports rendering beautiful math in inline and display modes using &lt;a href=&quot;https://www.mathjax.org/&quot;&gt;MathJax 3&lt;/a&gt; engine. You just need to surround your math expression with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$&lt;/code&gt;, like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$ E = mc^2 $$&lt;/code&gt;. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).&lt;/p&gt; &lt;p&gt;To use display mode, again surround your expression with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$&lt;/code&gt; and place it as a separate paragraph. Here is an example:&lt;/p&gt; \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] &lt;p&gt;Note that MathJax 3 is &lt;a href=&quot;https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html&quot;&gt;a major re-write of MathJax&lt;/a&gt; that brought a significant improvement to the loading and rendering speed, which is now &lt;a href=&quot;http://www.intmath.com/cg5/katex-mathjax-comparison.php&quot;&gt;on par with KaTeX&lt;/a&gt;.&lt;/p&gt; &lt;h2 id=&quot;images-and-figures&quot;&gt;Images and Figures&lt;/h2&gt; &lt;p&gt;Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:&lt;/p&gt; &lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{% include figure.html path=&quot;assets/img/2022-12-01-distill-example/iclr.png&quot; class=&quot;img-fluid&quot; %} &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;which results in the following image:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/iclr-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/iclr-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/iclr-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/iclr.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;To ensure that there are no namespace conflicts, you must save your asset to your unique directory &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/assets/img/2023-05-01-[SUBMISSION NAME]&lt;/code&gt; within your submission.&lt;/p&gt; &lt;p&gt;Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/9-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/9-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/9-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/9.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/7-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/7-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/7-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/7.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; A simple, elegant caption looks good between image rows, after each row, or doesn&apos;t have to be there at all. &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/8-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/8-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/8-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/8.jpg&quot; class=&quot;img-fluid z-depth-2&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/10-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/10-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/10-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/10.jpg&quot; class=&quot;img-fluid z-depth-2&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/11-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/11-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/11-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/11.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/12-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/12-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/12-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/12.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/7-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/7-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/7-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-distill-example/7.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;h3 id=&quot;interactive-figures&quot;&gt;Interactive Figures&lt;/h3&gt; &lt;p&gt;Here’s how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work (&lt;strong&gt;no extra javascript is allowed!&lt;/strong&gt;). All that’s required is for you to export your figure into HTML format, and make sure that the file exists in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;assets/html/[SUBMISSION NAME]/&lt;/code&gt; directory in this repository’s root directory. To embed it into any page, simply insert the following code anywhere into your page.&lt;/p&gt; &lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{% include [FIGURE_NAME].html %} &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;For example, the following code can be used to generate the figure underneath it.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plotly.express&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;density_mapbox&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Latitude&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Longitude&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Magnitude&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;radius&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;center&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;180&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zoom&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mapbox_style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;stamen-terrain&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;write_html&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;./assets/html/2022-12-01-distill-example/plotly_demo_1.html&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;And then include it with the following:&lt;/p&gt; &lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;l-page&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;&amp;lt;iframe&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{{ &apos;assets/html/2022-12-01-distill-example/plotly_demo_1.html&apos; | relative_url }}&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;frameborder=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;0&apos;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;scrolling=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;no&apos;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;height=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;600px&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;width=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;100%&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/iframe&amp;gt;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Voila!&lt;/p&gt; &lt;div class=&quot;l-page&quot;&gt; &lt;iframe src=&quot;/deep-learning-blog.github.io/assets/html/2022-12-01-distill-example/plotly_demo_1.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;600px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;h2 id=&quot;citations&quot;&gt;Citations&lt;/h2&gt; &lt;p&gt;Citations are then used in the article body with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;d-cite&amp;gt;&lt;/code&gt; tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.&lt;/p&gt; &lt;p&gt;The citation is presented inline like this: &lt;d-cite key=&quot;gregor2015draw&quot;&gt;&lt;/d-cite&gt; (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.&lt;/p&gt; &lt;p&gt;Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt; &lt;p&gt;Just wrap the text you would like to show up in a footnote in a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;d-footnote&amp;gt;&lt;/code&gt; tag. The number of the footnote will be automatically generated.&lt;d-footnote&gt;This will become a hoverable footnote.&lt;/d-footnote&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&quot;code-blocks&quot;&gt;Code Blocks&lt;/h2&gt; &lt;p&gt;This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:&lt;/p&gt; &lt;p&gt;{% highlight c++ linenos %} &lt;br /&gt; code code code &lt;br /&gt; {% endhighlight %}&lt;/p&gt; &lt;p&gt;The keyword &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;linenos&lt;/code&gt; triggers display of line numbers. You can try toggling it on or off yourself below:&lt;/p&gt; &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[])&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;myString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;input a string: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;getline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;myString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;myString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;charArray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;charArray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;myString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;charArray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt; &lt;hr /&gt; &lt;h2 id=&quot;diagrams&quot;&gt;Diagrams&lt;/h2&gt; &lt;p&gt;This theme supports generating various diagrams from a text description using &lt;a href=&quot;https://github.com/zhustec/jekyll-diagrams&quot; target=&quot;\_blank&quot;&gt;jekyll-diagrams&lt;/a&gt; plugin. Below, we generate a few examples of such diagrams using languages such as &lt;a href=&quot;https://mermaid-js.github.io/mermaid/&quot; target=&quot;\_blank&quot;&gt;mermaid&lt;/a&gt;, &lt;a href=&quot;https://plantuml.com/&quot; target=&quot;\_blank&quot;&gt;plantuml&lt;/a&gt;, &lt;a href=&quot;https://vega.github.io/vega-lite/&quot; target=&quot;\_blank&quot;&gt;vega-lite&lt;/a&gt;, etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; different diagram-generation packages require external dependencies to be installed on your machine. Also, be mindful of that because of diagram generation the fist time you build your Jekyll website after adding new diagrams will be SLOW. For any other details, please refer to &lt;a href=&quot;https://github.com/zhustec/jekyll-diagrams&quot; target=&quot;\_blank&quot;&gt;jekyll-diagrams&lt;/a&gt; README.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This is not supported for local rendering!&lt;/p&gt; &lt;p&gt;The diagram below was generated by the following code:&lt;/p&gt; &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{% mermaid %} sequenceDiagram participant John participant Alice Alice-&amp;gt;&amp;gt;John: Hello John, how are you? John--&amp;gt;&amp;gt;Alice: Great! {% endmermaid %} &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;div class=&quot;jekyll-diagrams diagrams mermaid&quot;&gt; &lt;svg id=&quot;mermaid-1697137176319&quot; width=&quot;100%&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;100%&quot; style=&quot;max-width:450px;&quot; viewBox=&quot;-50 -10 450 231&quot;&gt;&lt;style&gt;#mermaid-1697137176319 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1697137176319 .node circle,#mermaid-1697137176319 .node ellipse,#mermaid-1697137176319 .node polygon,#mermaid-1697137176319 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1697137176319 .node.clickable{cursor:pointer}#mermaid-1697137176319 .arrowheadPath{fill:#333}#mermaid-1697137176319 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1697137176319 .edgeLabel{background-color:#e8e8e8}#mermaid-1697137176319 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1697137176319 .cluster text{fill:#333}#mermaid-1697137176319 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1697137176319 .actor{stroke:#ccf;fill:#ececff}#mermaid-1697137176319 text.actor{fill:#000;stroke:none}#mermaid-1697137176319 .actor-line{stroke:grey}#mermaid-1697137176319 .messageLine0{marker-end:&quot;url(#arrowhead)&quot;}#mermaid-1697137176319 .messageLine0,#mermaid-1697137176319 .messageLine1{stroke-width:1.5;stroke-dasharray:&quot;2 2&quot;;stroke:#333}#mermaid-1697137176319 #arrowhead{fill:#333}#mermaid-1697137176319 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1697137176319 .messageText{fill:#333;stroke:none}#mermaid-1697137176319 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1697137176319 .labelText,#mermaid-1697137176319 .loopText{fill:#000;stroke:none}#mermaid-1697137176319 .loopLine{stroke-width:2;stroke-dasharray:&quot;2 2&quot;;marker-end:&quot;url(#arrowhead)&quot;;stroke:#ccf}#mermaid-1697137176319 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1697137176319 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1697137176319 .section{stroke:none;opacity:.2}#mermaid-1697137176319 .section0{fill:rgba(102,102,255,.49)}#mermaid-1697137176319 .section2{fill:#fff400}#mermaid-1697137176319 .section1,#mermaid-1697137176319 .section3{fill:#fff;opacity:.2}#mermaid-1697137176319 .sectionTitle0,#mermaid-1697137176319 .sectionTitle1,#mermaid-1697137176319 .sectionTitle2,#mermaid-1697137176319 .sectionTitle3{fill:#333}#mermaid-1697137176319 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1697137176319 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1697137176319 .grid path{stroke-width:0}#mermaid-1697137176319 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1697137176319 .task{stroke-width:2}#mermaid-1697137176319 .taskText{text-anchor:middle;font-size:11px}#mermaid-1697137176319 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1697137176319 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1697137176319 .taskText0,#mermaid-1697137176319 .taskText1,#mermaid-1697137176319 .taskText2,#mermaid-1697137176319 .taskText3{fill:#fff}#mermaid-1697137176319 .task0,#mermaid-1697137176319 .task1,#mermaid-1697137176319 .task2,#mermaid-1697137176319 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1697137176319 .taskTextOutside0,#mermaid-1697137176319 .taskTextOutside1,#mermaid-1697137176319 .taskTextOutside2,#mermaid-1697137176319 .taskTextOutside3{fill:#000}#mermaid-1697137176319 .active0,#mermaid-1697137176319 .active1,#mermaid-1697137176319 .active2,#mermaid-1697137176319 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1697137176319 .activeText0,#mermaid-1697137176319 .activeText1,#mermaid-1697137176319 .activeText2,#mermaid-1697137176319 .activeText3{fill:#000!important}#mermaid-1697137176319 .done0,#mermaid-1697137176319 .done1,#mermaid-1697137176319 .done2,#mermaid-1697137176319 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1697137176319 .doneText0,#mermaid-1697137176319 .doneText1,#mermaid-1697137176319 .doneText2,#mermaid-1697137176319 .doneText3{fill:#000!important}#mermaid-1697137176319 .crit0,#mermaid-1697137176319 .crit1,#mermaid-1697137176319 .crit2,#mermaid-1697137176319 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1697137176319 .activeCrit0,#mermaid-1697137176319 .activeCrit1,#mermaid-1697137176319 .activeCrit2,#mermaid-1697137176319 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1697137176319 .doneCrit0,#mermaid-1697137176319 .doneCrit1,#mermaid-1697137176319 .doneCrit2,#mermaid-1697137176319 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1697137176319 .activeCritText0,#mermaid-1697137176319 .activeCritText1,#mermaid-1697137176319 .activeCritText2,#mermaid-1697137176319 .activeCritText3,#mermaid-1697137176319 .doneCritText0,#mermaid-1697137176319 .doneCritText1,#mermaid-1697137176319 .doneCritText2,#mermaid-1697137176319 .doneCritText3{fill:#000!important}#mermaid-1697137176319 .titleText{text-anchor:middle;font-size:18px;fill:#000}#mermaid-1697137176319 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1697137176319 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1697137176319 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1697137176319 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1697137176319 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1697137176319 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1697137176319 #compositionEnd,#mermaid-1697137176319 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1697137176319 #aggregationEnd,#mermaid-1697137176319 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1697137176319 #dependencyEnd,#mermaid-1697137176319 #dependencyStart,#mermaid-1697137176319 #extensionEnd,#mermaid-1697137176319 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1697137176319 .branch-label,#mermaid-1697137176319 .commit-id,#mermaid-1697137176319 .commit-msg{fill:#d3d3d3;color:#d3d3d3}&lt;/style&gt;&lt;style&gt;#mermaid-1697137176319 { color: rgb(0, 0, 0); font: normal normal 400 normal 16px / normal &quot;Times New Roman&quot;; }&lt;/style&gt;&lt;g&gt;&lt;/g&gt;&lt;g&gt;&lt;line id=&quot;actor0&quot; x1=&quot;75&quot; y1=&quot;5&quot; x2=&quot;75&quot; y2=&quot;220&quot; class=&quot;actor-line&quot; stroke-width=&quot;0.5px&quot; stroke=&quot;#999&quot;&gt;&lt;/line&gt;&lt;rect x=&quot;0&quot; y=&quot;0&quot; fill=&quot;#eaeaea&quot; stroke=&quot;#666&quot; width=&quot;150&quot; height=&quot;65&quot; rx=&quot;3&quot; ry=&quot;3&quot; class=&quot;actor&quot;&gt;&lt;/rect&gt;&lt;text x=&quot;75&quot; y=&quot;32.5&quot; dominant-baseline=&quot;central&quot; alignment-baseline=&quot;central&quot; class=&quot;actor&quot; style=&quot;text-anchor: middle;&quot;&gt;&lt;tspan x=&quot;75&quot; dy=&quot;0&quot;&gt;John&lt;/tspan&gt;&lt;/text&gt;&lt;/g&gt;&lt;g&gt;&lt;line id=&quot;actor1&quot; x1=&quot;275&quot; y1=&quot;5&quot; x2=&quot;275&quot; y2=&quot;220&quot; class=&quot;actor-line&quot; stroke-width=&quot;0.5px&quot; stroke=&quot;#999&quot;&gt;&lt;/line&gt;&lt;rect x=&quot;200&quot; y=&quot;0&quot; fill=&quot;#eaeaea&quot; stroke=&quot;#666&quot; width=&quot;150&quot; height=&quot;65&quot; rx=&quot;3&quot; ry=&quot;3&quot; class=&quot;actor&quot;&gt;&lt;/rect&gt;&lt;text x=&quot;275&quot; y=&quot;32.5&quot; dominant-baseline=&quot;central&quot; alignment-baseline=&quot;central&quot; class=&quot;actor&quot; style=&quot;text-anchor: middle;&quot;&gt;&lt;tspan x=&quot;275&quot; dy=&quot;0&quot;&gt;Alice&lt;/tspan&gt;&lt;/text&gt;&lt;/g&gt;&lt;defs&gt;&lt;marker id=&quot;arrowhead&quot; refX=&quot;5&quot; refY=&quot;2&quot; markerWidth=&quot;6&quot; markerHeight=&quot;4&quot; orient=&quot;auto&quot;&gt;&lt;path d=&quot;M 0,0 V 4 L6,2 Z&quot;&gt;&lt;/path&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;defs&gt;&lt;marker id=&quot;crosshead&quot; markerWidth=&quot;15&quot; markerHeight=&quot;8&quot; orient=&quot;auto&quot; refX=&quot;16&quot; refY=&quot;4&quot;&gt;&lt;path fill=&quot;black&quot; stroke=&quot;#000000&quot; stroke-width=&quot;1px&quot; d=&quot;M 9,2 V 6 L16,4 Z&quot; style=&quot;stroke-dasharray: 0, 0;&quot;&gt;&lt;/path&gt;&lt;path fill=&quot;none&quot; stroke=&quot;#000000&quot; stroke-width=&quot;1px&quot; d=&quot;M 0,1 L 6,7 M 6,1 L 0,7&quot; style=&quot;stroke-dasharray: 0, 0;&quot;&gt;&lt;/path&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;g&gt;&lt;text x=&quot;175&quot; y=&quot;93&quot; class=&quot;messageText&quot; style=&quot;text-anchor: middle;&quot;&gt;Hello John, how are you?&lt;/text&gt;&lt;line x1=&quot;275&quot; y1=&quot;100&quot; x2=&quot;75&quot; y2=&quot;100&quot; class=&quot;messageLine0&quot; stroke-width=&quot;2&quot; stroke=&quot;black&quot; marker-end=&quot;url(#arrowhead)&quot; style=&quot;fill: none;&quot;&gt;&lt;/line&gt;&lt;/g&gt;&lt;g&gt;&lt;text x=&quot;175&quot; y=&quot;128&quot; class=&quot;messageText&quot; style=&quot;text-anchor: middle;&quot;&gt;Great!&lt;/text&gt;&lt;line x1=&quot;75&quot; y1=&quot;135&quot; x2=&quot;275&quot; y2=&quot;135&quot; class=&quot;messageLine1&quot; stroke-width=&quot;2&quot; stroke=&quot;black&quot; marker-end=&quot;url(#arrowhead)&quot; style=&quot;stroke-dasharray: 3, 3; fill: none;&quot;&gt;&lt;/line&gt;&lt;/g&gt;&lt;g&gt;&lt;rect x=&quot;0&quot; y=&quot;155&quot; fill=&quot;#eaeaea&quot; stroke=&quot;#666&quot; width=&quot;150&quot; height=&quot;65&quot; rx=&quot;3&quot; ry=&quot;3&quot; class=&quot;actor&quot;&gt;&lt;/rect&gt;&lt;text x=&quot;75&quot; y=&quot;187.5&quot; dominant-baseline=&quot;central&quot; alignment-baseline=&quot;central&quot; class=&quot;actor&quot; style=&quot;text-anchor: middle;&quot;&gt;&lt;tspan x=&quot;75&quot; dy=&quot;0&quot;&gt;John&lt;/tspan&gt;&lt;/text&gt;&lt;/g&gt;&lt;g&gt;&lt;rect x=&quot;200&quot; y=&quot;155&quot; fill=&quot;#eaeaea&quot; stroke=&quot;#666&quot; width=&quot;150&quot; height=&quot;65&quot; rx=&quot;3&quot; ry=&quot;3&quot; class=&quot;actor&quot;&gt;&lt;/rect&gt;&lt;text x=&quot;275&quot; y=&quot;187.5&quot; dominant-baseline=&quot;central&quot; alignment-baseline=&quot;central&quot; class=&quot;actor&quot; style=&quot;text-anchor: middle;&quot;&gt;&lt;tspan x=&quot;275&quot; dy=&quot;0&quot;&gt;Alice&lt;/tspan&gt;&lt;/text&gt;&lt;/g&gt;&lt;/svg&gt; &lt;/div&gt; &lt;hr /&gt; &lt;h2 id=&quot;tweets&quot;&gt;Tweets&lt;/h2&gt; &lt;p&gt;An example of displaying a tweet:&lt;/p&gt; &lt;div class=&quot;jekyll-twitter-plugin&quot;&gt;&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;sv&quot; dir=&quot;ltr&quot;&gt;jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API &lt;a href=&quot;http://t.co/m4EIQPM9h4&quot;&gt;http://t.co/m4EIQPM9h4&lt;/a&gt;&lt;/p&gt;&amp;mdash; RubyGems (@rubygems) &lt;a href=&quot;https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw&quot;&gt;October 5, 2014&lt;/a&gt;&lt;/blockquote&gt; &lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;/div&gt; &lt;p&gt;An example of pulling from a timeline:&lt;/p&gt; &lt;div class=&quot;jekyll-twitter-plugin&quot;&gt;&lt;a class=&quot;twitter-timeline&quot; data-width=&quot;500&quot; data-tweet-limit=&quot;3&quot; href=&quot;https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw&quot;&gt;Tweets by jekyllrb&lt;/a&gt; &lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;/div&gt; &lt;p&gt;For more details on using the plugin visit: &lt;a href=&quot;https://github.com/rob-murray/jekyll-twitter-plugin&quot;&gt;jekyll-twitter-plugin&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&quot;blockquotes&quot;&gt;Blockquotes&lt;/h2&gt; &lt;blockquote&gt; We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin &lt;/blockquote&gt; &lt;hr /&gt; &lt;h2 id=&quot;layouts&quot;&gt;Layouts&lt;/h2&gt; &lt;p&gt;The main text column is referred to as the body. It is the assumed layout of any direct descendants of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d-article&lt;/code&gt; element.&lt;/p&gt; &lt;div class=&quot;fake-img l-body&quot;&gt; &lt;p&gt;.l-body&lt;/p&gt; &lt;/div&gt; &lt;p&gt;For images you want to display a little larger, try &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.l-page&lt;/code&gt;:&lt;/p&gt; &lt;div class=&quot;fake-img l-page&quot;&gt; &lt;p&gt;.l-page&lt;/p&gt; &lt;/div&gt; &lt;p&gt;All of these have an outset variant if you want to poke out from the body text a little bit. For instance:&lt;/p&gt; &lt;div class=&quot;fake-img l-body-outset&quot;&gt; &lt;p&gt;.l-body-outset&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;fake-img l-page-outset&quot;&gt; &lt;p&gt;.l-page-outset&lt;/p&gt; &lt;/div&gt; &lt;p&gt;Occasionally you’ll want to use the full browser width. For this, use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.l-screen&lt;/code&gt;. You can also inset the element a little from the edge of the browser by using the inset variant.&lt;/p&gt; &lt;div class=&quot;fake-img l-screen&quot;&gt; &lt;p&gt;.l-screen&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;fake-img l-screen-inset&quot;&gt; &lt;p&gt;.l-screen-inset&lt;/p&gt; &lt;/div&gt; &lt;p&gt;The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.l-body&lt;/code&gt; sized text except on mobile screen sizes.&lt;/p&gt; &lt;div class=&quot;fake-img l-gutter&quot;&gt; &lt;p&gt;.l-gutter&lt;/p&gt; &lt;/div&gt; &lt;hr /&gt; &lt;h2 id=&quot;other-typography&quot;&gt;Other Typography?&lt;/h2&gt; &lt;p&gt;Emphasis, aka italics, with &lt;em&gt;asterisks&lt;/em&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*asterisks*&lt;/code&gt;) or &lt;em&gt;underscores&lt;/em&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_underscores_&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;Strong emphasis, aka bold, with &lt;strong&gt;asterisks&lt;/strong&gt; or &lt;strong&gt;underscores&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Combined emphasis with &lt;strong&gt;asterisks and &lt;em&gt;underscores&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Strikethrough uses two tildes. &lt;del&gt;Scratch this.&lt;/del&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;First ordered list item&lt;/li&gt; &lt;li&gt;Another item ⋅⋅* Unordered sub-list.&lt;/li&gt; &lt;li&gt;Actual numbers don’t matter, just that it’s a number ⋅⋅1. Ordered sub-list&lt;/li&gt; &lt;li&gt;And another item.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).&lt;/p&gt; &lt;p&gt;⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅ ⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅ ⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unordered list can use asterisks&lt;/li&gt; &lt;li&gt;Or minuses&lt;/li&gt; &lt;li&gt;Or pluses&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href=&quot;https://www.google.com&quot;&gt;I’m an inline-style link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.google.com&quot; title=&quot;Google&apos;s Homepage&quot;&gt;I’m an inline-style link with title&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.mozilla.org&quot;&gt;I’m a reference-style link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;../blob/master/LICENSE&quot;&gt;I’m a relative reference to a repository file&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;http://slashdot.org&quot;&gt;You can use numbers for reference-style link definitions&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Or leave it empty and use the &lt;a href=&quot;http://www.reddit.com&quot;&gt;link text itself&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or &lt;a href=&quot;http://www.example.com&quot;&gt;http://www.example.com&lt;/a&gt; and sometimes example.com (but not on Github, for example).&lt;/p&gt; &lt;p&gt;Some text to show that the reference links can follow later.&lt;/p&gt; &lt;p&gt;Here’s our logo (hover to see the title text):&lt;/p&gt; &lt;p&gt;Inline-style: &lt;img src=&quot;https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png&quot; alt=&quot;alt text&quot; title=&quot;Logo Title Text 1&quot; /&gt;&lt;/p&gt; &lt;p&gt;Reference-style: &lt;img src=&quot;https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png&quot; alt=&quot;alt text&quot; title=&quot;Logo Title Text 2&quot; /&gt;&lt;/p&gt; &lt;p&gt;Inline &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;code&lt;/code&gt; has &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;back-ticks around&lt;/code&gt; it.&lt;/p&gt; &lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;JavaScript syntax highlighting&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;alert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Python syntax highlighting&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;No language indicated, so no syntax highlighting. But let&apos;s throw in a &amp;lt;b&amp;gt;tag&amp;lt;/b&amp;gt;. &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Colons can be used to align columns.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Tables&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;Are&lt;/th&gt; &lt;th style=&quot;text-align: right&quot;&gt;Cool&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;col 3 is&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;right-aligned&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;$1600&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;col 2 is&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;centered&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;$12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;zebra stripes&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;are neat&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;$1&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Markdown&lt;/th&gt; &lt;th&gt;Less&lt;/th&gt; &lt;th&gt;Pretty&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;em&gt;Still&lt;/em&gt;&lt;/td&gt; &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;renders&lt;/code&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;nicely&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;blockquote&gt; &lt;p&gt;Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Quote break.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can &lt;em&gt;put&lt;/em&gt; &lt;strong&gt;Markdown&lt;/strong&gt; into a blockquote.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Here’s a line for us to start with.&lt;/p&gt; &lt;p&gt;This line is separated from the one above by two newlines, so it will be a &lt;em&gt;separate paragraph&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the &lt;em&gt;same paragraph&lt;/em&gt;.&lt;/p&gt; </content> </entry> <entry> <title>Strategies for Classification Layer Initialization in Model-Agnostic Meta-Learning</title> <link href="https://jocelynshen.com/blog/2022/classification-layer-initialization-in-maml/"/> <updated>2022-12-01T00:00:00+00:00</updated> <id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/classification-layer-initialization-in-maml</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;In a previous study, Raghu et al. [2020] &lt;d-cite key=&quot;DBLP:conf/iclr/RaghuRBV20&quot;&gt;&lt;/d-cite&gt; found that in model-agnostic meta-learning (MAML) for few-shot classification, the majority of changes observed in the network during the inner loop fine-tuning process occurred in the linear classification head. It is commonly believed that during this phase, the linear head remaps encoded features to the classes of the new task. In traditional MAML, the weights of the final linear layer are meta-learned in the usual way. However, there are some issues with this approach:&lt;/p&gt; &lt;p&gt;First, it is difficult to imagine that a single set of optimal weights can be learned. This becomes apparent when considering class label permutations: two different tasks may have the same classes but in a different order. As a result, the weights that perform well for the first task will likely not be effective for the second task. This is reflected in the fact that MAML’s performance can vary by up to 15% depending on the class label assignments during testing &lt;d-cite key=&quot;DBLP:conf/iclr/YeC22&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Second, more challenging datasets are being proposed as few-shot learning benchmarks, such as Meta-Dataset &lt;d-cite key=&quot;DBLP:conf/iclr/TriantafillouZD20&quot;&gt;&lt;/d-cite&gt;. These datasets have varying numbers of classes per task, making it impossible to learn a single set of weights for the classification layer.&lt;/p&gt; &lt;p&gt;Therefore, it seems logical to consider how to initialize the final classification layer before fine-tuning on a new task. Random initialization may not be optimal, as it can introduce unnecessary noise &lt;d-cite key=&quot;DBLP:conf/iclr/KaoCC22&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;This blog post will discuss different approaches to last layer initialization that claim to outperform the original MAML method.&lt;/p&gt; &lt;h2 id=&quot;quick-recap-on-maml&quot;&gt;Quick recap on MAML&lt;/h2&gt; &lt;p&gt;Model-Agnostic Meta-Learning (MAML) &lt;d-cite key=&quot;DBLP:conf/icml/FinnAL17&quot;&gt;&lt;/d-cite&gt; is a well-established algorithm in the field of optimization-based meta-learning. Its goal is to find parameters $\theta$ for a parametric model $f_{\theta}$ that can be efficiently adapted to perform an unseen task from the same task distribution, using only a few training examples. The pre-training of $\theta$ is done using two nested loops (bi-level optimization), with meta-training occurring in the outer loop and task-specific fine-tuning in the inner loop. The task-specific fine-tuning is typically done using a few steps of gradient descent:&lt;/p&gt; \[\theta_{i}&apos; = \theta - \alpha\nabla_{\theta}\mathcal{L_{\mathcal{T_{i}}}}(\theta, \mathcal{D^{tr}})\] &lt;p&gt;where $\alpha$ is the inner loop learning rate, $\mathcal{L_{\mathcal{T_{i}}}}$ is a task’s loss function, and $\mathcal{D^{tr}}$ is a task’s training set. The task includes a test set as well: $\mathcal{T_{i}} = (\mathcal{D_{i}^{tr}}, \mathcal{D_{i}^{test}})$.&lt;/p&gt; &lt;p&gt;In the outer loop, the meta parameter $\theta$ is updated by backpropagating through the inner loop to reduce errors made on the tasks’ test set using the fine-tuned parameters:&lt;/p&gt; \[\theta&apos; = \theta - \eta\nabla_{\theta} \sum_{\mathcal{T_{i}} \sim p(\mathcal{T})}^{} \mathcal{L_{\mathcal{T_{i}}}}(\theta_{i}&apos;, \mathcal{D^{test}}).\] &lt;p&gt;Here, $\eta$ is the meta-learning rate. The differentiation through the inner loop involves calculating second-order derivatives, which mainly distinguishes MAML from simply optimizing for a $\theta$ that minimizes the average task loss.&lt;/p&gt; &lt;p&gt;It is worth noting that in practical scenarios, this second-order differentiation is computationally expensive, and approximation methods such as first-order MAML (FOMAML) &lt;d-cite key=&quot;DBLP:conf/icml/FinnAL17&quot;&gt;&lt;/d-cite&gt; or Reptile &lt;d-cite key=&quot;DBLP:journals/corr/abs-1803-02999&quot;&gt;&lt;/d-cite&gt; are often used. In FOMAML, the outer loop update is simply: \(\theta&apos; = \theta - \eta\nabla_{\theta&apos;} \sum_{\mathcal{T_{i}} \sim p(\mathcal{T})}^{}\mathcal{L_{\mathcal{T_{i}}}}(\theta_{i}&apos;, \mathcal{D^{test}})\), which avoids differentiating through the inner loop.&lt;/p&gt; &lt;p&gt;Before proceeding, let’s prepare ourselves for the next sections by looking at the notation we can use when discussing MAML in the few-shot classification regime: The model’s output prediction can be described as $\hat{y} = f_{\theta}(\mathbf{x}) = \underset{c\in[N]}{\mathrm{argmax}} ; h_{\mathbf{w}} (g_{\phi}(\mathbf{x}), c)$, where we divide our model $f_{\theta}(\mathbf{x})$ (which takes an input $\mathbf{x}$) into a feature extractor $g_{\phi}(\mathbf{x})$ and a classifier $h_\mathbf{w}(\mathbf{r}, c)$, which is parameterized by classification head weight vectors ${\mathbf{w}}_{c=1}^N$. $\mathbf{r}$ denotes an input’s representation, and $c$ is the index of the class we want the output prediction for.&lt;/p&gt; &lt;p&gt;Finally, $\theta = {\mathbf{w_1}, \mathbf{w_1}, …, \mathbf{w_N}, \phi}$, and we are consistent with our previous notation.&lt;/p&gt; &lt;h2 id=&quot;learning-a-single-initialization-vector&quot;&gt;Learning a single initialization vector&lt;/h2&gt; &lt;p&gt;The first two variants of MAML - we look at - approach the initialization task by initializing the classification head weight vectors uniformly for all classes. In the paper&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;   ▶  &lt;/span&gt;Han-Jia Ye &amp;amp; Wei-Lun Chao (ICLR, 2022) How to train your MAML to excel in few-shot classification &lt;d-cite key=&quot;DBLP:conf/iclr/YeC22&quot;&gt;&lt;/d-cite&gt;,&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;an approach called &lt;strong&gt;UnicornMAML&lt;/strong&gt; is presented. It is explicitly motivated by the effect that different class-label assignments can have. Ye &amp;amp; Chao [2022] &lt;d-cite key=&quot;DBLP:conf/iclr/YeC22&quot;&gt;&lt;/d-cite&gt; report that during testing, vanilla MAML can perform very differently for &lt;ins&gt;tasks with the same set of classes&lt;/ins&gt;, which are just &lt;ins&gt;differently ordered&lt;/ins&gt;. Namely, they report that classification accuracy can vary up to 15% in the one-shot setting and up to 8% in the five-shot setting. This makes MAMLs performance quite unstable. &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-classification-layer-initialization-in-maml/perm_final-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-classification-layer-initialization-in-maml/perm_final-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-classification-layer-initialization-in-maml/perm_final-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-classification-layer-initialization-in-maml/perm_final.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p align=&quot;center&quot;&gt; &lt;em&gt;Fig.1 Example of MAML and a class label permutation. We can see the randomness introduced, as $\mathbf{w_1}$ is supposed to interpret the input features as &quot;unicorn&quot; for the first task, and as &quot;bee&quot; for the second. For both tasks, the class outputted as a prediction should be the same, as in human perception, both tasks are identical. This, however, is obviously not the case.&lt;/em&gt; &lt;/p&gt; &lt;p&gt;The solution proposed is fairly simple: Instead of meta-learning $N$ weight vectors for the final layer, only a &lt;ins&gt;single vector&lt;/ins&gt; $\mathbf{w}$ is meta-learned and used to initialize all $ \{ \mathbf{w} \}_{c=1}^N $ before the fine-tuning stage.&lt;/p&gt; &lt;p&gt;This forces the model to make random predictions before the inner loop, as $\hat{y_c}= h_{\mathbf{w}} (g_{\phi} (\mathbf{x}), c)$ will be the same for all $c \in [1,…,N ]$.&lt;/p&gt; &lt;p&gt;After the inner loop, the updated parameters have been computed as usual: \(\theta&apos; = \\{\mathbf{w_1}&apos;, \mathbf{w_2}&apos;, ..., \mathbf{w_N}&apos;, \phi&apos;\\}\). The gradient for updating the single classification head meta weight vector $\mathbf{w}$, is just the aggregation of the gradients w.r.t. all the single $\mathbf{w_c}$:&lt;/p&gt; \[\nabla_{\mathbf{w}} \mathcal{L_{\mathcal{T_i}}} (\mathcal{D^{test}}, \theta_i) = \sum_{c \in [N]} \nabla_{\mathbf{w_c}} \mathcal{L_{\mathcal{T_i}}} (\theta_i, \mathcal{D^{test}})\] &lt;p&gt;This collapses the models meta-parameters to $ \theta = \{\mathbf{w}, \phi\} $. &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-classification-layer-initialization-in-maml/unicorn_maml_final-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-classification-layer-initialization-in-maml/unicorn_maml_final-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-classification-layer-initialization-in-maml/unicorn_maml_final-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-classification-layer-initialization-in-maml/unicorn_maml_final.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p align=&quot;center&quot;&gt; &lt;em&gt;Fig.2 Overview of UnicornMAML. We can see that class label permutations don&apos;t matter anymore, as before fine-tuning, the probability of predicting each class is the same.&lt;/em&gt; &lt;/p&gt; &lt;p&gt;This tweak to vanilla MAML makes UnicornMAML permutation invariant, as models fine-tuned on tasks including the same categories - just differently ordered - will now yield the same output predictions. Also, the method could be used with datasets where the number of classes varies without any further adaptation: It doesn’t matter how many classification head weight vectors are initialized by the single meta-classification head weight vector.&lt;/p&gt; &lt;p&gt;Furthermore, the uniform initialization in Unicorn-MAML addresses the problem of memorization overfitting &lt;d-cite key=&quot;DBLP:conf/iclr/YinTZLF20&quot;&gt;&lt;/d-cite&gt;. The phenomenon describes a scenario where a single model can learn all the training tasks only from the test data in the outer loop. This leads to a model that learns to perform the training tasks but also to a model that doesn’t do any fine-tuning and thus fails to generalize to unseen tasks. Again, the uniform initialization of the classification head for all classes forces the model to adapt during fine-tuning and thus prevents memorization overfitting.&lt;/p&gt; &lt;p&gt;The approach is reported to perform on par with recent few-shot algorithms.&lt;/p&gt; &lt;p&gt;Let’s finally think of how to interpret UnicornMAML: When meta-learning only a single classification head vector, one could say that not a mapping from features to classes is tried to be learned any more, but a prioritization of features, which seemed to be more relevant for the classification decision across tasks, than others.&lt;/p&gt; &lt;h2 id=&quot;zero-initialization&quot;&gt;Zero initialization&lt;/h2&gt; &lt;p&gt;The second approach for a uniform initialization is proposed in the paper&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;   ▶  &lt;/span&gt;Chia-Hsiang Kao et al. (ICLR, 2022) MAML is a Noisy Contrastive Learner in Classification &lt;d-cite key=&quot;DBLP:conf/iclr/KaoCC22&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;Kao et al. [2022] &lt;d-cite key=&quot;DBLP:conf/iclr/KaoCC22&quot;&gt;&lt;/d-cite&gt; modify the original MAML by setting the whole classification head to zero before each inner loop. They refer to this MAML-tweak as the &lt;strong&gt;zeroing trick&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;An overview of MAML with the zeroing trick is displayed below:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-classification-layer-initialization-in-maml/zeroing_trick-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-classification-layer-initialization-in-maml/zeroing_trick-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-classification-layer-initialization-in-maml/zeroing_trick-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-classification-layer-initialization-in-maml/zeroing_trick.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p align=&quot;center&quot;&gt; &lt;em&gt;Fig.3 MAML with the zeroing trick applied.&lt;/em&gt; &lt;/p&gt; &lt;p&gt;Note that $S_n$ and $Q_n$ refer to $\mathcal{D_{i}^{tr}}$ and $\mathcal{D_{i}^{test}}$ in this notation.&lt;/p&gt; &lt;p&gt;Through applying the zero initialization, three of the problems addressed by UnicornMAML are solved as well:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MAML with the zeroing trick applied leads to random predictions before fine-tuning. This solves the problem of class label assignment permutations during testing.&lt;/li&gt; &lt;li&gt;Through the random predictions before fine-tuning, memorization overfitting is prevented as well.&lt;/li&gt; &lt;li&gt;The zeroing trick makes MAML applicable for datasets with a varying number of classes per task.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Interestingly, the motivation for applying the zeroing trick, stated by Kao et al. [2022] &lt;d-cite key=&quot;DBLP:conf/iclr/KaoCC22&quot;&gt;&lt;/d-cite&gt;, is entirely different. In general, Kao et al. [2022] &lt;d-cite key=&quot;DBLP:conf/iclr/KaoCC22&quot;&gt;&lt;/d-cite&gt; want to unveil in what sense MAML encourages its models to learn general-purpose feature representations. They show that under some assumptions, there is a supervised contrastive learning (SCL) objective underlying MAML. In SCL, the label information is leveraged by pulling embeddings belonging to the same class closer together while increasing the embedding distances of samples from different classes &lt;d-cite key=&quot;DBLP:conf/nips/KhoslaTWSTIMLK20&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;More specifically, they show that the outer-loop update for the encoder follows a noisy SCL loss under the following assumptions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The encoder weights are frozen in the inner loop (EFIL assumption)&lt;/li&gt; &lt;li&gt;There is only a single inner loop update step.&lt;d-footnote&gt;Note that FOMAML technically follows a noisy SCL loss without this assumption. However, when applying the zeroing trick, this assumption is needed again for stating that the encoder update is following an SCL loss&lt;/d-footnote&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;A noisy SCL loss means that cases can occur where the loss forces the model to maximize similarities between embeddings from samples of different classes. The outer-loop encoder loss in this setting contains an “interference term” which causes the model to pull together embeddings from different tasks or to pull embeddings into a random direction, with the randomness being introduced by random initialization of the classification head. Those two phenomena are termed &lt;em&gt;cross-task interference&lt;/em&gt; and &lt;em&gt;initialization interference&lt;/em&gt;. Noise and interference in the loss vanish when applying the zeroing trick, and the outer-loop encoder loss turns into a proper SCL loss. Meaning that minimizing this loss forces embeddings of the same class/task together while pushing embeddings from the same task and different classes apart. A decent increase in performance is observed for MAML with the zeroing trick compared to vanilla MAML.&lt;/p&gt; &lt;p&gt;Those findings are derived using a general formulation of MAML, with a cross-entropy loss, and the details are available in the paper &lt;d-cite key=&quot;DBLP:conf/iclr/KaoCC22&quot;&gt;&lt;/d-cite&gt;. Also, a slightly simpler example is stated to give an intuition of MAMLs SCL properties. We will briefly summarize it in the following to share this intuition with you.&lt;/p&gt; &lt;h3 id=&quot;mamls-scl-intuition&quot;&gt;MAMLs SCL Intuition&lt;/h3&gt; &lt;p&gt;To get an intuition of how MAML relates to SCL, let’s look at the following setup: an N-way one-shot classification task using MAML with Mean Squared Error (MSE) between the one-hot encoded class label and the prediction of the model. Furthermore, the EFIL assumption is made, the zeroing trick is applied, only a single inner loop update step is used, and only a single task is sampled per batch.&lt;/p&gt; &lt;p&gt;In this setting, the classification heads inner-loop update for a single datapoint looks like this:&lt;/p&gt; \[\mathbf{w}&apos; = \mathbf{w} - \alpha (-g_{\phi} (\mathbf{x}_{1}^{tr}) \mathbf{t}_{1}^{tr\top})\] &lt;p&gt;$\mathbf{t}_1^{tr}$ refers to the one-hot encoded class label belonging to $\mathbf{x}_1^{tr}$. In words, the features extracted for training example $\mathbf{x}_1^{tr}$ are added to column $\mathbf{w}_c$, with $c$ being the index of 1 in $\mathbf{t}_1^{tr}$. For multiple examples, the features of all training examples labeled with class $c$ are added to the $c^{th}$ column of $\mathbf{w}$.&lt;/p&gt; &lt;p&gt;Now, for calculating the model’s output in the outer loop, the model computes the dot products of the columns \(\\{\mathbf{w} \\}_{c=1}^N\) and the encoded test examples \(g_{\phi}(\mathbf{x}_1^{test})\) (and takes a softmax afterward.) To match the one-hot encoded label as good as possible, the dot product has to be large when \(\mathbf{t}_1^{test}\) = \(1\) at index \(c\), and small otherwise. We can see that the loss enforces embedding similarity for features from the same classes while enforcing dissimilarity for embeddings from different classes, which fits the SCL objective.&lt;/p&gt; &lt;h2 id=&quot;initialization-using-prototypes&quot;&gt;Initialization using prototypes&lt;/h2&gt; &lt;p&gt;A more sophisticated approach for last-layer initialization in MAML is introduced in the paper&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;   ▶  &lt;/span&gt;Eleni Triantafillou et al. (ICLR, 2020) Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples &lt;d-cite key=&quot;DBLP:conf/iclr/TriantafillouZD20&quot;&gt;&lt;/d-cite&gt; .&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;As one might guess from the name, &lt;strong&gt;Proto-MAML&lt;/strong&gt; makes use of Prototypical Networks (PNs) for enhancing MAML. Opposite to the two initialization strategies presented above, Proto-MAML does not uniformly initialize the classification head weights before each inner loop for all classes. Instead, it calculates class-specific initialization vectors based on the training examples. This solves some of the problems mentioned earlier (see &lt;a href=&quot;#conclusion--discussion&quot;&gt;Conclusion &amp;amp; Discussion&lt;/a&gt;), but also it adds another type of logic to the classification layer.&lt;/p&gt; &lt;p&gt;Let’s revise how PNs work when used for few-shot learning for understanding Proto-MAML afterward:&lt;/p&gt; &lt;p&gt;Class prototypes \(\mathbf{c}_{c}\) are computed by averaging over train example embeddings of each class, created by a feature extractor \(g_{\phi}(\mathbf{x})\). For classifying a test example, a softmax over the distances (e.g., squared euclidean distance) between class prototypes \(\mathbf{c}_{c}\) and example embeddings \(g_{\phi}(\mathbf{x}^{test})\) is used, to generate probabilities for each class.&lt;/p&gt; &lt;p&gt;When using the squared euclidean distance, the model’s output logits are expressed as:&lt;/p&gt; \[\begin{align*} &amp;amp;- \vert \vert g_{\phi}(\mathbf{x}) - \mathbf{c}_c \vert \vert^2 \\ =&amp;amp; −g_{\phi}(\mathbf{\mathbf{x}})^{\top} g_{\phi}(\mathbf{x}) + 2 \mathbf{c}_{c}^{\top} g_{\phi}(\mathbf{x}) − \mathbf{c}_{c}^{\top} \mathbf{c}_{c} \\ =&amp;amp; 2 \mathbf{c}_{c}^{\top} g_{\phi}(\mathbf{x}) − \vert \vert \mathbf{c}_{c} \vert \vert^2 + constant. \end{align*}\] &lt;p&gt;Note that the “test” superscripts on $\mathbf{x}$ are left out for clarity. \(−g_{\phi}(\mathbf{x})^{\top} g_{\phi}(\mathbf{x})\) is disregarded here, as it’s the same for all logits, and thus doesn’t affect the output probabilities. When inspecting the left-over equation, we can see that it now has the shape of a linear classifier. More specifically, a linear classifier with weight vectors \(\mathbf{w}_c = 2 \mathbf{c}_c^{\top}\) and biases \(b_c = \vert \vert \mathbf{c}_{c} \vert \vert^2\).&lt;/p&gt; &lt;p&gt;Proceeding to Proto-MAML, Triantafillou et al. [2020] &lt;d-cite key=&quot;DBLP:conf/iclr/TriantafillouZD20&quot;&gt;&lt;/d-cite&gt; adapt vanilla MAML by initializing the classification head using the prototype weights and biases, as just discussed. The initialization happens before the inner loop for each task, and the prototypes are computed by MAMLs own feature extractor. Afterward, the fine-tuning works as usual. Finally, when updating $\theta$ in the outer loop, the gradients flow also through the initialization of \(\mathbf{w}_c\) and \(b_c\), which is easy as they fully depend on \(g_{\phi}(\mathbf{x})\).&lt;/p&gt; &lt;p&gt;Note that because of computational reasons, Triantafillou et al. [2020] &lt;d-cite key=&quot;DBLP:conf/iclr/TriantafillouZD20&quot;&gt;&lt;/d-cite&gt; refer to Proto-MAML as (FO-)Proto-MAML.&lt;/p&gt; &lt;p&gt;With Proto-MAML, one gets a task-specific, data-dependent initialization in a simple fashion, which seems super nice. For computing the model’s output logits after classification head initialization, dot products between class prototypes and embedded examples are computed, which again seems very reasonable.&lt;/p&gt; &lt;p&gt;One could argue that in the one-shot scenario, Proto-MAML doesn’t learn that much in the inner loop beside the initialization itself. This happens as the dot product between an embedded training example and one class prototype (which equals the embedded training example itself for one class) will be disproportionately high. For a k-shot example, this effect might be less, but still, there is always one training example embedding within the prototype to compare. Following this thought, the training samples would rather provide a useful initialization of the final layer than a lot of parameter adaptation. One has to say that Proto-MAML performed quite well in the authors’ experiments.&lt;/p&gt; &lt;h2 id=&quot;what-else-is-there&quot;&gt;What else is there?&lt;/h2&gt; &lt;p&gt;Before proceeding to &lt;a href=&quot;#conclusion--discussion&quot;&gt;Conclusion &amp;amp; Discussion&lt;/a&gt;, here are some pointers to methods that did not perfectly fit the topic but which are closely related:&lt;/p&gt; &lt;p&gt;The first method worth mentioning is called Latent Embedding Optimization (LEO) &lt;d-cite key=&quot;DBLP:conf/iclr/RusuRSVPOH19&quot;&gt;&lt;/d-cite&gt;. The authors encode the training data in a low dimensional subspace, from which model parameters $\theta$ can be generated. In the example presented, $\theta$ consists only of $\mathbf{w}$, so for the first inner-loop iteration, this would perfectly fit our initialization topic. The low dimensional code is generated using a feed-forward encoder, as well as a matching network. Using the matching network allows LEO to consider relations between the training examples of different classes. Very similar classes, for example, might require different decision boundaries than more distinct classes, hence the intuition.&lt;/p&gt; &lt;p&gt;LEO deviates from the initialization scheme, however, as optimization is done in the low dimensional subspace and not on the model’s parameters directly. It is stated that optimizing in a lower dimensional subspace helps in low-data regimes.&lt;/p&gt; &lt;p&gt;Another related method is called MetaOptNet &lt;d-cite key=&quot;DBLP:conf/cvpr/LeeMRS19&quot;&gt;&lt;/d-cite&gt;. In this approach, convex base learners, like support vector machines, are used as the classification head. Those can be optimized till convergence, which solves e.g., the problem of varying performance due to random class label assignments.&lt;/p&gt; &lt;h2 id=&quot;conclusion--discussion&quot;&gt;Conclusion &amp;amp; Discussion&lt;/h2&gt; &lt;p&gt;To conclude, we’ve seen that a variety of problems can be tackled by using initialization strategies for MAMLs linear classification head, including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Varying performance due to random class label assignments&lt;/li&gt; &lt;li&gt;Ability of MAML to work on datasets where the number of classes per task varies&lt;/li&gt; &lt;li&gt;Memorization overfitting&lt;/li&gt; &lt;li&gt;Cross-task interference&lt;/li&gt; &lt;li&gt;and Initialization interference.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Furthermore, for all the approaches presented, a decent gain in performance is reported in comparison to vanilla MAML. It seems, therefore, very reasonable to spend some time thinking about the last layer initialization.&lt;/p&gt; &lt;p&gt;Looking at the problems mentioned and variants discussed in more detail, we can state that all the different variants make MAML &lt;strong&gt;permutation invariant with regard to class label assignments&lt;/strong&gt;. UnicornMAML and the zeroing trick solve it by uniform initialization of $\mathbf{w}$. In Proto-MAML, the initialization happens with regard to the class label assignment, so it’s permutation invariant as well.&lt;/p&gt; &lt;p&gt;Also, all variants are compatible with &lt;strong&gt;datasets where the number of classes per task varies&lt;/strong&gt;. In UnicornMAML, an arbitrary number of classification head vectors can be initialized with the single meta-learned classification head weight vector. When zero-initializing the classification head, the number of classes per task does not matter as well. In Proto-MAML, prototypes can be computed for an arbitrary number of classes, so again, the algorithm works on such a dataset without further adaption.&lt;/p&gt; &lt;p&gt;Next, UnicornMAML and the zeroing trick solve &lt;strong&gt;memorization overfitting&lt;/strong&gt;, again by initializing $\mathbf{w}$ uniformly for all classes. Proto-MAML solves memorization overfitting as well, as the task-specific initialization of $\mathbf{w}$ itself can be interpreted as fine-tuning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Cross-task interference&lt;/strong&gt; and &lt;strong&gt;initialization interference&lt;/strong&gt; are solved by the zeroing trick. For the other models, this is harder to say, as the derivations made by &lt;a href=&quot;#Kao&quot;&gt;Kao et al.&lt;/a&gt; are quite a case specific. Intuitively, Proto-MAML should solve cross-task interference, as the classification head is reinitialized after each task. Initialization interference is not solved by either ProtoMAML or UnicornMAML, as random initialization remains.&lt;/p&gt; &lt;p&gt;Note that in discussion with a reviewer, &lt;a href=&quot;#kao&quot;&gt;Kao et al.&lt;/a&gt; state that the main results they show are achieved by models which had the zeroing trick implemented but which didn’t follow the EFIL assumption. They argue that using only the zeroing trick still enhances supervised contrastiveness. This kind of puts their whole theory into perspective, as without the EFIL assumption, MAML with the zeroing trick is neither an SCL algorithm nor a noisy SCL algorithm. Still, noticeable performance gains are reported though.&lt;/p&gt; &lt;p&gt;The question arises whether the whole theoretical background is needed or whether the zeroing tricks benefit is mainly the uniform initialization, like in UnicornMAML. It would be nice to see how the single learned initialization vector in UnicornMAML turns out to be shaped and how it compares to the zeroing trick. While the zeroing trick reduces cross-task noise and initialization noise, a single initialization vector can weight some features as more important than others for the final classification decision across tasks.&lt;/p&gt; &lt;p&gt;In contrast to the uniform initialization approaches, we have seen Proto-MAML, where class-specific classification head vectors are computed for initialization based on the training data.&lt;/p&gt; &lt;p&gt;Finally, Ye et al. [2022] &lt;d-cite key=&quot;DBLP:conf/iclr/YeC22&quot;&gt;&lt;/d-cite&gt; compare the performance between Proto-MAML and UnicornMAML on MiniImageNet and TieredImageNet. UnicornMAML performs slightly better here in the one- and five-shot settings. Kao et al. [2020] &lt;d-cite key=&quot;DBLP:conf/iclr/KaoCC22&quot;&gt;&lt;/d-cite&gt; do not report any particular numbers for their zeroing trick.&lt;/p&gt; </content> </entry> <entry> <title>Practical Applications of Bsuite For Reinforcement Learning</title> <link href="https://jocelynshen.com/blog/2022/bsuite-applications/"/> <updated>2022-12-01T00:00:00+00:00</updated> <id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/bsuite-applications</id> <content type="html">&lt;h2 id=&quot;0-introduction&quot;&gt;0. Introduction&lt;/h2&gt; &lt;p&gt;For the past few decades, the field of AI has appeared similar to the Wild West. There have been rapid achievements &lt;d-cite key=&quot;krizhevsky_imagenet_2012&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;hessel_rainbow_2018&quot;&gt;&lt;/d-cite&gt;, uncertain regulations &lt;d-cite key=&quot;ramesh_hierarchical_2022&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;chatgpt&quot;&gt;&lt;/d-cite&gt;, and epic showdowns &lt;d-cite key=&quot;brown_superhuman_2019&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;silver_mastering_2016&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;vinyals_sc2_2019&quot;&gt;&lt;/d-cite&gt; happening in the frontier of AI research. The subfield of reinforcement learning has been no exception, where progress in the frontier has generated sensational applied feats while leaving theoretical understanding in the dust &lt;d-cite key=&quot;osband_behaviour_2020&quot;&gt;&lt;/d-cite&gt;. As in many other AI subfields, there remain prevailing questions such as, &lt;em&gt;“Which model should I initially select for the given task?”&lt;/em&gt;, &lt;em&gt;“How can I tune hyperparameters to increase performance?”&lt;/em&gt;, and &lt;em&gt;“What is the best way to improve my already working model?”&lt;/em&gt;. In this blog post, we help tame the frontier of reinforcement learning research by providing insights and quantitative answers to such questions through diagnostic, methodical, and reproducible reinforcement learning techniques. In particular, we focus on DeepMind’s &lt;em&gt;Behaviour Suite for Reinforcement Learning&lt;/em&gt; (bsuite) codebase and showcase explicit examples of how it can aid reinforcement learning researchers in the development process and help provide a bridge between theoretical and applied reinforcement learning understanding.&lt;/p&gt; &lt;p&gt;This introduction section provides the necessary background and motivation to understand the importance of our contribution. The background section describes how deep learning provides a blueprint for bridging theory to practice, and then discusses traditional reinforcement learning benchmarks. The bsuite summary section provides a high-level overview of the core capabilities tested by bsuite, its motivation, an example environment, and a comparison against traditional benchmark environments. In the motivation section, we present arguments for increasing the wealth and diversity of documented bsuite examples, with references to the paper and reviewer comments. The contribution statement presents the four distinct contributions of our work that help extend the bsuite publication. Finally, the experiment summary section describes our setup and rationale for the experimental illustrations in sections 1-5. The information in this introduction section is primarily distilled from the original bsuite publication &lt;d-cite key=&quot;osband_behaviour_2020&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt; &lt;p&gt;The current state of reinforcement learning (RL) theory notably lags progress in practice, especially in challenging problems. There are examples of deep reinforcement learning (DRL) agents learning to play Go from scratch at the professional level &lt;d-cite key=&quot;silver_mastering_2016&quot;&gt;&lt;/d-cite&gt;, learning to navigate diverse video games from raw pixels &lt;d-cite key=&quot;mnih_human-level_2015&quot;&gt;&lt;/d-cite&gt;, and learning to manipulate objects with robotic hands &lt;d-cite key=&quot;andrychowicz_learning_2020&quot;&gt;&lt;/d-cite&gt;. While these algorithms have some foundational roots in theory, including gradient descent &lt;d-cite key=&quot;bottou_large-scale_2010&quot;&gt;&lt;/d-cite&gt;, TD learning &lt;d-cite key=&quot;sutton_learning_1988&quot;&gt;&lt;/d-cite&gt;, and Q-learning &lt;d-cite key=&quot;watkins_q-learning_1992&quot;&gt;&lt;/d-cite&gt;, the authors of bsuite acknowledge that, “The current theory of deep reinforcement learning is still in its infancy” &lt;d-cite key=&quot;osband_behaviour_2020&quot;&gt;&lt;/d-cite&gt;. A strong theory is prized since it can help provide insight and direction for improving known algorithms, while hinting at future research directions.&lt;/p&gt; &lt;p&gt;Fortunately, deep learning (DL) provides a blueprint of the interaction between theoretical and practical improvements. During the ‘neural network winter’, deep learning techniques were disregarded in favor of more theoretically sound convex loss methods &lt;d-cite key=&quot;cortes_support-vector_1995&quot;&gt;&lt;/d-cite&gt;, even though the main ideas and successful demonstrations existed many years previously &lt;d-cite key=&quot;rosenblatt_perceptron_1958&quot;&gt;&lt;/d-cite&gt;. It was only until the creation of benchmark problems, mainly for image recognition &lt;d-cite key=&quot;krizhevsky_imagenet_2012&quot;&gt;&lt;/d-cite&gt;, that deep learning earned the research spotlight due to better scores on the relevant benchmarks. Consequently, a renewed interested in deep learning theory followed shortly after &lt;d-cite key=&quot;kawaguchi_deep_2016&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;bartlett_spectrally-normalized_2017&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;belkin_reconciling_2019&quot;&gt;&lt;/d-cite&gt;, bolstered by the considerable wealth of applied research. Due to the lack of theory in DRL and the proximity of the DL and DRL research fields, &lt;span class=&quot;emph&quot;&gt;one enticing avenue to accelerate progress in reinforcement learning research is to follow the blueprint laid out by deep learning research and create well-defined and vetted benchmarks for the understanding of reinforcement learning algorithms&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;To this end, the trend of RL benchmarks has seen an increase in overall complexity and perhaps the publicity potential. The earliest such benchmarks were simple MDPs that served as basic testbeds with fairly obvious solutions, such as &lt;em&gt;Cartpole&lt;/em&gt; &lt;d-cite key=&quot;barto_neuronlike_1983&quot;&gt;&lt;/d-cite&gt; and &lt;em&gt;MountainCar&lt;/em&gt; &lt;d-cite key=&quot;moore_efficient_1990&quot;&gt;&lt;/d-cite&gt;. Other benchmarks proved to be more diagnostic by targeting certain capabilities such as &lt;em&gt;RiverSwim&lt;/em&gt; &lt;d-cite key=&quot;strehl_analysis_2008&quot;&gt;&lt;/d-cite&gt; for exploration and &lt;em&gt;Taxi&lt;/em&gt; &lt;d-cite key=&quot;dietterich_hierarchical_2000&quot;&gt;&lt;/d-cite&gt; for temporal abstraction. Modern benchmarks such as the &lt;em&gt;ATARI Learning Environment&lt;/em&gt; &lt;d-cite key=&quot;bellemare_arcade_2013&quot;&gt;&lt;/d-cite&gt; and board games such as &lt;em&gt;Chess&lt;/em&gt;, &lt;em&gt;Go&lt;/em&gt;, and &lt;em&gt;Shogi&lt;/em&gt; are more complex and prove difficult for humans, with even the best humans unable to achieve perfect play. The corresponding achievements were highly publicized &lt;d-cite key=&quot;silver_mastering_2016&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;mnih_human-level_2015&quot;&gt;&lt;/d-cite&gt; due to the superhuman performance of the agents, with the agents taking actions that were not even considered by their human counterparts. Consequently, this surge in publicity has been a strong driver of progress in the field and has vaulted the notion of superhuman performance to be the most coveted prize on numerous benchmarks &lt;d-cite key=&quot;dietterich_hierarchical_2000&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;silver_general_2018&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;dietterich_hierarchical_2000&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;ecoffet_first_2021&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;bakhtin_diplomacy_2022&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h3 id=&quot;summary-of-bsuite&quot;&gt;Summary of bsuite&lt;/h3&gt; &lt;p&gt;The open-source &lt;em&gt;Behaviour Suite for Reinforcement Learning&lt;/em&gt; (bsuite) benchmark &lt;d-cite key=&quot;osband_behaviour_2020&quot;&gt;&lt;/d-cite&gt; goes against the grain of the current benchmark trend of increasing complexity and publicity. Instead of chasing superhuman performance, it acts as a complement to existing benchmarks by creating 23 environments with minimal confounding factors to test 7 behavioral core capabilities of RL agents, as follows: &lt;strong&gt;basic&lt;/strong&gt;, &lt;strong&gt;exploration&lt;/strong&gt;, &lt;strong&gt;memory&lt;/strong&gt;, &lt;strong&gt;generalization&lt;/strong&gt;, &lt;strong&gt;noise&lt;/strong&gt;, &lt;strong&gt;scale&lt;/strong&gt;, and &lt;strong&gt;credit assignment&lt;/strong&gt;. Current benchmarks often contain most of these capabilities within a single environment, whereas bsuite tailors its environments to target one or a few of these capabilities. Each bsuite environment is scalable and has 16 to 22 levels of difficulty, providing a more precise analysis of the corresponding capabilities than a simple, and possibly misleading &lt;d-cite key=&quot;agarwal_deep_2021&quot;&gt;&lt;/d-cite&gt;, ranking of algorithm performance. Furthermore, algorithms have fixed evaluation regimes based on the number of seeds and episodes allowed during training, which rewards algorithms that exhibit the capabilities rather than those that focus on sheer compute power. The targeted and scalable nature of bsuite can provide insights such as eliciting bottlenecks and revealing scaling properties that are opaque in traditional benchmarks. With respect to the benchmarks described in the preceding paragraph, bsuite is most similar to the diagnostic benchmarks of &lt;em&gt;RiverSwim&lt;/em&gt; &lt;d-cite key=&quot;strehl_analysis_2008&quot;&gt;&lt;/d-cite&gt; for and &lt;em&gt;Taxi&lt;/em&gt; &lt;d-cite key=&quot;dietterich_hierarchical_2000&quot;&gt;&lt;/d-cite&gt; due to its purpose as a stepping stone for tackling more challenging benchmarks.&lt;/p&gt; &lt;p&gt;The bsuite evaluation of an agent yields a radar chart (Fig. 1) that displays the agent’s score from 0 to 1 on all seven capabilities, usually based on regret, that yields a quick quantitative comparison between agents. Scores near 0 indicate poor performance, often akin to an agent acting randomly, while scores near 1 indicate mastery of all environment difficulties. A central premise of bsuite is that &lt;span class=&quot;emph&quot;&gt;if an agent achieves high scores on certain environments, then it is much more likely to exhibit the associated core capabilities due to the targeted nature of the environments. Therefore, the agent will more likely perform better on a challenging environment that contains many of the capabilities than one with lower scores on bsuite&lt;/span&gt;. This premise is corroborated by recent research that shows how insights on small-scale environments can still hold true on large-scale environments &lt;d-cite key=&quot;ceron_revisiting_2021&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;div style=&quot;text-align: center;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar01-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar01-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar01-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar01.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 1. Example radar chart of DQN on all 7 bsuite core capabilities. &lt;/div&gt; &lt;p&gt;An example environment is &lt;em&gt;deep sea&lt;/em&gt; that targets exploration power. As shown in Figure 2, &lt;em&gt;deep sea&lt;/em&gt; is an $N \times N$ grid with starting state at cell $(1, 1)$ and treasure at $(N, N)$, with $N$ ranging from 10 to 100. The agent has two actions, move downward left and downward right; the goal is to reach the treasure and receive a reward of $1$ by always moving downward right. A reward of $0$ is given to the agent for moving downward left at a timestep, while a penalizing reward of $-0.01/N$ is given for moving downward right. The evaluation protocol of &lt;em&gt;deep sea&lt;/em&gt; only allows for $10K$ episodes of $N-1$ time steps each, which prevents an algorithm with unlimited time from casually exploring the entire state space and stumbling upon the treasure. Note that superhuman performance is nonexistent in &lt;em&gt;deep sea&lt;/em&gt; (and more precisely in the entire bsuite gamut) since a human can spot the optimal policy nearly instantaneously. Surprisingly, we will show later that baseline DRL agents fail miserably at this task.&lt;/p&gt; &lt;div style=&quot;text-align: center;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/diagram02-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/diagram02-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/diagram02-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/diagram02.png&quot; class=&quot;img-fluid asdf&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 2. Illustration of deep sea environment taken from &lt;d-cite key=&quot;osband_behaviour_2020&quot;&gt;&lt;/d-cite&gt;. &lt;/div&gt; &lt;/div&gt; &lt;p&gt;The &lt;strong&gt;challenge&lt;/strong&gt; of &lt;em&gt;deep sea&lt;/em&gt; is the necessity of exploration in an environment that presents an irreversible, suboptimal greedy action (moving downward left) at every time step. This environment &lt;strong&gt;targets&lt;/strong&gt; exploration power by ensuring that a successful agent must deliberately choose to explore the state space by neglecting the greedy action. The &lt;strong&gt;simplistic&lt;/strong&gt; implementation removes confounding goals, such as learning to see from pixels while completing other tasks &lt;d-cite key=&quot;mnih_human-level_2015&quot;&gt;&lt;/d-cite&gt;. Furthermore, this environment provides a granular exploration score through &lt;strong&gt;scaling&lt;/strong&gt; the environment size by $N$ and determining when an agent starts to fail. Finally, the implementation of the environment yields &lt;strong&gt;fast&lt;/strong&gt; computation, allowing multiple, quick runs with minimal overhead and compute cost. These 5 aforementioned key qualities are encompassed by all bsuite environments, and we contrast such environments against traditional benchmark environments in the below table.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Key Quality&lt;/th&gt; &lt;th&gt;Traditional Benchmark Environment&lt;/th&gt; &lt;th&gt;bsuite Environment&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Targeted&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Performance on environment subtly related to many or all core capabilities.&lt;/td&gt; &lt;td&gt;Performance on environment directly related with one or few core capabilities.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Simple&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Exhibits many confounding factors related to performance.&lt;/td&gt; &lt;td&gt;Removes confounding factors related to performance.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Challenging&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Requires competency in many core capabilities but not necessarily past normal range in any capability.&lt;/td&gt; &lt;td&gt;Pushes agents beyond normal range in one or few core capabilities.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Scalable&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Discerns agent’s power through comparing against other agents and human performance.&lt;/td&gt; &lt;td&gt;Discerns agent’s competency of core capabilities through increasingly more difficult environments.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Fast&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Long episodes with computationally-intensive observations.&lt;/td&gt; &lt;td&gt;Relatively small episode and experiment lengths with low observation complexity.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h3 id=&quot;motivation&quot;&gt;Motivation&lt;/h3&gt; &lt;p&gt;The authors of bsuite stated, “Our aim is that these experiments can help provide a bridge between theory and practice, with benefits to both sides” &lt;d-cite key=&quot;osband_behaviour_2020&quot;&gt;&lt;/d-cite&gt;. As discussed in the background section, establishing clear benchmarks can yield applied progress, which in turn can accelerate theoretical progress. The use of bsuite in this manner seems highly fruitful since its environments are targeted, which allows for hypothesis testing and eventual formalization into provable guarantees. As such, &lt;span class=&quot;emph&quot;&gt;it is instrumental that the applied aspect of bsuite is emphasized through the adoption and diverse application of reinforcement learning practitioners&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;The applied examples in the published paper are rather meagre: there are two examples of algorithm comparison on two specific environments and three example comparisons of algorithms, optimizers, and ensemble sizes across the entire bsuite gamut in the appendix. The two examples on the specific environments showcase how bsuite can be used for directed algorithm improvement, but the experiments in the appendices only discuss the general notion of algorithm comparison using bsuite scores. In addition to the examples, the authors supply some comments throughout the paper that provide hints regarding the applied usage of bsuite. Looking at the &lt;a href=&quot;https://openreview.net/forum?id=rygf-kSYwH&quot;&gt;paper reviews&lt;/a&gt;, &lt;a href=&quot;https://openreview.net/forum?id=rygf-kSYwH&amp;amp;noteId=rkxk2BR3YH&quot;&gt;reviewer #1&lt;/a&gt; mentioned how there was no explicit conclusion from the evaluation, and &lt;a href=&quot;https://openreview.net/forum?id=rygf-kSYwH&amp;amp;noteId=rJxjmH6otS&quot;&gt;reviewer #3&lt;/a&gt; mentioned that examples of diagnostic use and concrete examples would help support the paper. Furthermore, &lt;a href=&quot;https://openreview.net/forum?id=rygf-kSYwH&amp;amp;noteId=SJgEVpbAFr&quot;&gt;reviewer #2&lt;/a&gt; encouraged publication of bsuite at a top venue to see traction within with the RL research community, and the &lt;a href=&quot;https://openreview.net/forum?id=rygf-kSYwH&amp;amp;noteId=7x_6G9OVWG&quot;&gt;program chairs&lt;/a&gt; mentioned how success or failure can rely on community acceptance. Considering that bsuite received a spotlight presentation at ICLR 2020 and has amassed over 100 citations in the relatively small field of RL reproducibility during the past few years, bsuite has all intellectual merit and some community momentum to reach the level of a timeless benchmark in RL research. &lt;span class=&quot;emph&quot;&gt;To elevate bsuite to the status of a timeless reinforcement learning benchmark and to help bridge the theoretical and applied sides of reinforcement learning, we believe that it is necessary to develop and document concrete bsuite examples that help answer difficult and prevailing questions throughout the reinforcement learning development process&lt;/span&gt;.&lt;/p&gt; &lt;h3 id=&quot;contribution-statement&quot;&gt;Contribution Statement&lt;/h3&gt; &lt;p&gt;This blog post extends the work of bsuite by showcasing 12 example use cases with experimental illustration that directly address specific questions in the reinforcement learning development process to (i) help bridge the gap between theory and practice, (ii) promote community acceptance, (iii) aid applied practitioners, and (iv) highlight potential research directions in reproducible reinforcement learning.&lt;/p&gt; &lt;h3 id=&quot;experiment-summary&quot;&gt;Experiment Summary&lt;/h3&gt; &lt;p&gt;We separate our examples into 5 categories of &lt;strong&gt;initial model selection&lt;/strong&gt;, &lt;strong&gt;preprocessing choice&lt;/strong&gt;, &lt;strong&gt;hyperparameter tuning&lt;/strong&gt;, &lt;strong&gt;testing and debugging&lt;/strong&gt;, and &lt;strong&gt;model improvement&lt;/strong&gt;. This blog post follows a similar structure to the paper &lt;em&gt;Deep Reinforcement Learning that Matters&lt;/em&gt; &lt;d-cite key=&quot;henderson_deep_2018&quot;&gt;&lt;/d-cite&gt; by posing and answering a question in each category, and then providing a few illustrative examples with conclusions. Most examples use Stable-Baselines3 (SB3) &lt;d-cite key=&quot;raffin_stable-baselines3_2022&quot;&gt;&lt;/d-cite&gt; for training DRL agents due to its clarity and simplicity, and the examples focus on DRL due to its pervasiveness in the applied RL community. We provide code and instructions for each experiment in our &lt;a href=&quot;https://anonymous.4open.science/r/bsuite-applications/README.md&quot;&gt;GitHub codebase&lt;/a&gt;, along with hyperparameters and implementation details. Since the focus of this blog post is the discussion of diverse example use cases, not architectural considerations or implementation details, we refer the reader to the &lt;a href=&quot;https://openreview.net/pdf?id=rygf-kSYwH#page=13&quot;&gt;paper appendix&lt;/a&gt; and the &lt;a href=&quot;https://colab.research.google.com/github/deepmind/bsuite/blob/master/bsuite/analysis/results.ipynb&quot;&gt;colab analysis tutorial&lt;/a&gt; for more information about the environments and to the &lt;a href=&quot;https://colab.research.google.com/drive/1rU20zJ281sZuMD1DHbsODFr1DbASL0RH&quot;&gt;colab intro tutorial&lt;/a&gt; and our own codebase for instructions and examples regarding the implementation of bsuite.&lt;/p&gt; &lt;p&gt;Although running a bsuite environment is orders of magnitude faster than most benchmark environments, the wealth of our examples required us to create a subset of bsuite, which we will refer to as &lt;em&gt;mini-bsuite&lt;/em&gt; or &lt;em&gt;msuite&lt;/em&gt; in this work. We designed msuite to mirror the general scaling pattern of each bsuite environment and the diversity of core capabilities in bsuite; a complete description of msuite can be found in our GitHub codebase. Running experiments on a subset of bsuite highlights its flexibility, and we will show, still elicits quality insights. Since we use a subset of bsuite for our experiments, our radar charts will look different from those in the original bsuite paper. We generally keep the more challenging environments and consequently produce lower scores, especially in the generalization category.&lt;/p&gt; &lt;p&gt;We stress that the below examples are not meant to amaze the reader or exhibit state-of-the-art research. &lt;span class=&quot;epmh&quot;&gt;The main products of this work are the practicality and diversity of ideas in the examples&lt;/span&gt;, while the experiments are primarily for basic validation and illustrative purposes. Moreover, these experiments use modest compute power and showcase the effectiveness of bsuite in the low-compute regime. Each example has tangible benefits such as saving development time, shortening compute time, increasing performance, and lessening frustration of the practitioner, among others. To maintain any sense of brevity in this post, we now begin discussion of the examples.&lt;/p&gt; &lt;h2 id=&quot;1-initial-model-selection&quot;&gt;1. Initial Model Selection&lt;/h2&gt; &lt;p&gt;The reinforcement learning development cycle typically begins with an environment to solve. A natural question usually follows: “&lt;em&gt;Which underlying RL model should I choose to best tackle this environment, given my resources&lt;/em&gt;?”. Resources can range from the hardware (e.g. model size on the GPU), to temporal constraints, to availability of off-the-shelf algorithms &lt;d-cite key=&quot;liang_rllib_2018&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;raffin_stable-baselines3_2022&quot;&gt;&lt;/d-cite&gt;, to programming efficiency of the practitioner. Initially selecting an effective model can save a great amount of development time due to the potentially greater performance baseline of the agent. In this section, we illustrate how bsuite can be used to effectively answer the question of initial model selection.&lt;/p&gt; &lt;h3 id=&quot;comparing-baseline-algorithms&quot;&gt;Comparing Baseline Algorithms&lt;/h3&gt; &lt;p&gt;Perhaps the first choice in the RL development cycle is choosing the algorithm. A considerable amount of RL research is focused on the corresponding algorithms, which presents many possibilities for the researcher. The No Free Lunch Theorem &lt;d-cite key=&quot;wolpert_no_1997&quot;&gt;&lt;/d-cite&gt; tailored to reinforcement learning would state that no algorithm will prove better than any other unless the characteristics of the underlying environment are known. Using bsuite provides a quantitative assessment of algorithm performance on capabilities that are prevalent in many or even most reinforcement learning environments of interest.&lt;/p&gt; &lt;p&gt;Example: Figure 3 shows the performance of the Stable-Baselines3 (SB3) implementations of DQN, A2C, and PPO on msuite with our default hyperparameters. Recent research &lt;d-cite key=&quot;andrychowicz_what_2020&quot;&gt;&lt;/d-cite&gt; suggests that PPO is the most commonly used RL algorithm, and it was a successor to DQN and A2C. The results indeed show that PPO is superior on msuite in most categories, providing credibility for its use as the premiere baseline DRL algorithm.&lt;/p&gt; &lt;div style=&quot;text-align: center;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar11-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar11-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar11-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar11.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 3. Comparison of SB3 default DQN, A2C, and PPO baseline algorithms. &lt;/div&gt; &lt;/div&gt; &lt;h3 id=&quot;comparing-off-the-shelf-implementations&quot;&gt;Comparing Off-the-Shelf Implementations&lt;/h3&gt; &lt;p&gt;Due to the vast number of reinforcement learning paradigms (e.g. model-based, hierarchical), there are many off-the-shelf (OTS) libraries that provide a select number of thoroughly tested reinforcement learning algorithms. Often, temporal resources or coding capabilities do not allow for practitioners to implement every algorithm by hand. Fortunately, running an algorithm on bsuite can provide a quick glance of an OTS algorithm’s abilities at low cost to the practitioner.&lt;/p&gt; &lt;p&gt;Example: Figure 4 compares our default DQN implementation against the example DQN implementation in the bsuite codebase. There is a significant difference between the performance of each implementation on msuite, with the bsuite implementation displaying its superiority. Note that the hyperparameters of bsuite DQN were most likely chosen with the evaluation on bsuite in mind, which could explain its increased performance.&lt;/p&gt; &lt;div style=&quot;text-align: center;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar12-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar12-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar12-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar12.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 4. Comparison of SB3 DQN and bsuite DQN. &lt;/div&gt; &lt;/div&gt; &lt;h3 id=&quot;gauging-hardware-necessities&quot;&gt;Gauging Hardware Necessities&lt;/h3&gt; &lt;p&gt;Even after an initial algorithm is selected, hardware limitations such as network size and data storage can prevent the agent from being deployed. Using bsuite provides a low-cost comparison among possible hardware choices that can be used to argue for their necessity. This is especially important for small development teams since there can likely be a major disparity between their own hardware resources and those discussed in corresponding research publications.&lt;/p&gt; &lt;p&gt;Example: Figure 5 compares the default DQN implementation when varying replay buffer sizes, from $1e2$ to $1e5$, with the default having size $1e4$. The original DQN implementation used a replay buffer of size $1e6$, which is too large for the RAM constraints of many personal computers. The results show that increasing the buffer size to at least $1e4$ yields significant returns on msuite. Note that since the experiment lengths (total time steps for all episodes) of msuite were sometimes less than $1e5$, the larger buffer size of $1e5$ did not always push out experiences from very old episodes, which most likely worsened performance.&lt;/p&gt; &lt;div style=&quot;text-align: center;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar13-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar13-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar13-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar13.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 5. Comparison of DQN with varying buffer sizes. &lt;/div&gt; &lt;/div&gt; &lt;h3 id=&quot;future-work&quot;&gt;Future Work&lt;/h3&gt; &lt;p&gt;Due to the diversity of OTS libraries, one possible research direction in reproducible RL is to test algorithms from different OTS libraries using the same hyperparameters on bsuite and create a directory of bsuite radar charts. This provides practitioners a comparison with their own implementation or a starting point when selecting an OTS library and algorithm. Another direction is to test various aspects related to hardware constraints and attempt to show the tradeoff between constraints and performance on bsuite and other benchmarks. This would especially help practitioners with low compute resources to budget resource use on multiple projects.&lt;/p&gt; &lt;h2 id=&quot;2-preprocessing-choice&quot;&gt;2. Preprocessing Choice&lt;/h2&gt; &lt;p&gt;Most benchmark environments present complexities such as high-dimensional observations, unscaled rewards, unnecessary actions, and partially-observable Markov Decision Process (POMDP) dynamics. Some of these difficulties can be curbed using environment preprocessing techniques. While certain environments such as &lt;em&gt;ATARI&lt;/em&gt; have formalized standards for preprocessing, there are some aspects such as frame skipping that are considered part of the underlying algorithm, and therefore, a choice of the practitioner &lt;d-cite key=&quot;machado_revisiting_2018&quot;&gt;&lt;/d-cite&gt;. A natural question to ask is, “&lt;em&gt;What environment preprocessing techniques will best help my agent attain its goal in this environment&lt;/em&gt;?”. In this section, we show how bsuite can provide insight to the choice of preprocessing, with benefits of increased performance and shortened training time.&lt;/p&gt; &lt;h3 id=&quot;verification-of-preprocessing&quot;&gt;Verification of Preprocessing&lt;/h3&gt; &lt;p&gt;Preprocessing techniques usually targeted to ease some aspect of the agent’s training. For example, removing unnecessary actions (e.g. in a joystick action space) prevents the agent from having to learn which actions are useless. While a new preprocessing technique can provide improvements, there is always the chance that it fails to make a substantial improvement, or worse yet, generally decreases performance. Invoking bsuite can help provide verification that the preprocessing provided the planned improvement.&lt;/p&gt; &lt;p&gt;Example: Figure 6 shows the performance of the default DQN agent versus an agent that received normalized rewards from the environment. Normalizing the rewards increases the speed of training a neural network, since the parameters are usually initialized to expect target values in a range from $-1$ to $1$. Our results show that the normalization preprocessing indeed increases the capability of navigating varying reward scales while not suffering drastically in any other capability.&lt;/p&gt; &lt;div style=&quot;text-align: center;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar21-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar21-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar21-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar21.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 6. Comparison of DQN with and without reward normalization. &lt;/div&gt; &lt;/div&gt; &lt;h3 id=&quot;better-model-versus-preprocessing&quot;&gt;Better Model versus Preprocessing&lt;/h3&gt; &lt;p&gt;Instead of choosing to preprocess the environment, a more sophisticated algorithm may better achieve the preprocessing goals. For example, many improvements on the original DQN algorithm have been directed towards accomplishing goals such as improving stability, reducing overestimation, and bolstering exploration. Comparing preprocessing against an algorithmic improvement provides a quantitative reason for deciding between the two options, especially since development time of many common preprocessing wrappers is quite short.&lt;/p&gt; &lt;p&gt;Example: Figure 7 shows the results of PPO with a recurrent network versus PPO having its observation as the last 4 stacked frames from the environment. Frame stacking is common on &lt;em&gt;ATARI&lt;/em&gt; since it converts the POMDP dynamics to an MDP, which is necessary to determine velocity of any element on the screen. An improvement to DQN, Deep Recurrent Q-networks &lt;d-cite key=&quot;hausknecht_deep_2017&quot;&gt;&lt;/d-cite&gt; uses a recurrent LSTM to aid in memory and achieve the same effects of frame stacking. The msuite results show that memory is considerably improved with PPO RNN and therefore may be worth the extra development time.&lt;/p&gt; &lt;div style=&quot;text-align: center;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar22-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar22-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar22-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar22.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 7. Comparison of PPO with frame stacking and PPO with RNN. &lt;/div&gt; &lt;/div&gt; &lt;h3 id=&quot;future-work-1&quot;&gt;Future Work&lt;/h3&gt; &lt;p&gt;One research direction is to document common preprocessing techniques and determine their scores on bsuite. This would provide practitioners a summary of directed strengths for each preprocessing technique while possibly uncovering unexpected behavior. Another direction is to determine the extent to which preprocessing techniques aided previous results in the literature, which could illuminate strengths or weaknesses in the corresponding algorithms.&lt;/p&gt; &lt;h2 id=&quot;3-hyperparameter-tuning&quot;&gt;3. Hyperparameter Tuning&lt;/h2&gt; &lt;p&gt;After selecting a model and determining any preprocessing of the environment, an agent must eventually be trained on the environment to gauge its performance. During the training process, initial choices of hyperparameters can heavily influence the agent’s performance &lt;d-cite key=&quot;andrychowicz_what_2020&quot;&gt;&lt;/d-cite&gt;, including how to explore and how quickly the model should learn from past experience. The corresponding question to ask is, “&lt;em&gt;How can I choose hyperparameters to yield the best performance, given a model?&lt;/em&gt;” In this section, we show how bsuite can be used to tune hyperparameters, thereby increasing performance and shortening compute time.&lt;/p&gt; &lt;h3 id=&quot;unintuitive-hyperparameters&quot;&gt;Unintuitive Hyperparameters&lt;/h3&gt; &lt;p&gt;Some hyperparameters such as exploration percentage and batch size are more concrete, while others such as discounting factor and learning rate are a little less intuitive. Determining a starting value of an unintuitive hyperparameter can be challenging and require a few trials before honing in on a successful value. Instead of having to run experiments on a costly environment, using bsuite can provide a thoughtful initial guess of the value with minimal compute.&lt;/p&gt; &lt;p&gt;Example: Figure 8 shows the results of running PPO with various entropy bonus coefficients across msuite (default is $0.01$). The entropy bonus affects the action distribution of the agent, and the value of $1\mathrm{e}{-2}$ presented in the original paper &lt;d-cite key=&quot;schulman_proximal_2017&quot;&gt;&lt;/d-cite&gt; is fairly unintuitive. The results show that the value of $1\mathrm{e}{-2}$ is indeed superior on msuite by a small margin. Since SB3 has the entropy bonus initialized to 0, this example also shows how hyperparameter tuning with msuite can improve performance even on OTS implementations.&lt;/p&gt; &lt;div style=&quot;text-align: center;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar31-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar31-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar31-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar31.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 8. Comparison of default PPO with varying entropy bonuses. &lt;/div&gt; &lt;/div&gt; &lt;h3 id=&quot;promising-ranges-of-hyperparameters&quot;&gt;Promising Ranges of Hyperparameters&lt;/h3&gt; &lt;p&gt;Instead of determining a single value of a hyperparameter, gauging an acceptable range may be required. Since hyperparameters can have confounding effects, knowing approximate soft boundaries of hyperparameters at which agents start to fail basic tasks can provide useful information during a more general hyperparameter tuning process. For example, smaller learning rates generally take longer for algorithm convergence, and a practitioner may want to know a promising range of learning rates if the computing budget is flexible. The scaling nature of bsuite presents knowledge of the extent to which different hyperparameter choices affect performance, greatly aiding in ascertaining a promising hyperparameter range.&lt;/p&gt; &lt;p&gt;Example: Figure 9 shows the results of default DQN with varying learning rates on msuite (default $7\mathrm{e}{-4}$). The results suggest that learning rates above $1\mathrm{e}{-2}$ start to yield diminishing returns. Since some experiment lengths in msuite only run for $10K$ episodes, the lowest learning rate of $1\mathrm{e}{-6}$ may never converge in time even with high-quality training data, necessitating a modification to msuite to learn a lower bound.&lt;/p&gt; &lt;div style=&quot;text-align: center;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar32-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar32-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar32-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar32.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 9. Comparison of default DQN with varying learning rates. &lt;/div&gt; &lt;/div&gt; &lt;h3 id=&quot;pace-of-annealing-hyperparameters&quot;&gt;Pace of Annealing Hyperparameters&lt;/h3&gt; &lt;p&gt;While some hyperparameters stay fixed, others must change throughout the course of training. Typically, these include hyperparameters that control the exploration vs. exploitation dilemma, such as entropy bonus and epsilon-greedy exploration. These hyperparameters are often dependent on the entire experiment; for example, SB3 anneals epsilon-greedy exploration for a fixed fraction of the experiment. Therefore, entire experiments, some consisting of millions of episodes, need to be run to determine successful values of these hyperparameters. Using bsuite can provide a quick confirmation that the annealing of these parameters happens at an acceptable rate.&lt;/p&gt; &lt;p&gt;Example: Figure 10 shows the performance of DQN with various epsilon-greedy exploration annealing lengths, based on a fixed fraction of the entire experiment (default $0.1$). The annealing fraction of $0.1$ performs best on msuite, which is the same choice of parameter in the original DQN paper. Furthermore, performance decreases with greater annealing lengths. Since bsuite environments are generally scored with regret, we acknowledge that the longer annealing lengths may have better relative performance if bsuite were scored with a training versus testing split.&lt;/p&gt; &lt;div style=&quot;text-align: center;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar33-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar33-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar33-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar33.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 10. Comparison of default DQN with varying epsilon annealing lengths. &lt;/div&gt; &lt;/div&gt; &lt;h3 id=&quot;future-work-2&quot;&gt;Future Work&lt;/h3&gt; &lt;p&gt;The three experiments above can be extended by documenting the effect of varying hyperparameters on performance, especially in OTS implementations. This would help practitioners understand the effects of certain hyperparameters on the bsuite core capabilities, allowing for a better initial hyperparameter choice when certain capabilities are necessary for the environment at hand. Another research direction is to determine if integrating a fast hyperparameter tuner on general environments such as bsuite into a hyperparameter tuner for single, complex environments would increase the speed of tuning on the fixed environment. Since the bsuite core capabilities are necessary in many complex environments, initially determining competency on bsuite would act as a first pass of the tuning algorithm.&lt;/p&gt; &lt;h2 id=&quot;4-testing-and-debugging&quot;&gt;4. Testing and Debugging&lt;/h2&gt; &lt;p&gt;Known to every RL practitioner, testing and debugging during the development cycle is nearly unavoidable. It is common to encounter silent bugs in RL code, where the program runs but the agent fails to learn because of an implementation error. Examples include incorrect preprocessing, incorrect hyperparameters, or missing algorithm additions. Quick unit tests can be invaluable for the RL practitioner, as shown in successor work to bsuite &lt;d-cite key=&quot;rajan_mdp_2021&quot;&gt;&lt;/d-cite&gt;. A corresponding question to ask during the testing and debugging phase is, “&lt;em&gt;What tests can I perform to verify that my agent is running as intended?&lt;/em&gt;” In this section, we show how bsuite can be used as a sanity check for the implementation, saving compute time and lessening the frustration of the practitioner. In an effort to refrain from contrived examples, the two examples below highlight real-life scenarios where using bsuite could have saved the authors of this blog post hours of frustration in their own work.&lt;/p&gt; &lt;h3 id=&quot;incorrect-hyperparameter&quot;&gt;Incorrect Hyperparameter&lt;/h3&gt; &lt;p&gt;As discussed in the previous section, hyperparameters are of major importance to the performance of a RL algorithm. A missing or incorrect hyperparameter will not necessarily prevent a program from running, but most such bugs will severely degrade performance. Using bsuite can quickly expose poor performance of an algorithm at a low cost to the practitioner.&lt;/p&gt; &lt;p&gt;Example: Figure 11 shows the default PPO implementation against a PPO implementation with an erroneous learning rate of $1\mathrm{e}{-3}$. Many hyperparameters such as total training steps and maximum buffer size are usually coded using scientific notation since they are so large; consequently, it is easy to forget the ‘minus sign’ when coding the learning rate and instead code the learning rate as $1e3$. The results on msuite show that performance has degraded severely from an OTS implementation, and more investigation into the code is required. One of the authors of this blog post would have saved roughly a day of training a PPO agent in their own work had they realized this exact mistake.&lt;/p&gt; &lt;div style=&quot;text-align: center;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar41-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar41-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar41-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar41.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 11. Comparison of default PPO with miscoded PPO. &lt;/div&gt; &lt;/div&gt; &lt;h3 id=&quot;off-the-shelf-algorithm-testing&quot;&gt;Off-the-Shelf Algorithm Testing&lt;/h3&gt; &lt;p&gt;While the previous example used an OTS algorithm for comparison to illuminate silent bugs, it may be the case that the OTS algorithm itself could have a silent bug. Whether due to an incorrect library being used or a misunderstanding of the OTS algorithm, any silent bug in an OTS algorithm can be difficult to detect due to the codebase being written by another practitioner. Again, bsuite can be used to diagnose poor performance and elucidate a coding problem.&lt;/p&gt; &lt;p&gt;Example: Figure 12 shows the results of the SB3 DQN with our default experimental hyperparameters and with the default SB3 hyperparameters on msuite. A core difference between the hyperparameters is the burn rate: the default SB3 hyperparameters perform $10K$ steps before learning takes place (e.g. backprop), while our default experimental hyperparameters start the learning after $1K$ steps. Since many of the easier msuite environments only last $10K$ time steps, failure to learn anything during that time severely degrades performance, as shown. Noticing the default value of this hyperparameter in SB3 would have saved the authors roughly 10 hours of training time.&lt;/p&gt; &lt;div style=&quot;text-align: center;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar42-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar42-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar42-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar42.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 12. Comparison of DQN with small and large burn-in. &lt;/div&gt; &lt;/div&gt; &lt;h3 id=&quot;future-work-3&quot;&gt;Future Work&lt;/h3&gt; &lt;p&gt;The training time for a complete run of bsuite can take an hour for even the most basic algorithms. Considering that a few of the easiest bsuite environments could have shown poor performance in the above examples within mere minutes, one research avenue is to create a fast debugging system for reinforcement learning algorithms. In the spirit of bsuite, it should implement targeted experiments to provide actionable solutions for eliminating silent bugs. Such work would primarily act as a public good, but it could also help bridge the gap between RL theory and practice if it embodies the targeted nature of bsuite.&lt;/p&gt; &lt;h2 id=&quot;5-model-improvement&quot;&gt;5. Model Improvement&lt;/h2&gt; &lt;p&gt;A natural milestone in the RL development cycle is getting an algorithm running bug-free with notable signs of learning. A common follow-up question to ask is, “&lt;em&gt;How can I improve my model to yield better performance?&lt;/em&gt;”. The practitioner may consider choosing an entirely new model and repeating some of the above steps; a more enticing option is usually to improve the existing model by reusing its core structure and only making minor additions or modifications, an approach taken in the development of the baseline RAINBOW DQN algorithm &lt;d-cite key=&quot;hessel_rainbow_2018&quot;&gt;&lt;/d-cite&gt;. In this section, we discuss how bsuite can be used to provide targeted improvements of existing models and increase performance while mitigating compute time.&lt;/p&gt; &lt;h3 id=&quot;increasing-network-complexity&quot;&gt;Increasing Network Complexity&lt;/h3&gt; &lt;p&gt;In DRL, the neural network usually encodes the policy, and its architecture directly affects the agent’s learning capacity. The more complicated CNN architecture was a driver for the first superhuman performance of a DRL algorithm on the &lt;em&gt;ATARI&lt;/em&gt; suite due to its ability to distill image data into higher-level features. Using bsuite can provide a quick verification if an architectural improvement produces its intended effect.&lt;/p&gt; &lt;p&gt;Example: Figure 13 shows the results of PPO against PPO with a recurrent neural network. As mentioned in a previous example, RNNs aid memory and were originally incorporated into DRL as a way to deal with POMDP dynamics. The results on msuite display the substantial increase in memory capability while sacrificing on credit assignment. This example highlights how bsuite can provide warnings of possible unexpected decreases in certain capabilities, which must be monitored closely by the practitioner.&lt;/p&gt; &lt;div style=&quot;text-align: center;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar51-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar51-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar51-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar51.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 13. Comparison of default PPO with PPO RNN. &lt;/div&gt; &lt;/div&gt; &lt;h3 id=&quot;off-the-shelf-improvements&quot;&gt;Off-the-Shelf Improvements&lt;/h3&gt; &lt;p&gt;While previous examples discussed comparison, verification, and debugging OTS implementations, many OTS libraries provide support for well-known algorithm improvements. For example, some DQN implementations have boolean values to signify the use of noisy networks, double Q-learning, and more. Using bsuite provides the necessary targeted analysis to help determine if certain improvements are fruitful for the environment at hand.&lt;/p&gt; &lt;p&gt;Example: Figure 14 shows the results of our default DQN compared against the SB3 QRDQN algorithm with default hyperparameters and the SBE QRDQN algorithm with hyperparameters matching our default DQN implementation. The QRDQN algorithm is an improvement over DQN that aims to capture the distribution over returns instead of a point estimate of the expected return. This implementation is more complex but allows for a precise estimate that aids in stability. The results show that this improvement was rather negligible on msuite, and unless credit assignment is the major concern in the environment at hand, a different improvement may prove more useful.&lt;/p&gt; &lt;div style=&quot;text-align: center;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar52-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar52-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar52-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-bsuite-applications/radar52.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 14. Comparison of DQN with QRDQN variants. &lt;/div&gt; &lt;/div&gt; &lt;h3 id=&quot;future-work-4&quot;&gt;Future Work&lt;/h3&gt; &lt;p&gt;Since bsuite provides quantitative results, one avenue of research is to create a recommender system that uses information from previous bsuite analyses to recommend improvements in DRL algorithms. The practitioner would need to provide as input the most important capabilities that an environment is believed to exhibit, and bsuite would tailor recommendations towards those capabilities. Such a recommender system could save compute time, increase performance, and ultimately expose the practitioner to new and exciting algorithmic possibilities.&lt;/p&gt; &lt;h2 id=&quot;6-conclusion&quot;&gt;6. Conclusion&lt;/h2&gt; &lt;p&gt;Traditional RL benchmarks contain many confounding variables, which makes analysis of agent performance rather opaque. In contrast, bsuite provides targeted environments that help gauge agent prowess in one or few core capabilities. The goal of bsuite is to help bridge the gap between practical theory and practical algorithms, yet there currently is no database or list of example use cases for the practitioner. Our work extends bsuite by providing concrete examples of its use, with a few examples in each of five categories. We supply at least one possible avenue of related future work or research in reproducible RL for each category. In its current state, bsuite is poised to be a standard RL benchmark for years to come due to its acceptance in a top-tier venue, well-structured codebase, multiple tutorials, and over 100 citations in the past few years in a relatively small field. We aim to help propel bsuite, and more generally methodical and reproducible RL research, into the mainstream through our explicit use cases and examples. With a diverse set of examples to choose from, we intend for applied RL practitioners to understand more use cases of bsuite, apply and document the use of bsuite in their experiments, and ultimately help bridge the gap between practical theory and practical algorithms.&lt;/p&gt; &lt;h3 id=&quot;green-computing-statement&quot;&gt;Green Computing Statement&lt;/h3&gt; &lt;p&gt;The use of bsuite can provide directed improvements in algorithms, from high-level model selection and improvement to lower-level debugging, testing, and hyperparameter tuning. Due to the current climate crisis, we feel that thoroughly-tested and accessible ideas that can reduce computational cost should be promoted to a wide audience of researchers.&lt;/p&gt; &lt;h3 id=&quot;inclusive-computing-statement&quot;&gt;Inclusive Computing Statement&lt;/h3&gt; &lt;p&gt;Many of the ideas in bsuite and this blog post are most helpful in regimes with low compute resources because of the targeted nature of these works. Due to the increasing gap between compute power of various research teams, we feel that thoroughly-tested and accessible ideas that can benefit teams with meagre compute power should be promoted to a wide audience of researchers.&lt;/p&gt; &lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt; &lt;p&gt;{Redacted for peer-review}&lt;/p&gt; </content> </entry> <entry> <title>Autoregressive Renaissance in Neural PDE Solvers</title> <link href="https://jocelynshen.com/blog/2022/autoregressive-neural-pde-solver/"/> <updated>2022-12-01T00:00:00+00:00</updated> <id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/autoregressive-neural-pde-solver</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;blockquote&gt; Improving PDE solvers has trickle down benefits to a vast range of other fields. &lt;/blockquote&gt; &lt;p&gt;Partial differential equations (PDEs) play a crucial role in modeling complex systems and understanding how they change over time and in space.&lt;/p&gt; &lt;p&gt;They are used across physics and engineering, modeling a wide range of physical phenomena like heat transfer, sound waves, electromagnetism, and fluid dynamics, but they can also be used in finance to model the behavior of financial markets, in biology to model the spread of diseases, and in computer vision to model the processing of images.&lt;/p&gt; &lt;p&gt;They are particularly interesting in deep learning!&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;span style=&quot;color:#9444e2;&quot;&gt;Neural networks can be used to solve complex PDEs.&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span style=&quot;color:#9444e2;&quot;&gt;Embedding knowledge of a PDE into a neural network can help it generalize better and/or use less data&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span style=&quot;color:#9444e2;&quot;&gt;PDEs can help explain and/or interpret neural networks.&lt;/span&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Despite their long history, dating back to equations first formalized by Euler over 250 years ago, finding numerical solutions to PDEs continues to be a challenging problem.&lt;/p&gt; &lt;p&gt;The recent advances in machine learning and artificial intelligence have opened up new possibilities for solving PDEs in a more efficient and accurate manner. These developments have the potential to revolutionize many fields, leading to a better understanding of complex systems and the ability to make more informed predictions about their behavior.&lt;/p&gt; &lt;p&gt;The background and problem set up precedes a brief look into classical and neural solvers, and finally discusses the message passing neural PDE solver (MP-PDE) introduced by Brandstetter et al. &lt;d-cite key=&quot;brandstetterMessagePassingNeural2022a&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt; &lt;h3 id=&quot;lets-brush-up-on-the-basics&quot;&gt;Let&apos;s brush up on the basics…&lt;/h3&gt; &lt;p&gt;&lt;em&gt;The notation and definitions provided match those in the paper for consistency, unless otherwise specified.&lt;/em&gt;&lt;/p&gt; &lt;div&gt; &lt;p&gt; Ordinary differential equations (ODEs) describe how a function changes with respect to a &lt;span style=&quot;color:#9444e2&quot;&gt;single independent variable&lt;/span&gt; and its derivatives. In contrast, PDEs are mathematical equations that describe the behavior of a dependent variable as it changes with respect to &lt;span style=&quot;color:#9444e2&quot;&gt;multiple independent variables&lt;/span&gt; and their derivatives. &lt;/p&gt; &lt;p&gt; Formally, for one time dimension and possibly multiple spatial dimensions denoted by \(\textbf{x}=[x_{1},x_{2},x_{3},\text{...}]^{\top} \in \mathbb{X}\), a general (temporal) PDE may be written as &lt;/p&gt; &lt;p&gt; $$\partial_{t}\textbf{u}= F\left(t, \textbf{x}, \textbf{u},\partial_{\textbf{x}}\textbf{u},\partial_{\textbf{xx}}\textbf{u},\text{...}\right) \qquad (t,\mathbf{x}) \in [0,T] \times \mathbb{X}$$ &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Initial condition: \(\mathbf{u}(0,\mathbf{x})=\mathbf{u}^{0}(\mathbf{x})\) for \(\mathbf{x} \in \mathbb{X}\)&lt;/li&gt; &lt;li&gt;Boundary conditions: \(B[ \mathbf{u}](t,x)=0\) for \((t,\mathbf{x}) \in [0,T] \times \partial \mathbb{X}\)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=&quot;fake-img l-gutter&quot;&gt; &lt;p&gt; Many equations are solutions to such PDEs alone. For example, the wave equation is given by \(\partial_{tt}u = \partial_{xx}u\). You will find that any function in the form \(u(x,t)=F(x-ct)+\) \(G(x+ct)\) is a potential solution. Initial conditions are used to specify how a PDE &quot;starts&quot; in time, and boundary conditions determine the value of the solution at the boundaries of the region where the PDE is defined. &lt;/p&gt; &lt;/div&gt; &lt;details&gt;&lt;summary&gt;Types of boundary conditions&lt;/summary&gt; Dirichlet boundary conditions prescribe a fixed value of the solution at a particular point on the boundary of the domain. Neumann boundary conditions, on the other hand, prescribe the rate of change of the solution at a particular point on the boundary. There are also mixed boundary conditions, which involve both Dirichlet and Neumann conditions, and Robin boundary conditions, which involve a linear combination of the solution and its derivatives at the boundary. &lt;/details&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;div class=&quot;l-body-outset&quot;&gt; &lt;iframe src=&quot;/deep-learning-blog.github.io/assets/html/2022-12-01-autoregressive-neural-pde-solver/slider.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;750px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Example of the wave equation PDE \(\partial^{2}_{t}u = c^{2}\partial^{2}_ {\mathbf{x}}u\). Drag the slider to watch it evolve in time! &lt;/div&gt; &lt;p&gt;The study of PDEs is in itself split into many broad fields. Briefly, these are two other important properties in addition to the initial and boundary conditions:&lt;/p&gt; &lt;details&gt;&lt;summary&gt;Linearity&lt;/summary&gt; &lt;ul&gt; &lt;li&gt;Linear: the highest power of the unknown function appearing in the equation is one (i.e., a linear combination of the unknown function and its derivatives)&lt;/li&gt; &lt;li&gt;Nonlinear: the highest power of the unknown function appearing in the equation is greater than one&lt;/li&gt; &lt;/ul&gt; &lt;/details&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;details&gt;&lt;summary&gt;Homogeneity&lt;/summary&gt; &lt;ul&gt; &lt;li&gt;Homogeneous: PDEs with no constant terms (i.e., the right-hand side is equal to zero)&lt;/li&gt; &lt;li&gt;Inhomogeneous: PDEs with a non-zero constant term on the right-hand side&lt;/li&gt; &lt;/ul&gt; &lt;/details&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;PDEs can be either linear or nonlinear, homogeneous or inhomogeneous, and can contain a combination of constant coefficients and variable coefficients. They can also involve a variety of boundary conditions, such as Dirichlet, Neumann, and Robin conditions, and can be solved using analytical, numerical, or semi-analytical methods &lt;d-cite key=&quot;straussPartialDifferentialEquations2007&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;hr style=&quot;width:40%&quot; /&gt; &lt;p&gt;Brandstetter et al. &lt;d-cite key=&quot;brandstetterMessagePassingNeural2022a&quot;&gt;&lt;/d-cite&gt; follow precedence set by Li et al. &lt;d-cite key=&quot;liFourierNeuralOperator2021&quot;&gt;&lt;/d-cite&gt; and Bar-Sinai et al. &lt;d-cite key=&quot;bar-sinaiLearningDatadrivenDiscretizations2019&quot;&gt;&lt;/d-cite&gt;to focus on &lt;span style=&quot;color:#9444e2;&quot;&gt;PDEs written in conservation form&lt;/span&gt;:&lt;/p&gt; &lt;p style=&quot;text-align:center;&quot;&gt; \(\partial_{t} \mathbf{u} + \nabla \cdot \mathbf{J}(\mathbf{u}) = 0\) &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;\(J\) is the flux, or the amount of some quantity that is flowing through a region at a given time&lt;/p&gt; &lt;/li&gt; &lt;li&gt;&lt;p&gt;\(\nabla \cdot J\) is the divergence of the flux, or the amount of outflow of the flux at a given point&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Additionally, they consider &lt;span style=&quot;color:#9444e2;&quot;&gt;Dirichlet and Neumann&lt;/span&gt; boundary conditions.&lt;/p&gt; &lt;h3 id=&quot;solving-pdes-the-classical-way&quot;&gt;Solving PDEs the classical way&lt;/h3&gt; &lt;p&gt;A brief search in a library will find numerous books detailing how to solve various types of PDEs. &lt;!-- Since Brandstetter et al. proposes to numerically solve PDEs, numerical methods are discussed in more detail. --&gt;&lt;/p&gt; &lt;details&gt;&lt;summary&gt;Analytical methods: an exact solution to a PDE can be found by mathematical means &lt;d-cite key=&quot;straussPartialDifferentialEquations2007&quot;&gt;&lt;/d-cite&gt;.&lt;/summary&gt;&lt;br /&gt; &lt;ul&gt; &lt;li&gt;Separation of Variables&lt;ul&gt; &lt;li&gt;This method involves expressing the solution as the product of functions of each variable, and then solving each function individually. It is mainly used for linear PDEs that can be separated into two or more ordinary differential equations.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Green&amp;#39;s Functions&lt;ul&gt; &lt;li&gt;This method involves expressing the solution in terms of a Green&amp;#39;s function, which is a particular solution to a homogeneous equation with specified boundary conditions.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/details&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;details&gt;&lt;summary&gt;Semi-analytical methods: an analytical solution is combined with numerical approximations to find a solution &lt;d-cite key=&quot;bartelsNumericalApproximationPartial&quot;&gt;&lt;/d-cite&gt;.&lt;/summary&gt;&lt;br /&gt; &lt;ul&gt; &lt;li&gt;Perturbation methods&lt;ul&gt; &lt;li&gt;This method is used when the solution to a PDE is close to a known solution or is a small deviation from a known solution. The solution is found by making a perturbation to the known solution and solving the resulting equation analytically.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Asymptotic methods&lt;ul&gt; &lt;li&gt;In this method, the solution is represented as a series of terms that are solved analytically. The solution is then approximated by taking the leading terms of the series.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/details&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;blockquote&gt; Very few PDEs have analytical solutions, so numerical methods have been developed to approximate PDE solutions over a wider range of potential problems. &lt;/blockquote&gt; &lt;h4 id=&quot;numerical-methods&quot;&gt;Numerical Methods&lt;/h4&gt; &lt;p&gt;Often, approaches for temporal PDEs follow the &lt;span style=&quot;color:#9444e2;&quot;&gt;method of lines (&lt;abbr title=&quot;method of lines&quot;&gt;MOL&lt;/abbr&gt;)&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;Every point of the discretization is then thought of as a separate ODE evolving in time, enabling the use of ODE solvers such as Runge-Kutta methods.&lt;/p&gt; &lt;details&gt;&lt;summary&gt;1. Discretizing the problem&lt;/summary&gt;&lt;br /&gt; &lt;p&gt; In the most basic case (&lt;span style=&quot;color:#9444e2;&quot;&gt;a regular grid&lt;/span&gt;), arbitrary spatial and temporal resolutions \(\mathbf{n_{x}}\) and \(n_{t}\) can be chosen and thus used to create a grid where \(\mathbf{n_{x}}\) is a vector containing a resolution for each spatial dimension. &lt;/p&gt; &lt;hr style=&quot;width:40%&quot; /&gt; &lt;p&gt; The domain may also be &lt;span style=&quot;color:#9444e2;&quot;&gt;irregularly sampled, resulting in a grid-free discretization&lt;/span&gt;. This is often the case with real-world data that comes from scattered sensors, for example. &lt;/p&gt; &lt;p&gt;&lt;abbr title=&quot;finite difference method&quot;&gt;FDM&lt;/abbr&gt; or any other time discretization technique can be used to discretize the time domain. &lt;/p&gt; &lt;p&gt; One direction of ongoing research seeks to determine discretization methods which can result in more efficient numerical solvers (for example, take larger steps in flatter regions and smaller steps in rapidly changing regions). &lt;/p&gt; &lt;/details&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;details&gt;&lt;summary&gt;2. Estimating the spatial derivatives&lt;/summary&gt;&lt;br /&gt; &lt;p&gt; A popular choice when using a gridded discretization is the &lt;span style=&quot;color:#9444e2;&quot;&gt;finite difference method (FDM)&lt;/span&gt;. Spatial derivative operators are replaced by a stencil which indicates how values at a finite set of neighboring grid points are combined to approximate the derivative at a given position. This stencil is based on the Taylor series expansion. &lt;/p&gt; &lt;p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/fdm_animation.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/fdm_animation.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/fdm_animation.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/fdm_animation.gif&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/p&gt; &lt;div class=&quot;caption&quot;&gt; Credits: Augusto Peres, Inductiva &lt;d-cite key=&quot;HeatHeatEquation&quot;&gt;&lt;/d-cite&gt;. &lt;/div&gt; &lt;hr style=&quot;width:40%&quot; /&gt; &lt;p&gt; The &lt;span style=&quot;color:#9444e2;&quot;&gt;finite volume method (FVM)&lt;/span&gt; is another approach which works for irregular geometries. Rather than requiring a grid, the computation domain can be divided into discrete, non-overlapping control volumes used to compute the solution for that portion &lt;d-cite key=&quot;bartelsNumericalApproximationPartial&quot;&gt;&lt;/d-cite&gt;. &lt;/p&gt; &lt;p&gt; While this method &lt;span style=&quot;color:#9444e2;&quot;&gt;only works for conservation form equations&lt;/span&gt;, it can handle complex problems with irregular geometries and fluxes that are difficult to handle with other numerical techniques such as the &lt;abbr title=&quot;finite difference method&quot;&gt;FDM&lt;/abbr&gt;. &lt;/p&gt; &lt;hr style=&quot;width:40%&quot; /&gt; &lt;p&gt; In the &lt;span style=&quot;color:#9444e2;&quot;&gt;pseudospectral method (PSM)&lt;/span&gt;, PDEs are solved pointwise in physical space by using basis functions to approximate the spatial derivatives &lt;d-cite key=&quot;brandstetterMessagePassingNeural2022a&quot;&gt;&lt;/d-cite&gt;. &lt;/p&gt; &lt;p&gt; It is well-suited for solving problems with &lt;span style=&quot;color:#9444e2;&quot;&gt;smooth solutions and periodic boundary conditions&lt;/span&gt;, but its performance drops for irregular or non-smooth solutions. &lt;/p&gt; &lt;/details&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;details&gt;&lt;summary&gt;3. Time updates&lt;/summary&gt;&lt;br /&gt; The resulting problem is a set of temporal ODEs which can be solved with classical ODE solvers such as any member of the Runge-Kutta method family. &lt;/details&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;h4 id=&quot;limitations-of-classical-methods&quot;&gt;Limitations of Classical Methods&lt;/h4&gt; &lt;p&gt;The properties of a PDE, such as its order, linearity, homogeneity, and boundary conditions, determine its solution method. &lt;span style=&quot;color:#9444e2;&quot;&gt;Different methods have been developed based on the different properties and requirements of the problem at hand.&lt;/span&gt; Brandstetter at al. categorizes these requirements into the following &lt;d-cite key=&quot;brandstetterMessagePassingNeural2022a&quot;&gt;&lt;/d-cite&gt;:&lt;/p&gt; &lt;div&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;User&lt;/th&gt; &lt;th&gt;Structural&lt;/th&gt; &lt;th&gt;Implementational&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Computation efficiency, computational cost, accuracy, guarantees (or uncertainty estimates), generalization across PDEs&lt;/td&gt; &lt;td&gt;Spatial and temporal resolution, boundary conditions, domain sampling regularity, dimensionality&lt;/td&gt; &lt;td&gt;Stability over long rollouts, preservation of invariants&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt; The countless combinations of requirements resulted in what Bartels defines as a &lt;span style=&quot;color:#9444e2;&quot;&gt;splitter field&lt;/span&gt; &lt;d-cite key=&quot;bartelsNumericalApproximationPartial&quot;&gt;&lt;/d-cite&gt;: a specialized classical solver is developed for each sub-problems, resulting in many specialized tools rather than a single one. &lt;/p&gt; &lt;p&gt; These methods, while effective and mathematically proven, often come at high computation costs. Taking into account that PDEs often exhibit chaotic behaviour and are sensitive to any changes in their parameters, &lt;span style=&quot;color:#ff4f4b;&quot;&gt;re-running a solver every time a coefficient or boundary condition changes in a single PDE can be computationally expensive&lt;/span&gt;. &lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;fake-img l-gutter&quot;&gt; &lt;p&gt; Courant–Friedrichs–Lewy (CFL) condition &lt;/p&gt; &lt;p&gt; &lt;it&gt;The maximum time step size should be proportional to the minimum spatial grid size.&lt;/it&gt; &lt;/p&gt; &lt;p&gt; According to this condition, as the number of dimensions increases, the size of the temporal step must decrease and therefore numerical solvers become very slow for complex PDEs. &lt;/p&gt; &lt;/div&gt; &lt;h3 id=&quot;neural-solvers&quot;&gt;Neural Solvers&lt;/h3&gt; &lt;p&gt; Neural solvers offer some very desirable properties that may serve to unify some of this splitter field. Neural networks can &lt;span style=&quot;color:#9444e2;&quot;&gt;learn and generalize to new contexts&lt;/span&gt; such as different initial/boundary conditions, coefficients, or even different PDEs entirely &lt;d-cite key=&quot;brandstetterMessagePassingNeural2022a&quot;&gt;&lt;/d-cite&gt;. They can also circumvent the CFL condition, making them a promising avenue for solving highly complex PDEs such as those found in weather prediction. &lt;/p&gt; &lt;h4 id=&quot;neural-operator-methods&quot;&gt;Neural operator methods&lt;/h4&gt; &lt;p&gt;Neural operator methods &lt;span style=&quot;color:#9444e2;&quot;&gt;model the solution of a PDE as an operator that maps inputs to outputs&lt;/span&gt;. The problem is set such that a neural operator \(\mathcal{M}\) satisfies \(\mathcal{M}(t,\mathbf{u}^{0}) = \mathbf{u}(t)\) where \(\mathbf{u}^{0}\) are the initial conditions &lt;d-cite key=&quot;luDeepONetLearningNonlinear2021, brandstetterMessagePassingNeural2022a&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt; One of the current state-of-the-art models is the &lt;span style=&quot;color:#9444e2;&quot;&gt;Fourier Neural Operator (FNO)&lt;/span&gt; &lt;d-cite key=&quot;liFourierNeuralOperator2021&quot;&gt;&lt;/d-cite&gt;. It operates within Fourier space and takes advantage of the convolution theorem to place the integral kernel in Fourier space as a convolutional operator. &lt;/p&gt; &lt;div&gt; &lt;p&gt; These global integral operators (implemented as Fourier space convolutional operators) are combined with local nonlinear activation functions, resulting in an architecture which is &lt;span style=&quot;color:#9444e2;&quot;&gt;highly expressive yet computationally efficient, as well as being resolution-invariant&lt;/span&gt;. &lt;/p&gt; &lt;p&gt; While the vanilla &lt;abbr title=&quot;Fourier neural operator&quot;&gt;FNO&lt;/abbr&gt; required the input function to be defined on a grid due to its reliance on the FFT, further work developed mesh-independent variations as well &lt;d-cite key=&quot;kovachkiNeuralOperatorLearning2022&quot;&gt;&lt;/d-cite&gt;. &lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;fake-img l-gutter&quot;&gt; &lt;p&gt; Convolution Theorem &lt;/p&gt; &lt;p&gt; The Fourier transform of the convolution of two signals is equal to the pointwise product of their individual Fourier transforms &lt;/p&gt; &lt;/div&gt; &lt;p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/FNO-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/FNO-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/FNO-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/FNO.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/p&gt; &lt;div class=&quot;caption&quot;&gt; &lt;abbr title=&quot;Fourier neural operator&quot;&gt;FNO&lt;/abbr&gt; architecture. For more details, see &lt;a href=&quot;https://zongyi-li.github.io/blog/2020/fourier-pde/&quot;&gt;this blogpost&lt;/a&gt;. Credits: Li et al. &lt;d-cite key=&quot;liFourierNeuralOperator2021&quot;&gt;&lt;/d-cite&gt;. &lt;/div&gt; &lt;p&gt; Neural operators are able to operate on multiple domains and can be completely data-driven. &lt;/p&gt; &lt;p&gt; However, these models &lt;span style=&quot;color:#ff4f4b;&quot;&gt;do not tend to predict out-of-distribution \(t\)&lt;/span&gt; and are therefore limited when dealing with temporal PDEs. Another major barrier is their relative &lt;span style=&quot;color:#ff4f4b;&quot;&gt;lack of interpretability and guarantees&lt;/span&gt; compared to classical solvers. &lt;/p&gt; &lt;h4 id=&quot;autoregressive-methods&quot;&gt;Autoregressive methods&lt;/h4&gt; &lt;p&gt;While neural operator methods directly mapped inputs to outputs, &lt;span style=&quot;color:#9444e2;&quot;&gt;autoregressive methods take an iterative approach instead&lt;/span&gt;. For example, iterating over time results in a problem such as \(\mathbf{u}(t+\Delta t) = \mathcal{A}(\Delta t, \mathbf{u}(t))\) where \(\mathcal{A}\) is some temporal update &lt;d-cite key=&quot;brandstetterMessagePassingNeural2022a&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;div class=&quot;l-body-outset&quot;&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/rnn-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/rnn-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/rnn-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/rnn.png&quot; class=&quot;img-fluid rounded&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/wavenet.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/wavenet.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/wavenet.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/wavenet.gif&quot; class=&quot;img-fluid rounded&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Similarly to &lt;abbr title=&quot;recurrent neural networks&quot;&gt;RNN&lt;/abbr&gt;s (left), autoregressive models take previous time steps to predict the next time step. However, autoregressive models (right) are entirely feed-forward and take the previous predictions as inputs rather than storing them in some hidden state. Credits: RNN diagram from Colah&apos;s Blog &lt;d-cite key=&quot;UnderstandingLSTMNetworks&quot;&gt;&lt;/d-cite&gt;, WaveNet from Deepmind Blog &lt;d-cite key=&quot;WaveNetGenerativeModel&quot;&gt;&lt;/d-cite&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;Three autoregressive works mentioned by Brandstetter et al. are hybrid methods which use neural networks to predict certain parameters for finite volume, multigrid, and iterative finite elements methods. &lt;span style=&quot;color:#9444e2;&quot;&gt;All three retain a (classical) computation grid which makes them somewhat interpretable&lt;/span&gt; &lt;d-cite key=&quot;bar-sinaiLearningDatadrivenDiscretizations2019, greenfeldLearningOptimizeMultigrid2019a, hsiehLearningNeuralPDE2019&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;div class=&quot;fake-img l-gutter&quot;&gt; &lt;p&gt; Other autoregressive models include PixelCNN for images, WaveNet for audio, and the Transformer for text. &lt;/p&gt; &lt;/div&gt; &lt;p&gt;However, autoregressive models have not gained the acclaim seen by neural operators as a whole.&lt;/p&gt; &lt;p&gt;This is on one hand due to their &lt;span style=&quot;color:#ff4f4b;&quot;&gt;limitations in generalization&lt;/span&gt;. In Hsieh et al.’s case, an existing numerical method must be used to craft a complementary neural iterator &lt;d-cite key=&quot;hsiehLearningNeuralPDE2019&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Another major concern is the &lt;span style=&quot;color:#ff4f4b;&quot;&gt;accumulation of error&lt;/span&gt;, which is particularly detrimental for PDE problems that often exhibit chaotic behavior &lt;d-cite key=&quot;brandstetterMessagePassingNeural2022a&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h2 id=&quot;message-passing-neural-pde-solver-mp-pde&quot;&gt;Message Passing Neural PDE Solver (MP-PDE)&lt;/h2&gt; &lt;p&gt;Brandstetter et al. propose a &lt;span style=&quot;color:#9444e2;&quot;&gt;fully neural PDE solver which capitalizes on neural message passing&lt;/span&gt;. The overall architecture is laid out below, consisting of an &lt;abbr title=&quot;multilayer perceptron&quot;&gt;MLP&lt;/abbr&gt; encoder, a &lt;abbr title=&quot;graph neural network&quot;&gt;GNN&lt;/abbr&gt; processor, and a CNN decoder.&lt;/p&gt; &lt;div class=&quot;l-body-outset&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/MP-PDE-Solver-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/MP-PDE-Solver-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/MP-PDE-Solver-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/MP-PDE-Solver.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Overall MP-PDE architecture. Credits: Brandstetter et al. &lt;d-cite key=&quot;brandstetterMessagePassingNeural2022a&quot;&gt;&lt;/d-cite&gt;. &lt;/div&gt; &lt;/div&gt; &lt;p&gt;At its core, this model is autoregressive and thus faces the same challenge listed above. Two key contributions of this work are the &lt;span style=&quot;color:#9444e2;&quot;&gt;pushforward trick and temporal bundling which mitigate the potential butterfly effect of error accumulation&lt;/span&gt;&lt;d-cite key=&quot;brandstetterMessagePassingNeural2022a&quot;&gt;&lt;/d-cite&gt;. The network itself, being fully neural, is capable of generalization across many changes as well.&lt;/p&gt; &lt;h3 id=&quot;the-pushforward-trick-and-temporal-bundling&quot;&gt;The Pushforward Trick and Temporal Bundling&lt;/h3&gt; &lt;div class=&quot;l-body-outset&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/pushforward3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/pushforward3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/pushforward3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/pushforward3.jpg&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Pushforward trick compared to one-step and unrolled training. Credits: Brandstetter et al. &lt;d-cite key=&quot;brandstetterMessagePassingNeural2022a&quot;&gt;&lt;/d-cite&gt;. &lt;/div&gt; &lt;/div&gt; &lt;p&gt;During testing, the model uses current time steps (first from data, then &lt;span style=&quot;color:#9444e2;&quot;&gt;from its own predictions&lt;/span&gt;) to approximate the next time step.&lt;/p&gt; &lt;p&gt;This results in a distribution shift problem because the inputs are no longer solely from ground truth data: &lt;span style=&quot;color:#9444e2;&quot;&gt;the distribution learned during training will always be an approximation of the true data distribution&lt;/span&gt;. The model will appear to overfit to the one-step training distribution and perform poorly the further it continues to predict.&lt;/p&gt; &lt;p&gt;An adversarial-style stability loss is added to the one-step loss so that the training distribution is brought closer to the test time distribution &lt;d-cite key=&quot;brandstetterMessagePassingNeural2022a&quot;&gt;&lt;/d-cite&gt;:&lt;/p&gt; &lt;details&gt;&lt;summary style=&quot;text-align:center;color:black;&quot;&gt; \(L_{\text{one-step}} =\) &lt;span style=&quot;color:#23a15c;&quot;&gt;\(\mathbb{E}_{k}\)&lt;/span&gt; &lt;span style=&quot;color:#928b54;&quot;&gt;\(\mathbb{E}_{\mathbf{u^{k+1}|\mathbf{u^{k},\mathbf{u^{k} \sim p_{k}}}}}\)&lt;/span&gt; \([\) &lt;span style=&quot;color:#5588e0;&quot;&gt;\(\mathcal{L}\)&lt;/span&gt; \((\) &lt;span style=&quot;color:#9444e2;&quot;&gt;\(\mathcal{A}(\mathbf{u}^{k})\)&lt;/span&gt; \(,\) &lt;span style=&quot;color:#46b4af;&quot;&gt;\(\mathbf{u}^{k+1}]\)&lt;/span&gt; &lt;/summary&gt; &lt;p&gt; The &lt;span style=&quot;color:#5588e0;&quot;&gt;loss function&lt;/span&gt; is used to evaluate the difference between the &lt;span style=&quot;color:#9444e2;&quot;&gt;temporal update&lt;/span&gt; and the &lt;span style=&quot;color:#46b4af;&quot;&gt;expected next state&lt;/span&gt;, and the overall one-step loss is calculated as the expected value of this loss over &lt;span style=&quot;color:#23a15c;&quot;&gt;all time-steps&lt;/span&gt; and &lt;span style=&quot;color:#928b54;&quot;&gt;all possible next states&lt;/span&gt;. &lt;/p&gt; &lt;/details&gt; &lt;p&gt;&lt;br style=&quot;line-height:5px&quot; /&gt;&lt;/p&gt; &lt;p style=&quot;text-align:center;&quot;&gt; \(L_{\text{stability}} = \mathbb{E}_{k}\mathbb{E}_{\mathbf{u^{k+1}|\mathbf{u^{k},\mathbf{u^{k} \sim p_{k}}}}}[\mathbb{E}_{\epsilon | \mathbf{u}^{k}} [\mathcal{L}(\mathcal{A}(\mathbf{u}^{k}+\) &lt;span style=&quot;color:#faad18;&quot;&gt;\(\epsilon\)&lt;/span&gt; \()),\mathbf{u}^{k+1}]]\) &lt;/p&gt; &lt;p style=&quot;text-align:center;&quot;&gt; \(L_{\text{total}} = L_{\text{one-step}} + L_{\text{stability}}\) &lt;/p&gt; &lt;p&gt; The stability loss is largely based off the one-step loss, but now assumes that the temporal update uses &lt;span style=&quot;color:#faad18;&quot;&gt;noisy data&lt;/span&gt;. &lt;/p&gt; &lt;p&gt; The pushforward trick lies in the choice of &lt;span style=&quot;color:#faad18;&quot;&gt;\(\epsilon\)&lt;/span&gt; such that \(\mathbf{u}^{k}+\epsilon = \mathcal{A}(\mathbf{u}^{k-1})\), similar to the test time distribution. Practically, it is implemented to be &lt;span style=&quot;color:#9444e2;&quot;&gt;noise from the network itself&lt;/span&gt; so that as the network improves, the loss decreases. &lt;/p&gt; &lt;p&gt; Necessarily, the noise of the network must be known or calculated to implement this loss term. So, &lt;span style=&quot;color:#9444e2;&quot;&gt;the model is unrolled for 2 steps&lt;/span&gt; but only backpropagated over the most recent unroll step, which already has the neural network noise &lt;d-cite key=&quot;brandstetterMessagePassingNeural2022a&quot;&gt;&lt;/d-cite&gt;. &lt;/p&gt; &lt;p&gt; While the network could be unrolled during training, this not only slows the training down but also might result in the network learning shortcuts across unrolled steps. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Temporal bundling&lt;/strong&gt;&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-8&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/NN-AR-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/NN-AR-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/NN-AR-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/NN-AR.jpg&quot; class=&quot;img-fluid rounded&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-4&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/temporalbundling-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/temporalbundling-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/temporalbundling-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/temporalbundling.jpg&quot; class=&quot;img-fluid rounded&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Temporal bundling compared to neural operators and autoregressive models. Credits: Brandstetter et al. &lt;d-cite key=&quot;brandstetterMessagePassingNeural2022a&quot;&gt;&lt;/d-cite&gt;. &lt;/div&gt; &lt;!-- &lt;/div&gt; --&gt; &lt;p&gt;This trick complements the previous by &lt;span style=&quot;color:#9444e2;&quot;&gt;reducing the amount of times the test time distribution changes&lt;/span&gt;. Rather than predicting a single value at a time, the MP-PDE predicts multiple time-steps at a time, as seen above &lt;d-cite key=&quot;brandstetterMessagePassingNeural2022a&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h3 id=&quot;network-architecture&quot;&gt;Network Architecture&lt;/h3&gt; &lt;p&gt;&lt;abbr title=&quot;graph neural network&quot;&gt;GNN&lt;/abbr&gt;s have been used as PDE solvers in a variety of works &lt;d-cite key=&quot;liNeuralOperatorGraph2020, eliasofPdegcnNovelArchitectures2021, iakovlevLearningContinuoustimePDEs2021&quot;&gt;&lt;/d-cite&gt;; however, in this implementation, &lt;span style=&quot;color:#9444e2;&quot;&gt;links can be drawn directly from the &lt;abbr title=&quot;method of lines&quot;&gt;MOL&lt;/abbr&gt; to each component of the network architecture centering around the use of a message passing algorithm.&lt;/span&gt;&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Classical Numerical Method&lt;/th&gt; &lt;th&gt;MP-PDE Network Component&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Partitioning the problem onto a grid&lt;/td&gt; &lt;td&gt;Encoder &lt;br /&gt;&lt;em&gt;Encodes a vector of solutions into node embeddings&lt;/em&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Estimating the spatial derivatives&lt;/td&gt; &lt;td&gt;Processor &lt;br /&gt;&lt;em&gt;Estimates spatial derivatives via message passing&lt;/em&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Time updates&lt;/td&gt; &lt;td&gt;Decoder &lt;br /&gt;&lt;em&gt;Combines some representation of spatial derivatives smoothed into a time update&lt;/em&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;ol&gt; &lt;li&gt;Encoder &lt;p&gt; The encoder is implemented as a two-layer &lt;abbr title=&quot;multilayer perceptron&quot;&gt;MLP&lt;/abbr&gt; which computes an embedding for each node \(i\) to cast the data to a &lt;span style=&quot;color:#9444e2;&quot;&gt;non-regular integration grid&lt;/span&gt;: &lt;/p&gt; &lt;details&gt;&lt;summary style=&quot;text-align:center;color:black;&quot;&gt;\(\mathbf{f}_{i}^{0} = \epsilon^{v}([\mathbf{u}_{i}^{k-K:k},\mathbf{x}_{i},t_{k},\theta_{PDE}])\) &lt;/summary&gt; where \(\mathbf{u}_{i}^{k-K:k}\) is a vector of previous solutions (the length equaling the temporal bundle length), \(\mathbf{x}_{i}\) is the node&apos;s position, \(t_{k}\) is the current timestep, and \(\theta_{PDE}\) holds equation parameters. &lt;/details&gt; &lt;/li&gt; &lt;li&gt; Processor &lt;p&gt; The node embeddings from the encoder are then used in a message passing &lt;abbr title=&quot;graph neural network&quot;&gt;GNN&lt;/abbr&gt;. &lt;a id=&quot;spatialderivative&quot; style=&quot;text-decoration:none;&quot;&gt;The message passing algorithm, which approximates spatial derivatives, is run \(M\) steps using the following updates:&lt;/a&gt; &lt;/p&gt; &lt;details&gt;&lt;summary style=&quot;text-align:center;color:black;&quot;&gt; \(\text{edge } j \to i \text{ message:} \qquad \mathbf{m}_{ij}^{m} =\) &lt;span style=&quot;color:#ae46b4;&quot;&gt;\(\phi\)&lt;/span&gt; \((\) &lt;span style=&quot;color:#b4a546;&quot;&gt;\(\mathbf{f}_{i}^{m}, \mathbf{f}_{j}^{m},\)&lt;/span&gt; &lt;span style=&quot;color:steelblue;&quot;&gt;\(\mathbf{u}_{i}^{k-K:k}-\mathbf{u}_{j}^{k-K:k}\)&lt;/span&gt;, &lt;span style=&quot;color:#6546b4;&quot;&gt;\(\mathbf{x}_{i}-\mathbf{x}_{j}\)&lt;/span&gt;, &lt;span style=&quot;color:#46b4af;&quot;&gt;\(\theta_{PDE}\)&lt;/span&gt; \())\) &lt;/summary&gt; The &lt;span style=&quot;color:#6546b4;&quot;&gt;difference in spatial coordinates&lt;/span&gt; helps enforce translational symmetry and, combined with the &lt;span style=&quot;color:steelblue;&quot;&gt;difference in node solutions&lt;/span&gt;, relates the message passing to a local difference operator. The addition of the &lt;span style=&quot;color:#46b4af;&quot;&gt;PDE parameters&lt;/span&gt; is motivated by considering what the MP-PDE should generalize over: by adding this information in multiple places, flexibility can potentially be learned since all this information (as well as the &lt;span style=&quot;color:#b4a546;&quot;&gt;node embeddings&lt;/span&gt;) is fed through &lt;span style=&quot;color:#ae46b4;&quot;&gt;a two-layer &lt;abbr title=&quot;multilayer perceptron&quot;&gt;MLP&lt;/abbr&gt;&lt;/span&gt;. In addition, the solution of a PDE at any timestep must respect the boundary condition (the same as in classical methods for BVPs), so adding the &lt;span style=&quot;color:#46b4af;&quot;&gt;PDE parameters&lt;/span&gt; in the edge update provides knowledge of the boundary conditions to the neural solver. &lt;/details&gt; &lt;br /&gt; &lt;details&gt;&lt;summary style=&quot;text-align:center;color:black;&quot;&gt; \(\text{node } i \text{ update:} \qquad\) &lt;span style=&quot;color:#ff4f4b;&quot;&gt;\(\mathbf{f}_{i}^{m+1}\)&lt;/span&gt; \(=\) &lt;span style=&quot;color:#928b54;&quot;&gt;\(\psi\)&lt;/span&gt; \((\) &lt;span style=&quot;color:#5588e0;&quot;&gt;\(\mathbf{f}^{m}_{i}\)&lt;/span&gt;, &lt;span style=&quot;color:#722e4e;&quot;&gt;\(\sum_{j \in \mathcal{N}(i)} \mathbf{m}_{ij}^{m}\)&lt;/span&gt;, &lt;span style=&quot;color:#46b4af;&quot;&gt;\(\theta_{PDE}\)&lt;/span&gt; \()\) &lt;/summary&gt; The &lt;span style=&quot;color:#ff4f4b;&quot;&gt;future node embedding&lt;/span&gt; is updated using &lt;span style=&quot;color:#5588e0;&quot;&gt;the current node embedding&lt;/span&gt;, &lt;span style=&quot;color:#722e4e;&quot;&gt;the aggregation of all received messages&lt;/span&gt;, and (again) the &lt;span style=&quot;color:#46b4af;&quot;&gt;PDE parameters&lt;/span&gt;. This information is also fed through &lt;span style=&quot;color:#928b54;&quot;&gt;a two-layer &lt;abbr title=&quot;multilayer perceptron&quot;&gt;MLP&lt;/abbr&gt;&lt;/span&gt;. &lt;/details&gt;&lt;br /&gt; &lt;p&gt; Bar-Sinai et al. explores the relationship between &lt;abbr title=&quot;finite difference method&quot;&gt;FDM&lt;/abbr&gt; and &lt;abbr title=&quot;finite volume method&quot;&gt;FVM&lt;/abbr&gt; as used in the method of lines &lt;d-cite key=&quot;bar-sinaiLearningDatadrivenDiscretizations2019&quot;&gt;&lt;/d-cite&gt;. In both methods, the \(n^{th}\) order derivative at a point \(x\) is approximated by &lt;/p&gt; &lt;p style=&quot;text-align:center;&quot;&gt; \(\partial^{(n)}_{x}u \approx \sum_{i} a^{(n)}_{i} u_{i}\) &lt;/p&gt; &lt;p&gt; for some precomputed coefficients \(a^{(n)}_{i}\). &lt;span style=&quot;color:#9444e2;&quot;&gt;The right hand side parallels the message passing scheme&lt;/span&gt;, which aggregates the local difference (&lt;span style=&quot;color:steelblue;&quot;&gt;\(\mathbf{u}_{i}^{k-K:k}-\mathbf{u}_{j}^{k-K:k}\)&lt;/span&gt; in the edge update) and other (learned) embeddings over neighborhoods of nodes. &lt;/p&gt; &lt;p&gt; This relationship gives an intuitive understanding of the message passing &lt;abbr title=&quot;graph neural network&quot;&gt;GNN&lt;/abbr&gt;, which mimics &lt;abbr title=&quot;finite difference method&quot;&gt;FDM&lt;/abbr&gt; for a single layer, &lt;abbr title=&quot;finite volume method&quot;&gt;FVM&lt;/abbr&gt; for two layers, and &lt;abbr title=&quot;Weighted Essentially Non-Oscillatory (5th order)&quot;&gt;WENO5&lt;/abbr&gt; for three layers &lt;d-cite key=&quot;brandstetterMessagePassingNeural2022a&quot;&gt;&lt;/d-cite&gt;. &lt;abbr title=&quot;Weighted Essentially Non-Oscillatory (5th order)&quot;&gt;WENO5&lt;/abbr&gt; is a numerical interpolation scheme used to reconstruct the solution at cell interfaces in &lt;abbr title=&quot;finite volume method&quot;&gt;FVM&lt;/abbr&gt;. &lt;/p&gt; &lt;p&gt; While the interpretation is desirable, how far this holds in the actual function of the &lt;abbr title=&quot;message passing graph neural network&quot;&gt;MP-GNN&lt;/abbr&gt; is harder to address. The concepts of the nodes as integration points and messages as local differences break down as the nodes and edges update. In addition, the furthest node that contributes a message from for any point is at \(n\) edges away for the \(n^{th}\) layer (or a specified limit). This results in a very coarse and potentially underinformed approximation for the first layer which is then propagated to the next layers. However, both the updates use two layer &lt;abbr title=&quot;multilayer perceptron&quot;&gt;MLP&lt;/abbr&gt;s which (although abstracting away from their respective interpretations) may in effect learn optimal weightings to counterbalance this. &lt;/p&gt; &lt;/li&gt; &lt;li&gt; Decoder &lt;p&gt; The approximated spatial derivatives are then &lt;span style=&quot;color:#9444e2;&quot;&gt;combined and smoothed using a CNN&lt;/span&gt; which outputs a bundle of next time steps (recall temporal bundling) \(\mathbf{d}_{i}\). The solution is then updated: &lt;/p&gt; &lt;p style=&quot;text-align:center;&quot;&gt; \(\mathbf{u}^{k+l}_{i} = u^{k}_{i} + (t_{k+l}-t_{k})\mathbf{d}^{l}_{i}\) &lt;/p&gt; &lt;p&gt; Some precedence is seen, for example, in classical linear multistep methods which (though effective) face stability concerns. Since the CNN is adaptive, it appears that it avoids this issue &lt;d-cite key=&quot;brandstetterMessagePassingNeural2022a&quot;&gt;&lt;/d-cite&gt;. &lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt; &lt;details&gt;&lt;summary&gt;Quantitative measures: accumulated error, runtime&lt;/summary&gt; &lt;p&gt; Accumulated error: \(\frac{1}{n_{x}} \sum_{x,t} MSE\) &lt;/p&gt; &lt;p&gt; Runtime (s): Measured time taken to run for a given number of steps. &lt;/p&gt; &lt;/details&gt; &lt;blockquote&gt; As a general neural PDE solver, the &lt;abbr title=&quot;message passing graph neural network&quot;&gt;MP-GNN&lt;/abbr&gt; surpasses even the current state-of-the-art &lt;abbr title=&quot;Fourier neural operator&quot;&gt;FNO&lt;/abbr&gt;. &lt;/blockquote&gt; &lt;p&gt;For example, after training a neural model and setting up an instance of &lt;abbr title=&quot;method of lines&quot;&gt;MOL&lt;/abbr&gt;, this is a brief comparison of how they can generalize without re-training.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Generalization to...&lt;/th&gt; &lt;th&gt;&lt;abbr title=&quot;message passing graph neural network&quot;&gt;MP-GNN&lt;/abbr&gt;&lt;/th&gt; &lt;th&gt;&lt;abbr title=&quot;Fourier neural operator&quot;&gt;FNO&lt;/abbr&gt;&lt;/th&gt; &lt;th&gt;Classical (&lt;abbr title=&quot;method of lines&quot;&gt;MOL&lt;/abbr&gt;)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;New PDEs&lt;/td&gt; &lt;td&gt;Yes&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Different resolutions&lt;/td&gt; &lt;td&gt;Yes&lt;/td&gt; &lt;td&gt;Yes&lt;/td&gt; &lt;td&gt;No (unless downsampling)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Changes in PDE parameters&lt;/td&gt; &lt;td&gt;Yes&lt;/td&gt; &lt;td&gt;Yes&lt;/td&gt; &lt;td&gt;Sometimes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Non-regular grids&lt;/td&gt; &lt;td&gt;Yes&lt;/td&gt; &lt;td&gt;Some&lt;/td&gt; &lt;td&gt;Yes (dependent on implementation)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Higher dimensions&lt;/td&gt; &lt;td&gt;Yes&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;l-body-outset&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/shock_formation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/shock_formation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/shock_formation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/shock_formation.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Demonstration of shock formation using MP-PDE from different training data resolutions. Credits: Brandstetter et al. &lt;d-cite key=&quot;brandstetterMessagePassingNeural2022a&quot;&gt;&lt;/d-cite&gt;. &lt;/div&gt; &lt;/div&gt; &lt;p&gt;This experiment exemplifies the MP-PDE’s ability to model shocks (where both the &lt;abbr title=&quot;finite difference method&quot;&gt;FDM&lt;/abbr&gt; and PSM methods fail) across multiple resolutions. Even at a fifth of the resolution of the ground truth, both the small and large shocks are captured well.&lt;/p&gt; &lt;div class=&quot;l-body-outset&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/2dshock-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/2dshock-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/2dshock-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-autoregressive-neural-pde-solver/2dshock.jpg&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Demonstration of shock formation using MP-PDE from different training data resolutions. Credits: Brandstetter et al. &lt;d-cite key=&quot;brandstetterMessagePassingNeural2022a&quot;&gt;&lt;/d-cite&gt;. &lt;/div&gt; &lt;/div&gt; &lt;p&gt;The same data is displayed in 2D to show the time evolution. After about 7.5s, the error accumulation is large enough to visibly diverge from the ground truth. The predictions become unreliable due to error accumulation.&lt;/p&gt; &lt;p&gt;In practice, this survival time should be empirically found (as seen here) to determine for how long the solution is reliable. However, the ground truth would be needed for comparison, rendering this as another chicken-egg problem.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th colspan=&quot;2&quot;&gt;&lt;/th&gt; &lt;th colspan=&quot;4&quot; style=&quot;border-left:1px solid lightgrey;&quot;&gt;Accumulated Error&lt;/th&gt; &lt;th colspan=&quot;2&quot; style=&quot;border-left:1px solid lightgrey;&quot;&gt;Runtime [s]&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td colspan=&quot;2&quot;&gt; \(\quad (n_{t},n_{x})\) &lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;WENO5&lt;/td&gt; &lt;td&gt;FNO-RNN&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;FNO-PF&lt;/td&gt; &lt;td&gt;MP-PDE&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;WENO5&lt;/td&gt; &lt;td&gt;MP-PDE&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;E1&lt;/b&gt;&lt;/td&gt; &lt;td&gt;(250,100)&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;2.02&lt;/td&gt; &lt;td&gt;11.93&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;0.54&lt;/td&gt; &lt;td&gt;1.55&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;1.9&lt;/td&gt; &lt;td&gt;0.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;E1&lt;/b&gt;&lt;/td&gt; &lt;td&gt;(250, 50)&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;6.23&lt;/td&gt; &lt;td&gt;29.98&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;0.51&lt;/td&gt; &lt;td&gt;1.67&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;1.8&lt;/td&gt; &lt;td&gt;0.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;E1&lt;/b&gt;&lt;/td&gt; &lt;td&gt;(250, 40)&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;9.63&lt;/td&gt; &lt;td&gt;10.44&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;0.57&lt;/td&gt; &lt;td&gt;1.47&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;1.7&lt;/td&gt; &lt;td&gt;0.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;E2&lt;/b&gt;&lt;/td&gt; &lt;td&gt;(250, 100)&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;1.19&lt;/td&gt; &lt;td&gt;17.09&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;2.53&lt;/td&gt; &lt;td&gt;1.58&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;1.9&lt;/td&gt; &lt;td&gt;0.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;E2&lt;/b&gt;&lt;/td&gt; &lt;td&gt;(250, 50)&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;5.35&lt;/td&gt; &lt;td&gt;3.57&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;2.27&lt;/td&gt; &lt;td&gt;1.63&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;1.8&lt;/td&gt; &lt;td&gt;0.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;E2&lt;/b&gt;&lt;/td&gt; &lt;td&gt;(250, 40)&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;8.05&lt;/td&gt; &lt;td&gt;3.26&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;2.38&lt;/td&gt; &lt;td&gt;1.45&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;1.7&lt;/td&gt; &lt;td&gt;0.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;E3&lt;/b&gt;&lt;/td&gt; &lt;td&gt;(250, 100)&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;4.71&lt;/td&gt; &lt;td&gt;10.16&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;5.69&lt;/td&gt; &lt;td&gt;4.26&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;4.8&lt;/td&gt; &lt;td&gt;0.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;E3&lt;/b&gt;&lt;/td&gt; &lt;td&gt;(250, 50)&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;11.71&lt;/td&gt; &lt;td&gt;14.49&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;5.39&lt;/td&gt; &lt;td&gt;3.74&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;4.5&lt;/td&gt; &lt;td&gt;0.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;E3&lt;/b&gt;&lt;/td&gt; &lt;td&gt;(250, 40)&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;15.97&lt;/td&gt; &lt;td&gt;20.90&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;5.98&lt;/td&gt; &lt;td&gt;3.70&lt;/td&gt; &lt;td style=&quot;border-left:1px solid lightgrey;&quot;&gt;4.4&lt;/td&gt; &lt;td&gt;0.09&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;caption&quot;&gt; Table of experiment results adapted from paper. Credits: Brandstetter et al. &lt;d-cite key=&quot;brandstetterMessagePassingNeural2022a&quot;&gt;&lt;/d-cite&gt;. &lt;/div&gt; &lt;details&gt;&lt;summary&gt;Abbreviations&lt;/summary&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Shorthand&lt;/th&gt; &lt;th&gt;Meaning&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;E1&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Burgers&amp;#39; equation without diffusion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;E2&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Burgers&amp;#39; equation with variable diffusion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;E3&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Mixed equation, see below&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;\(n_{t}\)&lt;/td&gt; &lt;td&gt;Temporal resolution&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;\(n_{x}\)&lt;/td&gt; &lt;td&gt;Spatial resolution&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;WENO5&lt;/td&gt; &lt;td&gt;Weighted Essentially Non-Oscillatory (5th order)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;abbr title=&quot;Fourier neural operator&quot;&gt;FNO&lt;/abbr&gt;-&lt;abbr title=&quot;recurrent neural networks&quot;&gt;RNN&lt;/abbr&gt;&lt;/td&gt; &lt;td&gt;Recurrent variation of &lt;abbr title=&quot;Fourier neural operator&quot;&gt;FNO&lt;/abbr&gt; from original paper&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;abbr title=&quot;Fourier neural operator&quot;&gt;FNO&lt;/abbr&gt;-PF&lt;/td&gt; &lt;td&gt;&lt;abbr title=&quot;Fourier neural operator&quot;&gt;FNO&lt;/abbr&gt; with the pushforward trick added&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MP-PDE&lt;/td&gt; &lt;td&gt;Message passing neural PDE solver&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt; The authors form a general PDE in the form &lt;/p&gt; &lt;p style=&quot;text-align:center;&quot;&gt; \([\partial_{t}u + \partial_{x}(\alpha u^{2} - \beta \partial_{x} u + \gamma \partial_{xx} u)](t,x) = \delta (t,x)\) &lt;/p&gt; &lt;p style=&quot;text-align:center;&quot;&gt; \(u(0,x) = \delta(0,x)\) &lt;/p&gt; &lt;p&gt; such that \(\theta_{PDE} = (\alpha, \beta, \gamma)\) and different combinations of these result in the heat equation, Burgers&apos; equation, and the KdV equation. \(\delta\) is a forcing term, allowing for greater variation in the equations being tested. &lt;/p&gt; &lt;/details&gt; &lt;p&gt;For this same experiment, the error and runtimes were recorded when solving using &lt;abbr title=&quot;Weighted Essentially Non-Oscillatory (5th order)&quot;&gt;WENO5&lt;/abbr&gt;, the recurrent variant of the &lt;abbr title=&quot;Fourier neural operator&quot;&gt;FNO&lt;/abbr&gt; (&lt;abbr title=&quot;Fourier neural operator&quot;&gt;FNO&lt;/abbr&gt;-&lt;abbr title=&quot;recurrent neural networks&quot;&gt;RNN&lt;/abbr&gt;), the &lt;abbr title=&quot;Fourier neural operator&quot;&gt;FNO&lt;/abbr&gt; with the pushforward trick (&lt;abbr title=&quot;Fourier neural operator&quot;&gt;FNO&lt;/abbr&gt;-PF), and the MP-PDE.&lt;/p&gt; &lt;blockquote&gt; The pushforward trick is successful in mitigating error accumulation. &lt;/blockquote&gt; &lt;p&gt;Comparing the accumulated errors of &lt;abbr title=&quot;Fourier neural operator&quot;&gt;FNO&lt;/abbr&gt;-&lt;abbr title=&quot;recurrent neural networks&quot;&gt;RNN&lt;/abbr&gt; and the &lt;abbr title=&quot;Fourier neural operator&quot;&gt;FNO&lt;/abbr&gt;-PF across all experiments highlights the advantage of the pushforward trick. While the MP-PDE outperforms all other tested methods in the two generalization experiments &lt;strong&gt;E2&lt;/strong&gt; and &lt;strong&gt;E3&lt;/strong&gt;, the &lt;abbr title=&quot;Fourier neural operator&quot;&gt;FNO&lt;/abbr&gt;-PF is most accurate for &lt;strong&gt;E1&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;When solving a single equation, the &lt;abbr title=&quot;Fourier neural operator&quot;&gt;FNO&lt;/abbr&gt; likely performs better, though both &lt;abbr title=&quot;Fourier neural operator&quot;&gt;FNO&lt;/abbr&gt;-PF and MP-PDE methods outperform &lt;abbr title=&quot;Weighted Essentially Non-Oscillatory (5th order)&quot;&gt;WENO5&lt;/abbr&gt;.&lt;/p&gt; &lt;blockquote&gt; Neural solvers are resolution-invariant. &lt;/blockquote&gt; &lt;p&gt;As \(n_{x}\) is decreased, &lt;abbr title=&quot;Weighted Essentially Non-Oscillatory (5th order)&quot;&gt;WENO5&lt;/abbr&gt; performs increasingly worse whereas all the neural solvers remain relatively stable.&lt;/p&gt; &lt;blockquote&gt; Neural solver runtimes are constant to resolution. &lt;/blockquote&gt; &lt;p&gt;Additionally, the runtimes of &lt;abbr title=&quot;Weighted Essentially Non-Oscillatory (5th order)&quot;&gt;WENO5&lt;/abbr&gt; decrease (likely proportionally) since fewer steps require fewer calculations, but the MP-PDE runtimes again appear relatively stable.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;h4 id=&quot;future-directions&quot;&gt;Future Directions&lt;/h4&gt; &lt;p&gt;The authors conclude by discussing some future directions.&lt;/p&gt; &lt;p&gt;For example, the MP-PDE can be modified for &lt;span style=&quot;color:#9444e2;&quot;&gt;PDE &lt;em&gt;retrieval&lt;/em&gt; (which they call parameter optimization)&lt;/span&gt;. There is some precedence for this: Cranmer et al. develop a method which fits a symbolic regression model (eg.: PySR, eureqa) to the learned internal functions of a GNN &lt;d-cite key=&quot;cranmerDiscoveringSymbolicModels2020&quot;&gt;&lt;/d-cite&gt;. Alternatively, the MP-PDE’s capacity for generalization means that biasing the model with a prior to determine coefficients could be as simple as training on an example instance of the predicted equation, fitting this model on real world data (much like a finetuning process), and extracting the \(\theta_{PDE}\) parameters.&lt;/p&gt; &lt;p&gt;&lt;span style=&quot;color:#9444e2;&quot;&gt;Adaptive time stepping&lt;/span&gt; is another avenue which could make the model more efficient and accurate by taking large steps over stable/predictable solution regions and smaller steps over changing/unpredictable solution regions. The choice of a CNN for the decoder works well over regular inputs and outputs, but other options like attention-based architectures could potentially weigh the outputted node embeddings such that the model might learn different time steps. Some care would have to be taken with temporal bundling in this case, since the resulting vectors would be potentially irregular in time.&lt;/p&gt; &lt;p&gt;The one-step loss which is the basis of the &lt;span style=&quot;color:#9444e2;&quot;&gt;adversarial-style loss&lt;/span&gt; is also used in reinforcement learning, which frequently uses deep autoregressive models. Other formulations which borrow from reinforcement learning (where distribution shifts are quite common) and other fields could prove successful as well. Transformer-based natural language processing are now capable of capturing extremely long sequence dependencies and generating coherent long-form text. Since &lt;a href=&quot;https://graphdeeplearning.github.io/post/transformers-are-gnns/&quot;&gt;Transformers are GNNs&lt;/a&gt; which use attention to aggregate neighborhoods, this may be a viable avenue to explore.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;One more potential direction is inspired by the recent GRAND paper.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Brandstetter et al. emphasizes the value of relationships to classical solvers - in fact, this is one of the key benefits of hybrid autoregressive models. However, modeling continuous functions as in neural operator models typically outperforms their competitors. Even the MP-PDE is fully neural, making it less explanable than the hybrid autoregressive models introduced earlier.&lt;/p&gt; &lt;p&gt;The Graph Neural Diffusion (GRAND) model introduced by Chamberlain et al. demonstrates that &lt;span style=&quot;color:#9444e2;&quot;&gt;&lt;abbr title=&quot;graph neural network&quot;&gt;GNN&lt;/abbr&gt; can be crafted using differential equations&lt;/span&gt; (like diffusion processes) where, &lt;a href=&quot;#spatialderivative&quot;&gt;similarly to Brandstetter et al.&lt;/a&gt;, the spatial derivative is analogous to the difference between node features &lt;d-cite key=&quot;chamberlainGRANDGraphNeural2021a&quot;&gt;&lt;/d-cite&gt;. The layers are however analogous to the temporal change in a continuous-time differential equation, diverging from the MP-PDE intuition.&lt;/p&gt; &lt;p&gt;Rather than “representationally [containing] some classical methods” &lt;d-cite key=&quot;brandstetterMessagePassingNeural2022a&quot;&gt;&lt;/d-cite&gt;, GRAND provides a &lt;span style=&quot;color:#9444e2;&quot;&gt;mathematical framework&lt;/span&gt; which not only offers explanability, but also a method to design new architectures with theoretical guarantees like stability or convergence &lt;d-cite key=&quot;chamberlainGRANDGraphNeural2021a&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;For example, standard &lt;abbr title=&quot;message passing graph neural network&quot;&gt;MP-GNN&lt;/abbr&gt;s are shown to be equivalent to the explicit single-step Euler scheme; other classical solvers result in different flavours of message passing. Using GRAND to extend the MP-PDE would require rethinking the encoder and decoder, but the potential benefit could result in more reliability and therefore wider adoption of neural solvers for real world applications.&lt;/p&gt; &lt;h4 id=&quot;remarks&quot;&gt;Remarks&lt;/h4&gt; &lt;p&gt;In their paper “Message Passing Neural PDE Solver”, Brandstetter at al. present a well-motivated neural solver based on the principle of message passing. The key contributions are the end-to-end network capable of one-shot generalization, and the mitigation of error accumulation in autoregressive models via temporal bundling and the pushforward trick. Note that the latter are self-contained can be applied to other architectures (as in the FNO-PF), providing a valuable tool to improve autoregressive models.&lt;/p&gt; </content> </entry> <entry> <title>Decay No More</title> <link href="https://jocelynshen.com/blog/2022/adamw/"/> <updated>2022-12-01T00:00:00+00:00</updated> <id>https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/adamw</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Weight decay is a regularization technique in machine learning which scales down the weights in every step. It dates back at least to the 1990’s and the work of Krogh and Hertz &lt;d-cite key=&quot;Krogh1991&quot;&gt;&lt;/d-cite&gt; and Bos and Chug &lt;d-cite key=&quot;Bos1996&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pytorch&lt;/code&gt;, weight decay is one simple line which typically is found somewhere in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;step&lt;/code&gt;-method:&lt;/p&gt; &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt; &lt;p&gt;Subtracting a multiple of the weight can be seen as taking a step into the negative gradient direction of the squared norm of the weight. This relates weight decay to \(\ell_2\)-regularization (see also the &lt;a href=&quot;#appendix&quot;&gt;Appendix&lt;/a&gt; with an excerpt of the original work by Krogh and Hertz &lt;d-cite key=&quot;Krogh1991&quot;&gt;&lt;/d-cite&gt;).&lt;/p&gt; &lt;p&gt;The exact mechanism of weight decay is still puzzling the machine learning community:&lt;/p&gt; &lt;div class=&quot;jekyll-twitter-plugin&quot;&gt;&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;The story of weight decay in pictures:&lt;br /&gt;&lt;br /&gt;weight decay ...&lt;br /&gt;1) improves data efficiency by &amp;gt; 50%&lt;br /&gt;2) is frequently found in the best hyperparam configs&lt;br /&gt;3) is among the most important hparams to tune&lt;br /&gt;4) is also tricky to tune &lt;a href=&quot;https://t.co/PjWpk3pJxz&quot;&gt;pic.twitter.com/PjWpk3pJxz&lt;/a&gt;&lt;/p&gt;&amp;mdash; Sebastian Raschka (@rasbt) &lt;a href=&quot;https://twitter.com/rasbt/status/1614327550058328064?ref_src=twsrc%5Etfw&quot;&gt;January 14, 2023&lt;/a&gt;&lt;/blockquote&gt; &lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;/div&gt; &lt;div class=&quot;jekyll-twitter-plugin&quot;&gt;&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;There is a gaping hole in the literature regarding the purpose of weight decay in deep learning. Nobody knows what weight decay does! AFAIK, the last comprehensive look at weight decay was this 2019 paper &lt;a href=&quot;https://t.co/7WDBZojsm0&quot;&gt;https://t.co/7WDBZojsm0&lt;/a&gt;, which argued that weight decay &lt;a href=&quot;https://t.co/qUpCbfhFRf&quot;&gt;https://t.co/qUpCbfhFRf&lt;/a&gt;&lt;/p&gt;&amp;mdash; Jeremy Cohen (@deepcohen) &lt;a href=&quot;https://twitter.com/deepcohen/status/1617274166570528769?ref_src=twsrc%5Etfw&quot;&gt;January 22, 2023&lt;/a&gt;&lt;/blockquote&gt; &lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;/div&gt; &lt;p&gt;The paper by Zhang et al. &lt;d-cite key=&quot;Zhang2019&quot;&gt;&lt;/d-cite&gt; - which is the one mentioned in the second tweet - gives a comprehensive overview of weight decay and its effect on generalization, in particular in the interplay with Batch Normalization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(BN)&lt;/code&gt; &lt;d-cite key=&quot;Ioffe2015&quot;&gt;&lt;/d-cite&gt;. Batch Normalization describes a module of a network that normalizes the output of the previous layer to have zero mean and variance of one (or a variant of this with learnable mean and variance). We will not go into the details here but refer to &lt;a href=&quot;https://iclr-blog-track.github.io/2022/03/25/unnormalized-resnets/&quot;&gt;this blog post&lt;/a&gt; &lt;d-cite key=&quot;pieterjan2022normalizationisdead&quot;&gt;&lt;/d-cite&gt; for the interested reader.&lt;/p&gt; &lt;p&gt;We want to summarize two findings of &lt;d-cite key=&quot;Zhang2019&quot;&gt;&lt;/d-cite&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;On the one hand, weight decay has (in theory) no effect on layers with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(BN)&lt;/code&gt;. This is simply due to the fact that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(BN)&lt;/code&gt; makes the output invariant to a rescaling of the weights.&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; Weight decay is widely used in networks with Batch Normalization (Ioffe &amp;amp; Szegedy, 2015). In principle, weight decay regularization should have no effect in this case, since one can scale the weights by a small factor without changing the network’s predictions. Hence, it does not meaningfully constrain the network’s capacity. —Zhang et al., 2019 &lt;/blockquote&gt; &lt;ul&gt; &lt;li&gt;However, te experiments of the paper show that weight decay on layers with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(BN)&lt;/code&gt; can nevertheless improve accuracy. The authors argue that this is due to an effectively larger learning rate.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This blog post will summarize the development of weight decay specifically for &lt;span style=&quot;font-family:monospace&quot;&gt;Adam&lt;/span&gt;. We try to shed some light on the following questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;What is the difference between &lt;span style=&quot;font-family:monospace&quot;&gt;Adam&lt;/span&gt; and its weight decay version &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt;? Does the existing literature give a clear answer to the question when (and why) &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; performs better?&lt;/li&gt; &lt;li&gt;Is the weight decay mechanism of &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; just &lt;em&gt;one more trick&lt;/em&gt; or can we actually motivate it from an optimization perspective?&lt;/li&gt; &lt;li&gt;The last section is somewhat explorational: could we come up with different formulas for a weight decay version of &lt;span style=&quot;font-family:monospace&quot;&gt;Adam&lt;/span&gt;? By doing so, we will see that &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; already combines several advantages for practical use.&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;notation&quot;&gt;Notation&lt;/h3&gt; &lt;p&gt;We denote by \(\alpha &amp;gt; 0\) the initial learning rate. We use \(\eta_t &amp;gt; 0\) for a learning rate schedule multiplier. By this, the effective learning rate in iteration \(t\) is \(\alpha \eta_t\). We use \(\lambda &amp;gt; 0\) for the weight decay parameter.&lt;/p&gt; &lt;h2 id=&quot;adam&quot;&gt;Adam&lt;/h2&gt; &lt;p&gt;&lt;span style=&quot;font-family:monospace&quot;&gt;Adam&lt;/span&gt; uses an exponentially moving average (EMA) of stochastic gradients, typically denoted by \(m_t\), and of the elementwise squared gradients, denoted by \(v_t\).&lt;/p&gt; &lt;p&gt;We denote with \(\hat m_t\) and \(\hat v_t\) the EMA estimates with bias correction (see &lt;d-cite key=&quot;Kingma2015&quot;&gt;&lt;/d-cite&gt;), this means&lt;/p&gt; \[\hat m_t = \frac{m_t}{1-\beta_1^t}, \quad \hat v_t = \frac{v_t}{1-\beta_2^t}\] &lt;p&gt;where \(\beta_1, \beta_2 \in [0,1)\). The update formula of &lt;span style=&quot;font-family:monospace&quot;&gt;Adam&lt;/span&gt; is given by&lt;/p&gt; \[w_t = w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.\] &lt;p&gt;How would &lt;span style=&quot;font-family:monospace&quot;&gt;Adam&lt;/span&gt; handle regularization? The first approach to this was to simply add the regularization term \(\frac{\lambda}{2}\|w\|^2\) on top of the loss, do backpropagation and then compute the &lt;span style=&quot;font-family:monospace&quot;&gt;Adam&lt;/span&gt; step as outlined above. This is usually referred to as &lt;span style=&quot;font-family:monospace&quot;&gt;AdamL2&lt;/span&gt;. However, Loshchilov and Hutter &lt;d-cite key=&quot;Loshchilov2019&quot;&gt;&lt;/d-cite&gt; showed that this can be suboptimal and one major contribution to alleviate this was the development of &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt;.&lt;/p&gt; &lt;h2 id=&quot;adamw&quot;&gt;AdamW&lt;/h2&gt; &lt;p&gt;For training with \(\ell_2\)-regularization, Loshchilov and Hutter proposed &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; in 2019 &lt;d-cite key=&quot;Loshchilov2019&quot;&gt;&lt;/d-cite&gt; as an alternative to &lt;span style=&quot;font-family:monospace&quot;&gt;AdamL2&lt;/span&gt;. In the paper, the update formula is given as&lt;/p&gt; \[\tag{AdamW} w_t = (1-\eta_t \lambda)w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.\] &lt;p&gt;While for &lt;span style=&quot;font-family:monospace&quot;&gt;Adam&lt;/span&gt; several results for convex and nonconvex problems are established &lt;d-cite key=&quot;Defossez2022, Reddi2018&quot;&gt;&lt;/d-cite&gt;, theoretical guarantees for &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; have been explored - to the best of our knowledge - only very recently &lt;d-cite key=&quot;Anonymous2023&quot;&gt;&lt;/d-cite&gt;. Despite this, the method has enjoyed considerable practical success: for instance, &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; is implemented in the machine learning libraries Tensorflow &lt;d-cite key=&quot;Abadi2015&quot;&gt;&lt;/d-cite&gt; and Pytorch &lt;d-cite key=&quot;Paszke2019&quot;&gt;&lt;/d-cite&gt;. Another example is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fairseq&lt;/code&gt; library, developped by Facebook Research, which implements many SeqToSeq models. In their codebase, when &lt;span style=&quot;font-family:monospace&quot;&gt;Adam&lt;/span&gt; is specified with weight decay, &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; is used by default (see &lt;a href=&quot;https://github.com/facebookresearch/fairseq/blob/main/fairseq/optim/adam.py&quot;&gt;here&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;We summarize the empirical findings of &lt;d-cite key=&quot;Loshchilov2019&quot;&gt;&lt;/d-cite&gt; as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; improves generalization as compared to &lt;span style=&quot;font-family:monospace&quot;&gt;AdamL2&lt;/span&gt; for image classification tasks. In the paper, the authors use a ResNet model &lt;d-cite key=&quot;He2016&quot;&gt;&lt;/d-cite&gt; for the CIFAR10 and Imagenet32 dataset.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Another advantage of &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; is stated in the abstract of &lt;d-cite key=&quot;Loshchilov2019&quot;&gt;&lt;/d-cite&gt;:&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; We provide empirical evidence that our proposed modification decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam [...]. —Loshchilov and Hutter, 2019 &lt;/blockquote&gt; &lt;p&gt;What the authors mean by &lt;em&gt;decoupling&lt;/em&gt; is that if we plot the test accuracy as a heatmap of learning rate and weight decay, the areas with high accuracy are more rectangular; the best learing rate is not too sensitive to the choice of weight decay. We illustrate this conceptually in the plot below which is inspired by Figure 2 in &lt;d-cite key=&quot;Loshchilov2019&quot;&gt;&lt;/d-cite&gt;. The advantage of a decoupled method is that if one of the two hyperparameters is changed, the optimal value for the other one might still be identical and does not need to be retuned - this could reduce a 2D grid search to two 1D line searches.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/heatmap-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/heatmap-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/heatmap-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/heatmap.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 1: Heatmap of the test accuracy (bright = good accuracy) depending on learning rate and weight decay parameter choice. &lt;/div&gt; &lt;p&gt;When revisiting the literature on &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; we made an interesting practical observation: the &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html&quot;&gt;Pytorch implementation&lt;/a&gt; of &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; is actually slightly different to the algorithm proposed in the paper. In Pytorch, the following is implemented:&lt;/p&gt; \[w_t = (1-\eta_t \alpha \lambda)w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.\] &lt;p&gt;The difference is that the decay factor in the code is \(1-\eta_t \alpha \lambda\) instead of \(1-\eta_t \lambda\) in the paper. Clearly, this is equivalent as we can simply reparametrize the weight decay factor \(\lambda\) to make up for this. However, as the default learning rate \(\alpha=0.001\) is rather small, this means that practicioners might need to choose rather high values of \(\lambda\) in order to get sufficiently strong decay. Moreover, this leaves a certain ambiguity when tuned values for \(\lambda\) are reported in the literature.&lt;/p&gt; &lt;h2 id=&quot;follow-up-work&quot;&gt;Follow-up work&lt;/h2&gt; &lt;p&gt;In a recent article, Zhuang et al. revisit the &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; method and try to explain its practical success &lt;d-cite key=&quot;Zhuang2022&quot;&gt;&lt;/d-cite&gt;. One of their central arguments is that &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; is approximately equal to &lt;span style=&quot;font-family:monospace&quot;&gt;Adam&lt;/span&gt; with a proximal update for \(\ell_2\)-regularization.&lt;/p&gt; &lt;p&gt;Before explaining this in detail, we first want to summarize the empirical findings of &lt;d-cite key=&quot;Zhuang2022&quot;&gt;&lt;/d-cite&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(BN)&lt;/code&gt; is &lt;em&gt;deactivated&lt;/em&gt;, &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; achieves better generalization compared to &lt;span style=&quot;font-family:monospace&quot;&gt;AdamL2&lt;/span&gt; for image classification with a standard ResNet architecture &lt;d-cite key=&quot;He2016&quot;&gt;&lt;/d-cite&gt;.&lt;/li&gt; &lt;li&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(BN)&lt;/code&gt; is &lt;em&gt;activated&lt;/em&gt;, the test accuracy of &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; and &lt;span style=&quot;font-family:monospace&quot;&gt;AdamL2&lt;/span&gt; are on par. Moreover, the best accuracy is achieved for no weight decay, i.e. \(\lambda=0\).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The second result is somewhat stunning as it seems to contradict the results in &lt;d-cite key=&quot;Loshchilov2019&quot;&gt;&lt;/d-cite&gt;, which had shown that &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; generalizes better than &lt;span style=&quot;font-family:monospace&quot;&gt;AdamL2&lt;/span&gt;.&lt;d-footnote&gt;It seems like the AdamW-paper also used (BN) in their experiments, see https://github.com/loshchil/AdamW-and-SGDW.&lt;/d-footnote&gt;&lt;/p&gt; &lt;p&gt;Comparing the details of the experimental setups, we presume the following explanations for this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;The model that is trained in &lt;d-cite key=&quot;Loshchilov2019&quot;&gt;&lt;/d-cite&gt; is slightly different as it uses a Shake-Shake-Image ResNet &lt;d-cite key=&quot;He2016, Gastaldi2017&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;From Figure 4 in &lt;d-cite key=&quot;Loshchilov2019&quot;&gt;&lt;/d-cite&gt;, one can observe that the improvement in accuracy for the CIFAR-10 dataset becomes noticeable very late in the training (see also Section 4.3 in &lt;d-cite key=&quot;Loshchilov2019&quot;&gt;&lt;/d-cite&gt;). Thus, depending on the number of epochs after which training is stopped, one can reach different conclusions.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;proxadam&quot;&gt;ProxAdam&lt;/h2&gt; &lt;p&gt;The paper by Zhuang et al. &lt;d-cite key=&quot;Zhuang2022&quot;&gt;&lt;/d-cite&gt; does not only compare &lt;span style=&quot;font-family:monospace&quot;&gt;AdamL2&lt;/span&gt; to &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; experimentally, but it also provides a mathematical motivation for weight decay. In order to understand this, we first need to introduce the &lt;strong&gt;proximal operator&lt;/strong&gt;, a central concept of convex analysis.&lt;/p&gt; &lt;h3 id=&quot;a-short-introduction-to-proximal-operators&quot;&gt;A short introduction to proximal operators&lt;/h3&gt; &lt;p&gt;Proximal algorithms have been studied for decades in the context of (non-smooth) optimization, way before machine learning was a thing. The groundwork of this field has been laid by R. Tyrrell Rockafellar from the 1970’s onwards &lt;d-cite key=&quot;Rockafellar1976,Rockafellar1998&quot;&gt;&lt;/d-cite&gt;. If \(\varphi: \mathbb{R}^n \to \mathbb{R}\) is convex then the proximal operator is defined as&lt;/p&gt; \[\mathrm{prox}_\varphi(x) := \mathrm{argmin}_{z \in \mathbb{R}^n} \varphi(z) + \frac12 \|z-x\|^2.\] &lt;p&gt;For many classical regularization functions (e.g. the \(\ell_1\)-norm), the proximal operator can be computed in closed form. This makes it a key ingredient of optimization algorithms for regularized problems. Assume that we want to minimize the sum of a differentiable loss \(f\) and a convex regularizer \(\varphi\), i.e.&lt;/p&gt; \[\min_{w \in \mathbb{R}^n} f(w) + \varphi(w).\] &lt;p&gt;The proximal gradient method in this setting has the update formula&lt;/p&gt; \[w_{t} = \mathrm{prox}_{\alpha \varphi}\big(w_{t-1}- \alpha \nabla f(w_{t-1})\big),\] &lt;p&gt;where \(\alpha&amp;gt;0\) is a step size (&lt;em&gt;aka&lt;/em&gt; learning rate). An equivalent way of writing this (which will become useful later on) is&lt;d-footnote&gt;This can be proven using the definition of the proximal operator and completing the square.&lt;/d-footnote&gt;&lt;/p&gt; \[\tag{1} w_{t} = \mathrm{argmin}_y \langle y-w_{t-1}, \nabla f(w_{t-1})\rangle + \varphi(y) + \frac{1}{2\alpha}\|y-w_{t-1}\|^2.\] &lt;h3 id=&quot;weight-decay-as-a-proximal-operator&quot;&gt;Weight decay as a proximal operator&lt;/h3&gt; &lt;p&gt;For \(\ell_2\)-regularization \(\varphi(w) = \frac{\lambda}{2}\|w\|^2\), the proximal operator at \(w\) is given by \(\frac{1}{1+\lambda}w = (1-\frac{\lambda}{1+\lambda})w\). Based on this, the authors of &lt;d-cite key=&quot;Zhuang2022&quot;&gt;&lt;/d-cite&gt; propose a proximal version of &lt;span style=&quot;font-family:monospace&quot;&gt;Adam&lt;/span&gt; called &lt;span style=&quot;font-family:monospace&quot;&gt;ProxAdam&lt;/span&gt;. It is given by&lt;/p&gt; \[\tag{ProxAdam} w_t = \big(1- \frac{\lambda\eta_t}{1+\lambda\eta_t} \big)w_{t-1} - \frac{\eta_t \alpha}{1+\lambda\eta_t} \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.\] &lt;p&gt;Knowing this, we can now understand why &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; is approximately a proximal version of &lt;span style=&quot;font-family:monospace&quot;&gt;Adam&lt;/span&gt;. Using the first-order Taylor-approximation \(\frac{ax}{1+bx}\approx ax\) for small \(x\), applied to the coefficients in front of \(w_{t-1}\) and \(\frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}\) gives the formula&lt;/p&gt; \[w_t = (1-\eta_t \lambda)w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}\] &lt;p&gt;which is equal to &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt;. The argument we just presented is exactly how &lt;d-cite key=&quot;Zhuang2022&quot;&gt;&lt;/d-cite&gt; concludes that &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; \(\approx\) &lt;span style=&quot;font-family:monospace&quot;&gt;ProxAdam&lt;/span&gt;.&lt;/p&gt; &lt;h3 id=&quot;changing-the-norm&quot;&gt;Changing the norm&lt;/h3&gt; &lt;p&gt;There is one more way of interpreting proximal methods. Let us begin with a simple example: Define the diagonal matrix \(D_t := \mathrm{Diag}(\epsilon + \sqrt{\hat v_t})\). Then, the &lt;span style=&quot;font-family:monospace&quot;&gt;Adam&lt;/span&gt; update can be equivalently written&lt;d-footnote&gt;This can be proven by first-order optimality and solving for $w_t$. We will do a similar calculation further below.&lt;/d-footnote&gt; as&lt;/p&gt; \[w_t = \mathrm{argmin}_y \langle y-w_{t-1}, \hat m_t \rangle + \frac{1}{2\eta_t\alpha}\|y-w_{t-1}\|_{D_t}^2.\] &lt;p&gt;In other words, &lt;span style=&quot;font-family:monospace&quot;&gt;Adam&lt;/span&gt; takes a proximal step of a linear function, but with the adaptive norm \(D_t\). This change in norm is what makes &lt;span style=&quot;font-family:monospace&quot;&gt;Adam&lt;/span&gt; different from &lt;span style=&quot;font-family:monospace&quot;&gt;SGD&lt;/span&gt; with (heavy-ball) momentum.&lt;/p&gt; &lt;p&gt;The update formula of &lt;span style=&quot;font-family:monospace&quot;&gt;ProxAdam&lt;/span&gt; can also be written as a proximal method:&lt;/p&gt; \[\tag{P1} w_t = \mathrm{argmin}_y \langle y-w_{t-1}, \hat m_t \rangle + \frac{\lambda}{2\alpha}\|y\|_{D_t}^2 + \frac{1}{2 \eta_t \alpha}\|y-w_{t-1}\|_{D_t}^2.\] &lt;p&gt;In fact, the first-order optimality conditions of (P1) are&lt;/p&gt; \[0 = \hat m_t + \frac{\lambda}{\alpha} D_t w_t + \frac{1}{\eta_t \alpha}D_t (w_t-w_{t-1}).\] &lt;p&gt;Solving for \(w_t\) (and doing simple algebra) gives&lt;/p&gt; \[\tag{2} w_t = (1+\lambda \eta_t)^{-1}\big[w_{t-1} - \eta_t \alpha D_t^{-1} \hat m_t\big]\] &lt;p&gt;which is equal to &lt;span style=&quot;font-family:monospace&quot;&gt;ProxAdam&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;What is slightly surprising here is the term \(\alpha^{-1}\|y\|_{D_t}^2\) in (P1) - we might have expected the regularization term to be used with the standard \(\ell_2\)-norm. This leads us to our final section.&lt;/p&gt; &lt;h2 id=&quot;adamw-is-scale-free&quot;&gt;&lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; is scale-free&lt;/h2&gt; &lt;p&gt;As an alternative to (P1), we could replace \(\alpha^{-1}\|y\|_{D_t}^2\) by \(\|y\|^2\) and update&lt;/p&gt; \[w_t = \mathrm{argmin}_y \langle y-w_{t-1}, \hat m_t \rangle + \frac{\lambda}{2}\|y\|^2 + \frac{1}{2\eta_t\alpha}\|y-w_{t-1}\|_{D_t}^2.\] &lt;p&gt;Again, setting the gradient of the objective to zero and solving for \(w_t\) we get&lt;/p&gt; \[w_t = \big(\mathrm{Id} + \eta_t \lambda \alpha D_t^{-1}\big)^{-1} \big[w_{t-1} - \eta_t\alpha D_t^{-1} \hat m_t \big].\] &lt;p&gt;Comparing this to (2) we see that the second factor is the same, but the decay factor now also depends on \(D_t\) and \(\alpha\). Let us call this method &lt;span style=&quot;font-family:monospace&quot;&gt;AdamP&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;Now the natural question is whether &lt;span style=&quot;font-family:monospace&quot;&gt;AdamP&lt;/span&gt; or &lt;span style=&quot;font-family:monospace&quot;&gt;ProxAdam&lt;/span&gt; (or &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; as its approximation) would be superior. One answer to this is that we would prefer a &lt;em&gt;scale-free&lt;/em&gt; algorithm: with this we mean that if the loss function would be multiplied by a positive constant, we could still run the method with exactly the same parameters and obtain the same result. &lt;span style=&quot;font-family:monospace&quot;&gt;Adam&lt;/span&gt; for example is scale-free and in &lt;d-cite key=&quot;Zhuang2022&quot;&gt;&lt;/d-cite&gt; it is explained that &lt;span style=&quot;font-family:monospace&quot;&gt;ProxAdam&lt;/span&gt;/&lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; are, too. The reason for this is the following: looking at (P1) we see that if the loss is scaled by \(c&amp;gt;0\), then \(\hat m_t\) and \(D_t\) are scaled by \(c\) (if we neglect the \(\epsilon\) in \(D_t\)). Hence, the objective in (P1) is multiplied by \(c\) which implies that &lt;span style=&quot;font-family:monospace&quot;&gt;ProxAdam&lt;/span&gt; for \(\epsilon=0\) is invariant to scaling for the same values of \(\lambda,\alpha,\eta_t\). Now, for (P2) the story is different, as here the second term \(\frac{\lambda}{2}\|y\|^2\) is not scaled by \(c\), but the other terms are. We would need to rescale \(\lambda\) by \(c\) to obtain the identical update. As a consequence, &lt;span style=&quot;font-family:monospace&quot;&gt;AdamP&lt;/span&gt; would &lt;strong&gt;not be scale-free&lt;/strong&gt; and this makes it less attractive as a method. We should point out that scale-freeness is rather a practical advantage that requires less tuning when changing the model or dataset - it does not imply that the test accuracy would be different when both methods are tuned.&lt;/p&gt; &lt;p&gt;To verify this, we ran a simple experiment on a ResNet20 for CIFAR10 with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(BN)&lt;/code&gt; deactivated. For &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; (the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pytorch&lt;/code&gt; version) and &lt;span style=&quot;font-family:monospace&quot;&gt;AdamP&lt;/span&gt; we tested the learning rates &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[1e-3,1e-2,1e-1]&lt;/code&gt; and weight decay &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[1e-5,1e-4,1e-3,1e-2]&lt;/code&gt;. From the plots below, we can see that both methods approximately achieve the same accuracy for the best configurations&lt;d-footnote&gt;The best configurations all have learning rate 1e-3.&lt;/d-footnote&gt;. The only difference - in this very simple example - is that &lt;span style=&quot;font-family:monospace&quot;&gt;AdamP&lt;/span&gt; seems to arrive at a model with smaller norm for the configurations with high accuracy (see right plot). Hence, its regularization seems to be stronger.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/resnet20val_score-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/resnet20val_score-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/resnet20val_score-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/resnet20val_score.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/resnet20model_norm-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/resnet20model_norm-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/resnet20model_norm-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/resnet20model_norm.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;For the sake of completeness, we also add a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pytorch&lt;/code&gt; implementation of &lt;span style=&quot;font-family:monospace&quot;&gt;AdamP&lt;/span&gt; in the &lt;a href=&quot;#appendix&quot;&gt;Appendix&lt;/a&gt;.&lt;/p&gt; &lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;Weight decay can be seen as a proximal way of handling \(\ell_2\)-regularization. Therefore, it is not a different &lt;em&gt;type&lt;/em&gt; of regularization itself but rather a different &lt;em&gt;treatment&lt;/em&gt; of regularization in the optimization method. As a consequence, &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; is an (almost) proximal version of &lt;span style=&quot;font-family:monospace&quot;&gt;Adam&lt;/span&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Whether or not weight decay brings advantages when used &lt;em&gt;together with&lt;/em&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(BN)&lt;/code&gt; seems to depend on several factors of the model and experimental design. However, in all experiments we discussed here &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; performed better or at least on par to &lt;span style=&quot;font-family:monospace&quot;&gt;AdamL2&lt;/span&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The second conclusion suggests that proximal algorithms such as &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; seem to be favourable. Together with the scale-free property that we described in the final section, this makes &lt;span style=&quot;font-family:monospace&quot;&gt;AdamW&lt;/span&gt; a robust method and explains its practical success.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a name=&quot;appendix&quot;&gt;&lt;/a&gt;&lt;/p&gt; &lt;h2 id=&quot;appendix&quot;&gt;Appendix&lt;/h2&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/krogh_snippet-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/krogh_snippet-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/krogh_snippet-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/deep-learning-blog.github.io/assets/img/2022-12-01-adamw/krogh_snippet.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 2: Excerpt of the introduction in &lt;d-cite key=&quot;Krogh1991&quot;&gt;&lt;/d-cite&gt;. &lt;/div&gt; &lt;p&gt;Below you find a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pytorch&lt;/code&gt; implementation of &lt;span style=&quot;font-family:monospace&quot;&gt;AdamP&lt;/span&gt;:&lt;/p&gt; &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch.optim&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Optimizer&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;AdamP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;sa&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; Arguments: params (iterable): iterable of parameters to optimize or dicts defining parameter groups lr (float, optional): learning rate (default: 1e-3) betas (Tuple[float, float], optional): coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)) eps (float, optional): term added to the denominator to improve numerical stability (default: 1e-8) weight_decay (float, optional): weight decay (L2 penalty) (default: 0) &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;betas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.999&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight_decay&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ValueError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Invalid learning rate: {}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ValueError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Invalid epsilon value: {}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;betas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ValueError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Invalid beta parameter at index 0: {}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;betas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;betas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ValueError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Invalid beta parameter at index 1: {}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;betas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight_decay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ValueError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Invalid weight_decay value: {}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight_decay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;defaults&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;betas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;betas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight_decay&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight_decay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_init_lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;defaults&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;closure&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Performs a single optimization step. Arguments: closure (callable, optional): A closure that reevaluates the model and returns the loss. &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&quot;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;closure&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;enable_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;closure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;param_groups&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# State initialization &lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Exponential moving average of gradient values &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;exp_avg&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Exponential moving average of squared gradient values &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;exp_avg_sq&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exp_avg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exp_avg_sq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;exp_avg&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;exp_avg_sq&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;betas&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias_correction1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias_correction2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Decay the first and second moment running average coefficient &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exp_avg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mul_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exp_avg_sq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mul_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;addcmul_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp_avg_sq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;div&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_correction2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;eps&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lmbda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;weight_decay&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;addcdiv_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp_avg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_correction1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lmbda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;div_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lmbda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# adaptive weight decay &lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt; </content> </entry> </feed>