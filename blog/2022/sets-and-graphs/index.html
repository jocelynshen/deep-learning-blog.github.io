<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Universality of Neural Networks on Sets vs. Graphs | ICLR Blogposts 2023 (staging)</title> <meta name="author" content="abc b c"/> <meta name="description" content="Universal function approximation is one of the central tenets in theoretical deep learning research. It is the question whether a specific neural network architecture is, in theory, able to approximate any function of interest. The ICLR paper “How Powerful are Graph Neural Networks?” shows that mathematically analysing the constraints of an architecture as a universal function approximator and alleviating these constraints can lead to more principled architecture choices, performance improvements, and long term impact on the field. Specifically in the fields of learning on sets and learning on graphs, universal function approximation is a well-studied property. The two fields are closely linked, because the need for permutation invariance in both cases lead to similar building blocks. However, these two fields have evolved in parallel, often lacking awareness of developments in the respective other field. This post aims at bringing these two fields closer together, particularly from the perspective of universal function approximation."/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/deep-learning-blog.github.io/assets/img/iclr_favicon.ico"/> <link rel="stylesheet" href="/deep-learning-blog.github.io/assets/css/main.css"> <link rel="canonical" href="https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/sets-and-graphs/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/deep-learning-blog.github.io/assets/js/theme.js"></script> <script src="/deep-learning-blog.github.io/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/deep-learning-blog.github.io/assets/js/distillpub/template.v2.js"></script> <script src="/deep-learning-blog.github.io/assets/js/distillpub/transforms.v2.js"></script> <script src="/deep-learning-blog.github.io/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "Universality of Neural Networks on Sets vs. Graphs",
      "description": "Universal function approximation is one of the central tenets in theoretical deep learning research. It is the question whether a specific neural network architecture is, in theory, able to approximate any function of interest. The ICLR paper “How Powerful are Graph Neural Networks?” shows that mathematically analysing the constraints of an architecture as a universal function approximator and alleviating these constraints can lead to more principled architecture choices, performance improvements, and long term impact on the field. Specifically in the fields of learning on sets and learning on graphs, universal function approximation is a well-studied property. The two fields are closely linked, because the need for permutation invariance in both cases lead to similar building blocks. However, these two fields have evolved in parallel, often lacking awareness of developments in the respective other field. This post aims at bringing these two fields closer together, particularly from the perspective of universal function approximation.",
      "published": "December 1, 2022",
      "authors": [
        {
          "author": "Anonymous",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/deep-learning-blog.github.io/">ICLR Blogposts 2023 (staging)</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/deep-learning-blog.github.io/about">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/deep-learning-blog.github.io/call">call for blogposts</a> </li> <li class="nav-item "> <a class="nav-link" href="/deep-learning-blog.github.io/submitting">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/deep-learning-blog.github.io/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" target="_blank" rel="noopener noreferrer">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Universality of Neural Networks on Sets vs. Graphs</h1> <p>Universal function approximation is one of the central tenets in theoretical deep learning research. It is the question whether a specific neural network architecture is, in theory, able to approximate any function of interest. The ICLR paper “How Powerful are Graph Neural Networks?” shows that mathematically analysing the constraints of an architecture as a universal function approximator and alleviating these constraints can lead to more principled architecture choices, performance improvements, and long term impact on the field. Specifically in the fields of learning on sets and learning on graphs, universal function approximation is a well-studied property. The two fields are closely linked, because the need for permutation invariance in both cases lead to similar building blocks. However, these two fields have evolved in parallel, often lacking awareness of developments in the respective other field. This post aims at bringing these two fields closer together, particularly from the perspective of universal function approximation.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#sets-and-graphs">Sets and Graphs</a></div> <div><a href="#why-do-we-care-about-universal-function-approximation">Why do we care about universal function approximation?</a></div> <div><a href="#learning-on-sets-universality">Learning on Sets &amp; Universality</a></div> <div><a href="#what-about-graph-representation-learning">What about _graph_ representation learning?</a></div> <div><a href="#learning-on-graphs-universality">Learning on Graphs &amp; Universality</a></div> <div><a href="#the-weisfeiler-lehman-test">The Weisfeiler-Lehman Test</a></div> </nav> </d-contents> <h2 id="sets-and-graphs">Sets and Graphs</h2> <p>Before we dive into<d-footnote>It is important to briefly focus on declaring the *conflict of interest* we had while writing this blog. We are actively working on set and graph representation learning. Accordingly, several paragraphs of this writeup focus on papers that we have co-written. That being said, and in the context of ICLR, we declare that the majority of the ICLR papers referenced in this blog post do _not_ present a conflict of interest for us. Hence, we believe we have, to the best of our efforts, provided an objective and impartial view of learning universal representations over graphs and sets.</d-footnote> universal function approximation, let’s start with the basics. What do we mean by learning on set- graphs-based data? In both cases, we assume no ordering, i.e. the task is permutation invariant (or equivariant). A graph is typically thought of as a set of nodes with edges between the nodes. A set doesn’t have edges, it just has the nodes, although we often don’t call them nodes, rather set elements. Both the nodes and the edges can have feature vectors attached to them. The figure below (originally from Wagstaff et al. 2021<d-cite key="wagstaff21"></d-cite>) visualises this relationship:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_graphsandsets-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_graphsandsets-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_graphsandsets-1400.webp"></source> <img src="/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_graphsandsets.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Examples for machine learning tasks on this type of data include 3D point cloud classification (a function mapping a set of coordinates to an object class) and molecular property prediction (a function mapping a molecular graph to, e.g., a free energy value).</p> <h2 id="why-do-we-care-about-universal-function-approximation">Why do we care about universal function approximation?</h2> <p>First of all, why do we need to be able to approximate all functions? After all, having <em>one</em> function that performs well on the train set and generalises to the test set is all we need in most cases. Well, the issue is that we have no idea what such a function looks like, otherwise we would implement it directly and wouldn’t need to train a neural network. Hence, the network not being a universal function approximator <em>may</em> hurt its performance. </p> <p>Graph Isomorphism Networks (GINs) by Xu et al.<d-cite key="GIN"></d-cite>) provide the quintessential example for the merit of universality research: the authors analysed Graph Convolutional Networks (a very popular class of graph neural networks by Kipf et al. 2016<d-cite key="GCN"></d-cite>), pointed out that GCNs are not universal, created a varation of the algorithm that <em>is</em> universal (or at least closer to), and achieved better results. </p> <p>However, this is not always the case. Sometimes, architecture changes motivated by universal function approximation arguments lead to <em>worse</em> results. Even in such unfortunate cases, however, we argue that thinking about universality is no waste of time. Firstly, it brings structure into the literature and into the wide range of models available. We need to group approaches together to see the similarities and differences. Universality research can and has served as a helpful tool for that.</p> <p>Moreover, proving that a certain architecture is or is not universal is an inherently interesting task and teaches us mathematical thinking and argumentation. In a deep learning world, where there is a general sense of randomness and magic in building high-performing neural networks and where it’s hard to interpret what’s going on, one might argue that an additional mathematical analysis is probably good for the balance, even if it turns out to not always directly result in better performance.</p> <h2 id="learning-on-sets--universality">Learning on Sets &amp; Universality</h2> <p>To prove universal function approximation<d-footnote>*Approximation* is actually not precisely what we will discuss in the rest of this text. Rather, we will consider universal function *representation*. That's also what we mean by phrases like *is able to learn* (even though that would also depend on the optimiser you are using and the amount of data you have and where you put the layernorms and so on...). The difference between approximation and representation is discussed in detail in Wagstaff et al. 2021 <d-cite key="wagstaff21"></d-cite>. Interestingly, the findings for universal function representation largely also hold for universal function approximation.</d-footnote>, we typically make two assumptions: 1) the MLP components of the neural networks are infinitely large. 2) the functions that we want to be able to learn are continuous on $\mathbb{R}$.</p> <p>The first part says: any concrete implementation of a ‘universal’ network architecture might not be able to learn the function of interest, but, if you make it <a href="https://i.redd.it/n9fgba8b0qr01.png" target="_blank" rel="noopener noreferrer">bigger</a>, eventually it will—and that is <em>guaranteed</em><d-footnote>Conversely, if the network is provably non-universal (like Graph Convolutional Networks), then there are functions it can *never* learn, no matter how many layers you stack.</d-footnote>. The second part is a non-intuitive mathematical technicality we will leave uncommented for now and get back to later (because it’s actually a really interesting and important techinicality).</p> <p>One of the seminal papers discussing both permutation invariant neural networks and universal function approximation was DeepSets by Zaheer et al. in 2017<d-cite key="Zaheer2017"></d-cite>. The idea is simple: apply the same neural network $\phi$ to several inputs, sum up their results, and apply a final neural network $\rho$.<d-footnote>Figure from Wagstaff et al. 2021.</d-footnote></p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_deepsets-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_deepsets-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_deepsets-1400.webp"></source> <img src="/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_deepsets.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Because the sum operation is permutation invariant, the final output is invariant with respect to the ordering of the inptus. In other words, the sum quite obviously restricts the space of learnable functions to permutation invariant ones. The question is, can a neural network with this architecture, in principle, learn <em>all</em> (continuous) permutation invariant functions. Perhaps surprisingly, the authors show that all functions can indeed be represented with this architecture. The idea is a form of binary bit-encoding in the latent space. Concretely, they argue that there is a bijective mapping from rational to natural numbers. Assuming that each input is a rational number, they first map each rational number $x$ to a natural number $c(x)$, and then each natural number to $\phi(x) = 4^{-c(x)}$. It is now easy to see that $\sum_i \phi(x_i) \neq \sum_i \phi(y_i)$ unless the finite sets $ \{ x_0, x_1, … \} $ and $\{y_0, y_1, …\}$ are the same. Now that we uniquely encoded each input, a universal decoder can map this to any output we want. This concludes the proof that the DeepSets architecture is, in theory, a universal function approximator, despite its simplicity.</p> <p>However, there is an issue with this proof: it builds on the assumption that the MLP components themselves are universal function approximators, in the limit of infinite width. However, the universal function approximation theorem says that this is the case only for continuous functions, where continuity is defined on the real numbers. That conitnuity is important is sort of intuitive: continuity means that a small change in the input implies a small change in the output. And because the building blocks of neural networks (specifically linear combinations and non-linearities) are continuous, it makes sense that the overall function we want the network to learn should be continuous.</p> <p>But why continuity on the real numbers? Because continuity on the rational numbers is not a very useful property as shown in Wagstaff et al. 2019<d-cite key="wagstaff19"></d-cite>. The mapping we described above is clearly highly discontinuous, and anyone could attest that it is completely unrealistic to assume that a neural network could learn such a metric. That doesn’t mean all is lost. Wagstaff et al. show that the DeepSets architecture is still a universal function approximator when requiring continuity, but only if the latent space (the range of $\phi$) has a dimensionality at least as large as the number of inputs, which is an important restriction.</p> <p>What about more complicated architectures? Murphy et al.<d-cite key="Janossy"></d-cite> generalise the idea of Deep Sets to applying networks to all possible $k$-tuples of inputs, where $k=1$ recovers the Deep Sets case. This can be seen as unifying other architecture classes such as self-attention. However, this is not known to alleviate the constraint on the latent space mentioned above, as explained in Wagstaff et al. 2021<d-cite key="wagstaff21"></d-cite>.</p> <h2 id="what-about-graph-representation-learning">What about <em>graph</em> representation learning?</h2> <p>So, this was universality in the context of machine learning on sets, but what about graphs? Interestingly, the graph representation learning community experienced a near-identical journey, evolving entirely in parallel! Perhaps this observation comes as little surprise: to meaningfully propagate information in a graph neural network (GNN), a local, permutation invariant operation is commonplace.</p> <p>Specifically, a GNN typically operates by computing representations (<em>“messages”</em>) sent from each node to its neighbours, followed by an <em>aggregation function</em> which, for every node, combines all of its incoming messages in a way that is <em>invariant to permutations</em>. Opinions are still divided on whether <em>every</em> permutation equivariant GNN can be expressed with such pairwise messaging, with a recent position paper by Velickovic<d-cite key="Velickovic22"></d-cite> claiming they <strong>can</strong>. Regardless of which way the debate goes in the future, aggregating messages over 1-hop neighbours gives rise to a highly elegant implementation of GNNs which is likely here to stay. This comes with very solid community backing, with <a href="https://www.pyg.org/" target="_blank" rel="noopener noreferrer">PyG</a>—one of the most popular GNN frameworks—<a href="https://github.com/pyg-team/pytorch_geometric/releases/tag/2.1.0" target="_blank" rel="noopener noreferrer">recently making aggregators a “first-class citizen”</a> in their GNN pipelining.</p> <p>Therefore, to build a GNN, it suffices to build a <em>permutation-invariant, local</em> layer which combines data coming from each node’s neighbours. This feels nearly identical to our previous discussion; what’s changed, really? Well, we need to take care of one seemingly minor detail: it is possible for <strong>two or more neighbours to send <em>exactly the same message</em></strong>. The theoretical framework of Deep Sets and/or Wagstaff et al. wouldn’t entirely suffice in this case, as they assumed a <em>set</em> input, whereas now we have a <em>multiset</em>.</p> <h2 id="learning-on-graphs--universality">Learning on Graphs &amp; Universality</h2> <p>Several influential GNN papers were able to overcome this limitation. The first key development came from the <em>graph isomorphism network</em> (<strong>GIN</strong>)<d-cite key="GIN"></d-cite>. GIN is an elegant example of how, over countable features, the maximally-powerful GNN can be built up using similar ideas as in Deep Sets; so long as the local layer we use is <em>injective</em> over multisets. Similarly to before, we must choose our encoder $\phi$ and aggregator $\bigoplus$, such that $\bigoplus\limits_i \phi(x_i) \neq \bigoplus\limits_i \phi(y_i)$ unless the finite <em>multisets</em> $\{ \mkern-4mu \{x_0, x_1, …\} \mkern-4mu \}$ and $\{\mkern-4mu\{y_0, y_1, …\} \mkern-4mu \}$ are the same ($x_i, y_i\in\mathbb{Q}$).</p> <p>In the multiset case, the framework from Deep Sets induces an additional constraint over $\bigoplus$—it needs to preserve the <em>cardinality</em> information about the repeated elements in a multiset. This immediately implies that some choices of $\bigoplus$, such as $\max$ or averaging, will not yield maximally powerful GNNs.</p> <p>For example, consider the multisets $\{\mkern-4mu\{1, 1, 2, 2\} \mkern-4mu \}$ and $\{\mkern-4mu\{1, 2\}\mkern-4mu\}$. As we assume the features to be countable, we specify the numbers as <em>one-hot</em> integers; that is, $1 = [1\ \ 0]$ and $2=[0\ \ 1]$. The maximum of these features, taken over the multiset, is $[1\ \ 1]$, and their average is $\left[\frac{1}{2}\ \ \frac{1}{2}\right]$. This is the case for both of these multisets, meaning that both maximising and averaging are <em>incapable</em> of telling them apart.</p> <p>Summations $\left(\bigoplus=\sum\right)$, however, are an example of a suitable injective operator.</p> <p>Very similarly to the analysis from Wagstaff et al. in the domain of sets, a similar extension in the domain of graphs came through the work on <a href="**PNA**"><em>principal neighbourhood aggregation</em></a> by Corso, Cavalleri et al.<d-cite key="Corso"></d-cite>. We already discussed why it is a good idea to focus on features coming from $\mathbb{R}$ rather than $\mathbb{Q}$—the universal approximation theorem is only supported over continuous functions. However, it turns out that, when we let $x_i, y_i\in\mathbb{R}$, it is easily possible to construct neighbourhood multisets for which setting $\bigoplus=\sum$ would <strong>not</strong> preserve injectivity:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_examples-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_examples-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_examples-1400.webp"></source> <img src="/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_examples.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>In fact, PNA itself is based on a proof that it is <em>impossible</em> to build an injective function over multisets with real-valued features using <em>any</em> <strong>single</strong> aggregator. In general, for an injective functon over $n$ neighbours, we need <em>at least</em> $n$ aggregation functions (applied in parallel). PNA then builds an empirically powerful aggregator combination, leveraging this insight while trying to preserve numerical stability.</p> <p>Note that there is an apparent <strong>similarity</strong> between the results of Wagstaff et al. and the analysis of PNA. Wagstaff et al. shows that, over real-valued sets of $n$ elements, it is necessary to have an encoder representation <em>width</em> at least $n$. Corso, Cavalleri et al. showed that, over real-valued multisets of $n$ elements, it is necessary to aggregate them with at least $n$ aggregators. It appears that potent processing of real-valued collections <em>necessitates</em> representational capacity proportional to the collection’s size, in order to guarantee injectivity. Discovering this correspondence is what brought the two of us together to publish this blog post in the first place, but we do not offer any in-depth analysis of this correspondence here. We do hope it inspires future connections between these two fields, however!</p> <p>We have established what is necessary to create a maximally-powerful GNN over both <em>countable</em> and <em>uncountable</em> input features. So, <em>how powerful are they</em>, exactly?</p> <h2 id="the-weisfeiler-lehman-test">The Weisfeiler-Lehman Test</h2> <p>While GNNs are often a powerful tool for processing graph data in the real world, they also won’t solve <em>all</em> tasks specified on a graph accurately! As a simple counterexample, consider any NP-hard problem, such as the Travelling Salesperson Problem. If we had a fixed-depth GNN that perfectly solves such a problem, we would have shown P=NP! Expectedly, not all GNNs will be equally good at solving various problems, and we may be highly interested in characterising their <em>expressive power</em>.</p> <p>The canonical example for characterising expressive power is <em>deciding graph isomorphism</em>; that is, can our GNN distinguish two non-isomorphic graphs? Specifically, if our GNN is capable of computing graph-level representations \(\mathbf{h}_{\mathcal{G}}\), we are interested whether \(\mathbf{h}_{\mathcal{G_{1}}} \neq\mathbf{h}_{\mathcal{G_{2}}}\) for non-isomorphic graphs \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\). If we cannot attach different representations to these two graphs, any kind of task requiring us to classify them differently is <em>hopeless</em>! This motivates assessing the power of GNNs by which graphs they are able to <em>distinguish</em>.</p> <p>A typical way in which this is formalised is by using the <em>Weisfeiler-Lehman</em> (<strong>WL</strong>) graph isomorphism test. To formalise this, we will study a popular algorithm for approximately deciding graph isomorphism.</p> <p>The WL algorithm featurises a graph $\mathcal{G}=(\mathcal{V},\mathcal{E})$ as follows. First, we set the representation of each node $i\in\mathcal{V}$ as $x_i^{(0)} = 1$. Then, it proceeds as follows:</p> <ol> <li>Let $\mathcal{X}_i^{(t+1)} = \{\mkern-4mu\{x_j^{(t)} :(i,j)\in\mathcal{E}\}\mkern-4mu\}$ be the multiset of features of all neighbours of $i$.</li> <li>Then, let \(x_i^{(t+1)}=\sum\limits_{y_j\in\mathcal{X}_i^{(t+1)}}\phi(y_j)\), where \(\phi : \mathbb{Q}\rightarrow\mathbb{Q}\) is an <em>injective</em> hash function.</li> </ol> <p>This process continues as long as the <em>histogram</em> of $x_i^{(t)}$ changes—initially, all nodes have the same representation. As steps 1–2 are iterated, certain $x_i^{(t)}$ values may become different. Finally, the WL test checks whether two graphs are (possibly) isomorphic by checking whether their histograms have the same (sorted) shape upon convergence.</p> <p>While remarkably simple, the WL test can accurately distinguish most graphs of real-world interest. It does have some rather painful failure modes, though; for example, it cannot distinguish a 6-cycle from two triangles!</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_wlfail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_wlfail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_wlfail-1400.webp"></source> <img src="/deep-learning-blog.github.io/assets/img/2022-12-01-sets-and-graphs/graphsuniv_wlfail.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>This is because, locally, <em>all nodes look the same</em> in these two graphs, and the histogram never changes.</p> <p>The key behind the power of the WL test is the <em>injectivity</em> of the hash function $\phi$—it may be interpreted as assigning each node a different <em>colour</em> if it has a different <em>local context</em>. Similarly, we saw that GNNs are maximally powerful when their propagation models are <em>injective</em>. It should come as little surprise then that, in terms of distinguishing graph structures over <em>countable</em> input features, GNNs can <strong>never be more powerful than the WL test</strong>! And, in fact, this level of power is achieved <em>exactly</em> when the aggregator is injective. This fact was first discovered by Morris et al.<d-cite key="Morris"></d-cite>, and reinterpreted from the perspective of multiset aggregation by the GIN paper.</p> <p>While the WL connection has certainly spurred a vast amount of works on improving GNN expressivity, it is also worth recalling the initial assumption: $x_i^{(0)} = 1$. That is, we assume that the input node features are <em>completely uninformative</em>! Very often, this is not a good idea! It can be proven that even placing <em>random numbers</em> in the nodes can yield to a provable improvement in expressive power (Sato et al.<d-cite key="Sato"></d-cite>). Further, many recent works (Loukas et al.<d-cite key="Loukas"></d-cite>); Kanatsoulis and Ribeiro<d-cite key="Ribeiro"></d-cite> make it very explicit that, if we allow GNNs access to “appropriate” input features, this leads to a vast improvement in their expressive power.</p> <p>Even beyond the limitation of the uninformative input features, recent influential works (published at ICLR’22 and ‘23 as orals) have demonstrated that the WL framework itself is worth extending. Geerts and Reutter<d-cite key="Geerts"></d-cite> demonstrate clear theoretical value to expressing GNN computations using a <em>tensor language</em> (TL), allowing for drawing significant connections to <em>color refinement</em> algorithms. And Zhang et al.<d-cite key="Zhang"></d-cite> demonstrate that the WL framework may be <em>weak</em> in terms of its architectural distinguishing power, showing that many higher-order GNNs that surpass the limitations of the 1-WL test, are in fact still incapable of computing many standard polynomial-time-computable metrics over graphs, such as ones relating to the graph’s <em>biconnected components</em>.</p> <p>Lastly, linking back to our central discussion, we argue that focussing the theoretical analysis only on discrete features may not lead to highly learnable target mappings. From the perspective of the WL test (and basically any discrete-valued procedure), the models presented in Deep Sets and PNA are no more powerful than 1-WL. However, moving into continuous feature support, PNA is indeed more powerful at distinguishing graphs than models like GIN.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/deep-learning-blog.github.io/assets/bibliography/2022-12-01-sets-and-graphs.bib"></d-bibliography> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2023" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>