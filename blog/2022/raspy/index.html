<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Thinking Like Transformers | ICLR Blogposts 2023 (staging)</title> <meta name="author" content="abc b c"/> <meta name="description" content="Learn to code as if you were a Transformer."/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/deep-learning-blog.github.io/assets/img/iclr_favicon.ico"/> <link rel="stylesheet" href="/deep-learning-blog.github.io/assets/css/main.css"> <link rel="canonical" href="https://jocelynshen.com/deep-learning-blog.github.io/blog/2022/raspy/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/deep-learning-blog.github.io/assets/js/theme.js"></script> <script src="/deep-learning-blog.github.io/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/deep-learning-blog.github.io/assets/js/distillpub/template.v2.js"></script> <script src="/deep-learning-blog.github.io/assets/js/distillpub/transforms.v2.js"></script> <script src="/deep-learning-blog.github.io/assets/js/distillpub/overrides.js"></script> <style type="text/css">img{display:block;margin-left:auto;margin-right:auto}.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "Thinking Like Transformers",
      "description": "Learn to code as if you were a Transformer.",
      "published": "December 1, 2022",
      "authors": [
        {
          "author": "Anonymous",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/deep-learning-blog.github.io/">ICLR Blogposts 2023 (staging)</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/deep-learning-blog.github.io/about">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/deep-learning-blog.github.io/call">call for blogposts</a> </li> <li class="nav-item "> <a class="nav-link" href="/deep-learning-blog.github.io/submitting">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/deep-learning-blog.github.io/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" target="_blank" rel="noopener noreferrer">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Thinking Like Transformers</h1> <p>Learn to code as if you were a Transformer.</p> </d-title> <d-byline></d-byline> <d-article> <link rel="stylesheet" href="custom.css"> <h1 id="thinking-like-transformers">Thinking Like Transformers</h1> <ul> <li> <a href="https://arxiv.org/pdf/2106.06981.pdf" target="_blank" rel="noopener noreferrer">Paper</a> by Gail Weiss, Yoav Goldberg, Eran Yahav</li> </ul> <p>Transformer models are foundational to AI systems. There are now countless explanations of “how transformers work?” in the sense of the architecture diagram at the heart of transformers.</p> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_5_0.svg" alt="svg"></p> <p>However this diagram does not provide any intuition into the computational model of this framework. As researchers become interested in how Transformers work, gaining intuition into their mechanisms becomes increasingly useful.</p> <p><a href="https://arxiv.org/pdf/2106.06981.pdf" target="_blank" rel="noopener noreferrer">Thinking like Transformers</a> proposes a computational framework for Transformer-like calculations. The framework uses discrete computation to simulate Transformer computations. The resulting language <a href="https://github.com/tech-srl/RASP" target="_blank" rel="noopener noreferrer">RASP</a> is a programming language where, ideally, every program can compile down to a specific Transformer (indeed, David Lindner and colleagues have recently released a <a href="https://arxiv.org/abs/2301.05062" target="_blank" rel="noopener noreferrer">compiler</a> for a large subset of RASP!).&lt;/p&gt;</p> <p>In this blog post, I reimplemented a variant of RASP in Python (RASPy). The language is roughly compatible with the original version, but with some syntactic changes that I thought were fun. With this language, we have a challenging set of puzzles to walk through and understand how it works.</p> <p>Before jumping into the language itself, let’s look at an example of what coding with Transformers looks like. Here is some code that computes the <code class="language-plaintext highlighter-rouge">flip</code>, i.e. reversing an input sequence. The code itself uses two Transformer layers to apply attention and mathematical computations to achieve the result.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">flip</span><span class="p">():</span>
    <span class="n">length</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="mi">1</span><span class="p">)).</span><span class="nf">value</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">flip</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">length</span> <span class="o">-</span> <span class="n">indices</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="n">indices</span><span class="p">)).</span><span class="nf">value</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">flip</span>
<span class="nf">flip</span><span class="p">()</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_11_0.svg" alt="svg"></p> <h2 id="table-of-contents">Table of Contents</h2> <ul> <li>Part 1: <a href="#transformers-as-code">Transformers as Code</a> </li> <li>Part 2: <a href="#coding-with-transformers">Coding with Transformers</a> </li> </ul> <h2 id="transformers-as-code">Transformers as Code</h2> <p>Our goal is to define a computational formalism that mimics the expressivity of Transformers. We will go through this process by analogy, describing each language construct next to the aspect of the Transformer it represents. (See the full <a href="https://arxiv.org/pdf/2106.06981.pdf" target="_blank" rel="noopener noreferrer">paper</a> for the formal language specification).</p> <p>The core unit of the language is a <em>sequence operation</em> that transforms a sequence to another sequence of the same length. I will refer to these throughout as <em>transforms</em>.</p> <h3 id="inputs">Inputs</h3> <p>In a Transformer, the base layer is the input fed to the model. This input usually contains the raw tokens as well as positional information.</p> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_15_0.svg" alt="svg"></p> <p>In code, the symbol <code class="language-plaintext highlighter-rouge">tokens</code> represents the simplest transform. It returns the tokens passed to the model. The default input is the sequence “hello”.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokens</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_17_0.svg" alt="svg"></p> <p>If we want to change the input to the transform, we use the input method to pass in an alternative.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokens</span><span class="p">.</span><span class="nf">input</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_19_0.svg" alt="svg"></p> <p>As with Transformers, we cannot access the positions of these sequences directly. However, to mimic position embeddings, we have access to a sequence of indices.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">indices</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_21_0.svg" alt="svg"></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sop</span> <span class="o">=</span> <span class="n">indices</span>
<span class="n">sop</span><span class="p">.</span><span class="nf">input</span><span class="p">(</span><span class="sh">"</span><span class="s">goodbye</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_22_0.svg" alt="svg"></p> <h3 id="feed-forward-network">Feed Forward Network</h3> <p>After the input layer, we reach the feed-forward network. In a Transformer, this stage can apply mathematical operations to each element of the sequence independently.</p> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_24_0.svg" alt="svg"></p> <p>In code, we represent this stage by computation on transforms. Mathematical operations are overloaded to represent independent computation on each element of the sequence .</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokens</span> <span class="o">==</span> <span class="sh">"</span><span class="s">l</span><span class="sh">"</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_26_0.svg" alt="svg"></p> <p>The result is a new transform. Once constructed it can be applied to new input.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">tokens</span> <span class="o">*</span> <span class="mi">2</span>  <span class="o">-</span> <span class="mi">1</span>
<span class="n">model</span><span class="p">.</span><span class="nf">input</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_28_0.svg" alt="svg"></p> <p>Operations can combine multiple transforms. For example, functions of <code class="language-plaintext highlighter-rouge">tokens</code> and <code class="language-plaintext highlighter-rouge">indices</code>. The analogy here is that the Transformer activations can keep track of multiple pieces of information simultaneously.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">tokens</span> <span class="o">-</span> <span class="mi">5</span> <span class="o">+</span> <span class="n">indices</span>
<span class="n">model</span><span class="p">.</span><span class="nf">input</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_30_0.svg" alt="svg"></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">tokens</span> <span class="o">==</span> <span class="sh">"</span><span class="s">l</span><span class="sh">"</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">indices</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_31_0.svg" alt="svg"></p> <p>We provide a few helper functions to make it easier to write transforms. For example, <code class="language-plaintext highlighter-rouge">where</code> provides an “if” statement like construct</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">where</span><span class="p">((</span><span class="n">tokens</span> <span class="o">==</span> <span class="sh">"</span><span class="s">h</span><span class="sh">"</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">tokens</span> <span class="o">==</span> <span class="sh">"</span><span class="s">l</span><span class="sh">"</span><span class="p">),</span> <span class="n">tokens</span><span class="p">,</span> <span class="sh">"</span><span class="s">q</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_33_0.svg" alt="svg"></p> <p>And <code class="language-plaintext highlighter-rouge">map</code> lets us define our own operators, for instance a string to int transform. (Users should be careful to only use operations here that could be computed with a simple neural network).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">atoi</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">ord</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="nf">ord</span><span class="p">(</span><span class="sh">'</span><span class="s">0</span><span class="sh">'</span><span class="p">))</span>
<span class="n">atoi</span><span class="p">.</span><span class="nf">input</span><span class="p">(</span><span class="sh">"</span><span class="s">31234</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_35_0.svg" alt="svg"></p> <p>When chaining these transforms, it is often easier to work with functions. For example the following applies where and then <code>atoi</code> and then adds 2.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">atoi</span><span class="p">(</span><span class="n">seq</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">seq</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">ord</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="nf">ord</span><span class="p">(</span><span class="sh">'</span><span class="s">0</span><span class="sh">'</span><span class="p">))</span> 

<span class="n">op</span> <span class="o">=</span> <span class="p">(</span><span class="nf">atoi</span><span class="p">(</span><span class="nf">where</span><span class="p">(</span><span class="n">tokens</span> <span class="o">==</span> <span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">0</span><span class="sh">"</span><span class="p">,</span> <span class="n">tokens</span><span class="p">))</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">op</span><span class="p">.</span><span class="nf">input</span><span class="p">(</span><span class="sh">"</span><span class="s">02-13</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_37_0.svg" alt="svg"></p> <p>From here on, unless we use a different input sequence, we will assume that the input is ‘hello’ and omit the input display in the illustrations.</p> <h3 id="attention-selectors">Attention Selectors</h3> <p>Things get more interesting when we start to apply attention. This allows routing of information between the different elements of the sequence.</p> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_39_0.svg" alt="svg"></p> <p>We begin by defining notation for the keys and queries of the model. Keys and queries are effectively transforms that we will broadcast and compare to each other to create <em>selectors</em>, our parallel to attention patterns. We create them directly from transforms. For example, if we want to define a key, we call <code class="language-plaintext highlighter-rouge">key</code> on a transform.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">key</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_41_0.svg" alt="svg"></p> <p>Similarly for <code class="language-plaintext highlighter-rouge">query</code>. (Queries are presented as columns to reflect their relation to the selectors we will create from them.)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">query</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_43_0.svg" alt="svg"></p> <p>Scalars can be used as keys or queries. They broadcast out to the length of the underlying sequence.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">query</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_45_0.svg" alt="svg"></p> <p>By applying a comparison operation between a key and a query we create a <em>selector</em>, our parallel to an attention matrix - though this one is unweighted.</p> <p>A selector is a binary matrix indicating which input position (column) each output position (row) will attend to in an eventual attention computation. In the comparison creating it, the key values describe the input (column) positions, and the query values describe the output (row) positions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">eq</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
<span class="n">eq</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_47_0.svg" alt="svg"></p> <p>Some examples:</p> <ul> <li>A selector that matches each output position to the previous input position.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">offset</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="n">indices</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">offset</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_49_0.svg" alt="svg"></p> <ul> <li>A selector that matches each output position to all earlier input positions.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">before</span> <span class="o">=</span> <span class="nf">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nf">query</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">before</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_51_0.svg" alt="svg"></p> <ul> <li>A selector that matches each output position to all later input positions.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">after</span> <span class="o">=</span> <span class="nf">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nf">query</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">after</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_53_0.svg" alt="svg"></p> <p>Selectors can be merged using boolean operations. For example, this selector focuses each output position on 1) earlier positions that 2) contain the same original input token as its own. We show this by including both pairs of keys and queries in the matrix.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">before</span> <span class="o">&amp;</span> <span class="n">eq</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_55_0.svg" alt="svg"></p> <h2 id="using-attention">Using Attention</h2> <p>Given an attention selector we can provide a value sequence to aggregate. We represent aggregation by <strong>summing</strong> up over the values that have a true value for their selector.</p> <p>(Note: in the original paper, they use a <strong>mean</strong> aggregation and show a clever construction where mean aggregation is able to represent a sum calculation. RASPy uses sum by default for simplicity and to avoid fractions. In practicce this means that RASPy may underestimate the number of layers needed to convert to a mean based model by a factor of 2.)</p> <p>Attention aggregation gives us the ability to compute functions like histograms.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="n">tokens</span><span class="p">)).</span><span class="nf">value</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_59_0.svg" alt="svg"></p> <p>Visually we follow the architecture diagram. Queries are to the left, Keys at the top, Values at the bottom, and the Output is to the right.</p> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_61_0.svg" alt="svg"></p> <p>Some attention operations may not even use the input tokens. For instance to compute the <code class="language-plaintext highlighter-rouge">length</code> of a sequence, we create a “select all” attention selector and then add 1 from each position.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">length</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="mi">1</span><span class="p">)).</span><span class="nf">value</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">length</span> <span class="o">=</span> <span class="n">length</span><span class="p">.</span><span class="nf">name</span><span class="p">(</span><span class="sh">"</span><span class="s">length</span><span class="sh">"</span><span class="p">)</span>
<span class="n">length</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_63_0.svg" alt="svg"></p> <p>Here’s a more complex example, shown step-by-step. (This is the kind of thing they ask in interviews!)</p> <p>Say we want to compute the sum of neighboring values in a sequence, along a sliding window. First we apply the forward cutoff, attending only to positions that are not too far in the past.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">WINDOW</span><span class="o">=</span><span class="mi">3</span>
<span class="n">s1</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="nf">query</span><span class="p">(</span><span class="n">indices</span> <span class="o">-</span> <span class="n">WINDOW</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>  
<span class="n">s1</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_65_0.svg" alt="svg"></p> <p>Then the backward cutoff, attending only to positions up to and including our own.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s2</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="nf">query</span><span class="p">(</span><span class="n">indices</span><span class="p">))</span>
<span class="n">s2</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_67_0.svg" alt="svg"></p> <p>Intersect.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sel</span> <span class="o">=</span> <span class="n">s1</span> <span class="o">&amp;</span> <span class="n">s2</span>
<span class="n">sel</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_69_0.svg" alt="svg"></p> <p>And finally aggregate.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sum2</span> <span class="o">=</span> <span class="n">sel</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> 
<span class="n">sum2</span><span class="p">.</span><span class="nf">input</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_71_0.svg" alt="svg"></p> <p>Here is a simple example that produces a 2-layer transform. The first corresponds to computing length and the second the cumulative sum. The cumulative sum has to go into a second layer because it is applied to a transform which uses length, and so it can only be computed after the computation of length is complete.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">cumsum</span><span class="p">(</span><span class="n">seq</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">before</span> <span class="o">|</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="n">indices</span><span class="p">))).</span><span class="nf">value</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="nf">name</span><span class="p">(</span><span class="sh">"</span><span class="s">cumsum</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">cumsum</span><span class="p">().</span><span class="nf">input</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_73_0.svg" alt="svg"></p> <h2 id="layers">Layers</h2> <p>The language supports building up more complex transforms. It keeps track of the <em>layers</em> by tracking the operations computed so far.</p> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_76_0.svg" alt="svg"></p> <p>Here is a simple example that produces a 2-layer transform. The first corresponds to computing length and the second the cumulative sum.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="nf">cumsum</span><span class="p">(</span><span class="n">length</span> <span class="o">-</span> <span class="n">indices</span><span class="p">)</span>
<span class="n">x</span><span class="p">.</span><span class="nf">input</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_78_0.svg" alt="svg"></p> <h2 id="coding-with-transformers">Coding with Transformers</h2> <p>Given this library of functions, we can write operations to accomplish surprisingly complex tasks.</p> <p><strong>Can we produce a Transformer that does basic addition of two arbitrary length numbers?</strong></p> <p>i.e. given a string “19492+23919” can we produce the correct output?</p> <p>We will go through these steps, and their solutions, here. If you would rather do them on your own, we provide a version where you can try them yourself!</p> <p>Before we dive in to the main task, we will do some challenges of increasing difficulty to help us build some intuitions.</p> <h3 id="challenge-1-select-a-given-index">Challenge 1: Select a given index</h3> <p>Produce a sequence where all the elements have the value at index i.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">index</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">seq</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="n">i</span><span class="p">)).</span><span class="nf">value</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="nf">name</span><span class="p">(</span><span class="sh">"</span><span class="s">index</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">index</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_83_0.svg" alt="svg"></p> <h3 id="challenge-2-shift">Challenge 2: Shift</h3> <p>Shift all of the tokens in a sequence to the right by i positions. (Here we introduce an optional parameter in the aggregation: the default value to be used when no input positions are selected. If not defined, this value is 0.)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">shift</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="sh">"</span><span class="s">_</span><span class="sh">"</span><span class="p">,</span> <span class="n">seq</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="n">indices</span><span class="o">-</span><span class="n">i</span><span class="p">)).</span><span class="nf">value</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">default</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="nf">name</span><span class="p">(</span><span class="sh">"</span><span class="s">shift</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">shift</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_85_0.svg" alt="svg"></p> <h3 id="challenge-3-minimum">Challenge 3: Minimum</h3> <p>Compute the minimum values of the sequence. (This one starts to get harder. Our version uses 2 layers of attention.)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">minimum</span><span class="p">(</span><span class="n">seq</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="n">sel1</span> <span class="o">=</span> <span class="n">before</span> <span class="o">&amp;</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="n">seq</span><span class="p">))</span>
    <span class="n">sel2</span> <span class="o">=</span> <span class="nf">key</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nf">query</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
    <span class="n">less</span> <span class="o">=</span> <span class="p">(</span><span class="n">sel1</span> <span class="o">|</span> <span class="n">sel2</span><span class="p">).</span><span class="nf">value</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">less</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="mi">0</span><span class="p">)).</span><span class="nf">value</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="nf">name</span><span class="p">(</span><span class="sh">"</span><span class="s">min</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">minimum</span><span class="p">()([</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_87_0.svg" alt="svg"></p> <p>The idea behind our solution is an implicit full ordering of the input positions: we (implicitly) order the positions according to input token value, with input position as tie breaker. Our first act is to have each position attend to all positions before it in the ordering: <code class="language-plaintext highlighter-rouge">sel1</code> focuses on earlier input positions with the same input token value, and <code class="language-plaintext highlighter-rouge">sel2</code> focuses on input positions with lower input token value. We then aggregate a 1 from all positions to get where each position is located in this ordering (i.e., how many other positions precede it). The minimum value is the input value at the first position according to this ordering (i.e., the one which had no other positions precede it).</p> <h3 id="challenge-4-first-index">Challenge 4: First Index</h3> <p>Compute the first index that has token q, assuming the sequence always has length shorter than 100. (2 layers)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">first</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">seq</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">minimum</span><span class="p">(</span><span class="nf">where</span><span class="p">(</span><span class="n">seq</span> <span class="o">==</span> <span class="n">q</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="mi">99</span><span class="p">))</span>
<span class="nf">first</span><span class="p">(</span><span class="sh">"</span><span class="s">l</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_90_0.svg" alt="svg"></p> <h3 id="challenge-5-right-align">Challenge 5: Right Align</h3> <p>Right align a padded sequence e.g. ralign().inputs(‘xyz___’) = ‘—xyz’” (2 layers)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ralign</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="p">,</span> <span class="n">sop</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">sop</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="sh">"</span><span class="s">_</span><span class="sh">"</span><span class="p">)).</span><span class="nf">value</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">indices</span> <span class="o">+</span> <span class="n">c</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="n">indices</span><span class="p">)).</span><span class="nf">value</span><span class="p">(</span><span class="n">sop</span><span class="p">,</span> <span class="n">default</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="nf">name</span><span class="p">(</span><span class="sh">"</span><span class="s">ralign</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">ralign</span><span class="p">()(</span><span class="sh">"</span><span class="s">xyz__</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_92_0.svg" alt="svg"></p> <h3 id="challenge-6-split">Challenge 6: Split</h3> <p>Split a sequence into two parts at value v and then right align. You can assume there is exactly one appearance of v in the sequence. (3 layers to get and align the first part of the sequence, but only 1 for the second.)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">get_first_part</span><span class="p">,</span> <span class="n">sop</span><span class="o">=</span><span class="n">tokens</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="sh">"</span><span class="s">0</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">split_point</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">sop</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="n">v</span><span class="p">)).</span><span class="nf">value</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">get_first_part</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nf">ralign</span><span class="p">(</span><span class="n">default</span><span class="p">,</span> 
                   <span class="nf">where</span><span class="p">(</span><span class="n">indices</span> <span class="o">&lt;</span> <span class="n">split_point</span><span class="p">,</span> 
                         <span class="n">sop</span><span class="p">,</span> <span class="sh">"</span><span class="s">_</span><span class="sh">"</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nf">where</span><span class="p">(</span><span class="n">indices</span> <span class="o">&gt;</span> <span class="n">split_point</span><span class="p">,</span> <span class="n">sop</span><span class="p">,</span> <span class="n">default</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
<span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">,</span> <span class="bp">False</span><span class="p">)(</span><span class="sh">"</span><span class="s">xyz+zyr</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_94_0.svg" alt="svg"></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">,</span> <span class="mi">0</span><span class="p">)(</span><span class="sh">"</span><span class="s">xyz+zyr</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_95_0.svg" alt="svg"></p> <h3 id="challenge-6-slide">Challenge 6: Slide</h3> <p>Replace special tokens “&lt;” with the closest non “&lt;” value to their right. (2 layers)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">slide</span><span class="p">(</span><span class="n">match</span><span class="p">,</span> <span class="n">seq</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nf">cumsum</span><span class="p">(</span><span class="n">match</span><span class="p">)</span> 
    <span class="n">y</span> <span class="o">=</span> <span class="p">((</span><span class="nf">key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="o">&amp;</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">match</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="bp">True</span><span class="p">))).</span><span class="nf">value</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
    <span class="n">seq</span> <span class="o">=</span>  <span class="nf">where</span><span class="p">(</span><span class="n">match</span><span class="p">,</span> <span class="n">seq</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">seq</span><span class="p">.</span><span class="nf">name</span><span class="p">(</span><span class="sh">"</span><span class="s">slide</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">slide</span><span class="p">(</span><span class="n">tokens</span> <span class="o">!=</span> <span class="sh">"</span><span class="s">&lt;</span><span class="sh">"</span><span class="p">).</span><span class="nf">input</span><span class="p">(</span><span class="sh">"</span><span class="s">xxxh&lt;&lt;&lt;l</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_97_0.svg" alt="svg"></p> <h3 id="challenge-7-add">Challenge 7: Add</h3> <p>For this one you want to perform addition of two numbers. Here are the steps.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">add</span><span class="p">().</span><span class="nf">input</span><span class="p">(</span><span class="sh">"</span><span class="s">683+345</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <ol> <li>Split into parts (challenge 6). Convert to ints. Add.</li> </ol> <blockquote> <p>“683+345” =&gt; [0, 0, 0, 9, 12, 8]</p> </blockquote> <ol> <li>Compute the carry terms. Three possibilities: definitely receives carry (“1”), definitely doesn’t receive carry (“0”), maybe receives carry (“&lt;”).Because we are only adding two numbers, the only case in which a position might receive a carry is if the position after it sums to 9. In that case, it will receive a carry if and only if the position after <em>that</em> receives a carry.</li> </ol> <blockquote> <p>[0, 0, 0, 9, 12, 8] =&gt; “00&lt;100”</p> </blockquote> <ol> <li>Slide the carry coefficients. A position that might receive a carry will get one if and only if the next position receives a carry - and so on down the chain until the next definite carry/no carry.</li> </ol> <blockquote> <p>“00&lt;100” =&gt; 001100”</p> </blockquote> <ol> <li>Complete the addition.</li> </ol> <p>Each of these is 1 line of code. The full system is 6 layers. (if you are careful you can do it in 5!).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">sop</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="c1"># 0) Parse and add
</span>    <span class="n">x</span> <span class="o">=</span> <span class="nf">atoi</span><span class="p">(</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="n">sop</span><span class="p">))</span> \
        <span class="o">+</span> <span class="nf">atoi</span><span class="p">(</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">,</span> <span class="bp">False</span><span class="p">,</span> <span class="n">sop</span><span class="p">))</span>
    <span class="c1"># 1) Check for carries 
</span>    <span class="n">gets_carry</span> <span class="o">=</span> <span class="nf">shift</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="sh">"</span><span class="s">0</span><span class="sh">"</span><span class="p">,</span> <span class="nf">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">9</span><span class="p">,</span> <span class="sh">"</span><span class="s">1</span><span class="sh">"</span><span class="p">,</span> <span class="nf">where</span><span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="mi">9</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">0</span><span class="sh">"</span><span class="p">)))</span>
    <span class="c1"># 2) Slide carries to their columns - all in one parallel go!                                         
</span>    <span class="n">gets_carry</span> <span class="o">=</span> <span class="nf">atoi</span><span class="p">(</span><span class="nf">slide</span><span class="p">(</span><span class="n">gets_carry</span> <span class="o">!=</span> <span class="sh">"</span><span class="s">&lt;</span><span class="sh">"</span><span class="p">,</span> <span class="n">gets_carry</span><span class="p">))</span>
    <span class="c1"># 3) Add in carries, and remove overflow from original addition.                                                                                  
</span>    <span class="nf">return </span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">gets_carry</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span>
<span class="nf">add</span><span class="p">()(</span><span class="sh">"</span><span class="s">683+345</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/deep-learning-blog.github.io/assets/img/2022-12-01-raspy/Blog_99_0.svg" alt="svg"></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">683</span> <span class="o">+</span> <span class="mi">345</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1028
</code></pre></div></div> <p>Pretty neat stuff. If you are interested more in this topic, be sure to check at the paper:</p> <p><a href="https://arxiv.org/pdf/2106.06981.pdf" target="_blank" rel="noopener noreferrer">Thinking like Transformers</a> and the <a href="https://github.com/tech-srl/RASP" target="_blank" rel="noopener noreferrer">RASP language</a>.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/deep-learning-blog.github.io/assets/bibliography/"></d-bibliography> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2023" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>