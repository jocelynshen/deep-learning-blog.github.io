<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> blog | ICLR Blogposts 2023 (staging)</title> <meta name="author" content="abc b c"/> <meta name="description" content="Staging website for the 2023 ICLR Blogposts track "/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/deep-learning-blog.github.io/assets/img/iclr_favicon.ico"/> <link rel="stylesheet" href="/deep-learning-blog.github.io/assets/css/main.css"> <link rel="canonical" href="https://jocelynshen.com/deep-learning-blog.github.io/blog/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/deep-learning-blog.github.io/assets/js/theme.js"></script> <script src="/deep-learning-blog.github.io/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/deep-learning-blog.github.io/">ICLR Blogposts 2023 (staging)</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/deep-learning-blog.github.io/about">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/deep-learning-blog.github.io/call">call for blogposts</a> </li> <li class="nav-item "> <a class="nav-link" href="/deep-learning-blog.github.io/submitting">submitting</a> </li> <li class="nav-item active"> <a class="nav-link" href="/deep-learning-blog.github.io/blog/index.html">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" target="_blank" rel="noopener noreferrer">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="header-background"><div class="img"></div></div> <div class="container mt-5"> <div class="post"> <div class="header-bar"> <h1>blogposts</h1> <h2>Blogpost track entries</h2> </div> <div class="tag-list"> <ul class="p-0 m-0"> <li> <i class="fas fa-hashtag fa-sm"></i> <a href="/deep-learning-blog.github.io/blog/tag/formatting">formatting</a> </li> <p>•</p> <li> <i class="fas fa-hashtag fa-sm"></i> <a href="/deep-learning-blog.github.io/blog/tag/images">images</a> </li> <p>•</p> <li> <i class="fas fa-hashtag fa-sm"></i> <a href="/deep-learning-blog.github.io/blog/tag/links">links</a> </li> <p>•</p> <li> <i class="fas fa-hashtag fa-sm"></i> <a href="/deep-learning-blog.github.io/blog/tag/math">math</a> </li> <p>•</p> <li> <i class="fas fa-hashtag fa-sm"></i> <a href="/deep-learning-blog.github.io/blog/tag/code">code</a> </li> </ul> </div> <ul class="post-list"> <li> <h3> <a class="post-title" href="/deep-learning-blog.github.io/blog/2023/welcome-to-jekyll/">Welcome to Jekyll!</a> </h3> <p></p> <p class="post-meta"> 2 min read   ·   October 12, 2023 </p> <p class="post-tags"> <a href="/deep-learning-blog.github.io/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/deep-learning-blog.github.io/blog/category/jekyll"> <i class="fas fa-tag fa-sm"></i> jekyll</a>   <a href="/deep-learning-blog.github.io/blog/category/update"> <i class="fas fa-tag fa-sm"></i> update</a>   </p> </li> <li> <h3> <a class="post-title" href="/deep-learning-blog.github.io/blog/2023/diffusion-is-all-you-need/">A Match Made in Drug Discovery - Marrying Geometric and Diffusion Models</a> </h3> <p>The chemical space of molecular candidates is vast, and within this space, interesting and powerful drugs are waiting to be found. The use of machine learning has shown to be an effective method to speed up the process of discovering novel compounds, especially the use of (deep) generative models. The recent surge in graph generative models have opened up new avenues for exploring the chemical space of molecular candidates, enabling a more efficient and systematic exploration of the chemical space, increasing the chances of finding novel and potent molecules. One of the recent breakthroughs includes the use of diffusion models, which have proven to yield superior performance in molecular conformation tasks, among others. In this blog post, we aim to highlight one of them, which is the 'GeoDiff - A Geometric Diffusion Model for Molecular Conformation Generation' paper by Xu et al. (2022). We aim to distill the paper in semi-layman terms, to provide researchers and practitioners with a deeper understanding of the (i) methodology and results and (ii) (societal) implications of this breakthrough in the field of drug discovery and (iii) discuss future applications in the field of (bio)medicine.</p> <p class="post-meta"> 17 min read   ·   February 17, 2023 </p> <p class="post-tags"> <a href="/deep-learning-blog.github.io/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/deep-learning-blog.github.io/blog/2023/language-rl/">Language and (Meta-) RL - An ode to structure</a> </h3> <p>It has been argued that language can be a very powerful way to compress information about the world. In fact, the learning of humans is significantly sped up around the time they start understanding and using language. A natural question, then, is whether the same can be argued for sequential decision-making systems that either learn to optimize a single or multiple, task. To this end, there has been a surge of works exploring the use of Language in Reinforcement Learning (RL) and Meta-RL. The goal of this blog post is to try and explain some of the recent works in this sub-field and help elucidate how language can help with incorporating structure about the environment to improve learning generalization in (Meta) RL</p> <p class="post-meta"> 14 min read   ·   February 11, 2023 </p> <p class="post-tags"> <a href="/deep-learning-blog.github.io/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/deep-learning-blog.github.io/blog/2023/taking_notes_on_the_fly/">Transformers Learn Faster by Taking Notes on the Fly</a> </h3> <p>If your unsupervised pre-training is taking forever and you need a lightweight solution that will accelerate it, taking notes might be the method you are looking for! This method takes notes of the contextual information of the rare words and incorporates this information as a part of their embeddings on the fly! The solution is lightweight in the sense that it does not increase the inference time and it does not require an additional pass during training. The experiments demonstrate that this method reduces the pre-training time of large language models by up to 60%.</p> <p class="post-meta"> 15 min read   ·   February 7, 2023 </p> <p class="post-tags"> <a href="/deep-learning-blog.github.io/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/deep-learning-blog.github.io/blog/2023/controllable-music-generation-via-midi-ddsp/">Controllable Music Generation via MIDI-DDSP</a> </h3> <p>Recently, there has been a lot of fascinating work focusing on making music generation for a larger and more general audience. However, these models could be of great help to the artists if they can intervene into the generation process at multiple levels in order to control what notes are played and how they are performed. We hence dive deeper into MIDI-DDSP that helps with high-fidelity generations using an interpretable hierarchy with several degrees of granularity.</p> <p class="post-meta"> 10 min read   ·   February 2, 2023 </p> <p class="post-tags"> <a href="/deep-learning-blog.github.io/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/deep-learning-blog.github.io/blog/2023/Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/">Adaptive Reward Penalty in Safe Reinforcement Learning</a> </h3> <p>In this blog, we dive into the ICLR 2019 paper Reward Constrained Policy Optimization (RCPO) by Tessler et al. and highlight the importance of adaptive reward shaping in safe reinforcement learning. We reproduce the paper's experimental results by implementing RCPO into Proximal Policy Optimization (PPO). This blog aims to provide researchers and practitioners with (1) a better understanding of safe reinforcement learning in terms of constrained optimization and (2) how penalized reward functions can be effectively used to train a robust policy.</p> <p class="post-meta"> 16 min read   ·   January 31, 2023 </p> <p class="post-tags"> <a href="/deep-learning-blog.github.io/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/deep-learning-blog.github.io/blog/2023/Underfitting-and-Regularization-Finding-the-Right-Balance/">Underfitting and Regularization: Finding the Right Balance</a> </h3> <p>In this blog post, we will go over the ICLR 2022 paper titled NETWORK AUGMENTATION FOR TINY DEEP LEARNING. This paper introduces a new training method for improving the performance of tiny neural networks. NetAug augments the network (reverse dropout), it puts the tiny model into larger models and encourages it to work as a sub-model of larger models to get extra supervision.</p> <p class="post-meta"> 21 min read   ·   January 15, 2023 </p> <p class="post-tags"> <a href="/deep-learning-blog.github.io/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/deep-learning-blog.github.io/blog/2022/riit/">Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-agent Reinforcement Learning</a> </h3> <p></p> <p class="post-meta"> 44 min read   ·   December 13, 2022 </p> <p class="post-tags"> <a href="/deep-learning-blog.github.io/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/deep-learning-blog.github.io/blog/tag/multi-agent"> <i class="fas fa-hashtag fa-sm"></i> multi-agent</a>   <a href="/deep-learning-blog.github.io/blog/tag/reinforcement-learning"> <i class="fas fa-hashtag fa-sm"></i> reinforcement-learning</a>   <a href="/deep-learning-blog.github.io/blog/tag/experimental-techniques"> <i class="fas fa-hashtag fa-sm"></i> experimental techniques</a>   <a href="/deep-learning-blog.github.io/blog/tag/monotonicity"> <i class="fas fa-hashtag fa-sm"></i> monotonicity</a>   </p> </li> <li> <h3> <a class="post-title" href="/deep-learning-blog.github.io/blog/2022/hippo-to-h3/">From HiPPO to H3</a> </h3> <p>Equipping State Space Models for Language</p> <p class="post-meta"> 46 min read   ·   December 1, 2022 </p> <p class="post-tags"> <a href="/deep-learning-blog.github.io/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </li> <li> <h3> <a class="post-title" href="/deep-learning-blog.github.io/blog/2022/text2vid/">Building Blocks of Text-to-Video Generation</a> </h3> <p>Google's Imagen Video and Meta's Make-a-Video Explained</p> <p class="post-meta"> 21 min read   ·   December 1, 2022 </p> <p class="post-tags"> <a href="/deep-learning-blog.github.io/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </li> <li> <h3> <a class="post-title" href="/deep-learning-blog.github.io/blog/2022/sets-and-graphs/">Universality of Neural Networks on Sets vs. Graphs</a> </h3> <p>Universal function approximation is one of the central tenets in theoretical deep learning research. It is the question whether a specific neural network architecture is, in theory, able to approximate any function of interest. The ICLR paper “How Powerful are Graph Neural Networks?” shows that mathematically analysing the constraints of an architecture as a universal function approximator and alleviating these constraints can lead to more principled architecture choices, performance improvements, and long term impact on the field. Specifically in the fields of learning on sets and learning on graphs, universal function approximation is a well-studied property. The two fields are closely linked, because the need for permutation invariance in both cases lead to similar building blocks. However, these two fields have evolved in parallel, often lacking awareness of developments in the respective other field. This post aims at bringing these two fields closer together, particularly from the perspective of universal function approximation.</p> <p class="post-meta"> 18 min read   ·   December 1, 2022 </p> <p class="post-tags"> <a href="/deep-learning-blog.github.io/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </li> <li> <h3> <a class="post-title" href="/deep-learning-blog.github.io/blog/2022/raspy/">Thinking Like Transformers</a> </h3> <p>Learn to code as if you were a Transformer.</p> <p class="post-meta"> 21 min read   ·   December 1, 2022 </p> <p class="post-tags"> <a href="/deep-learning-blog.github.io/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </li> <li> <h3> <a class="post-title" href="/deep-learning-blog.github.io/blog/2022/how-much-meta-learning-is-in-image-to-image-translation/">How much meta-learning is in image-to-image translation?</a> </h3> <p>...in which we find a connection between meta-learning literature and a paper studying how well CNNs deal with nuisance transforms in a class-imbalanced setting. Closer inspection reveals a surprising amount of similarity - from meta-information to loss functions.</p> <p class="post-meta"> 22 min read   ·   December 1, 2022 </p> <p class="post-tags"> <a href="/deep-learning-blog.github.io/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </li> <li> <h3> <a class="post-title" href="/deep-learning-blog.github.io/blog/2022/how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/">How does the inductive bias influence the generalization capability of neural networks?</a> </h3> <p>The blog post discusses how memorization and generalization are affected by extreme overparameterization.</p> <p class="post-meta"> 11 min read   ·   December 1, 2022 </p> <p class="post-tags"> <a href="/deep-learning-blog.github.io/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </li> <li> <h3> <a class="post-title" href="/deep-learning-blog.github.io/blog/2022/hitchhikers-momentum/">A Hitchhiker's Guide to Momentum</a> </h3> <p>Polyak momentum is one of the most iconic methods in optimization. Despite it's simplicity, it features rich dynamics that depend both on the step-size and momentum parameter. In this blog post we identify the different regions of the parameter space and discuss their convergence properties using the theory of Chebyshev polynomials.</p> <p class="post-meta"> 18 min read   ·   December 1, 2022 </p> <p class="post-tags"> <a href="/deep-learning-blog.github.io/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </li> <li> <h3> <a class="post-title" href="/deep-learning-blog.github.io/blog/2022/facial-poisoning/">Data Poisoning is Hitting a Wall</a> </h3> <p>In this post, we look at the paper 'Data Poisoning Won't Save You From Facial Recognition', discuss the impact of the work, and additionally look at how this work fares in the current state of adversarial machine learning. Being a blog post as opposed to a traditional paper, we try to avoid inundating the reader with mathematical equations and complex terminologies. Instead, we aim to put forth this work's primary concept and implications, along with our observations, in a clear, concise manner. Don't want to go through the entire post? Check out the TL;DR at the end for a quick summary.</p> <p class="post-meta"> 17 min read   ·   December 1, 2022 </p> <p class="post-tags"> <a href="/deep-learning-blog.github.io/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </li> <li> <h3> <a class="post-title" href="/deep-learning-blog.github.io/blog/2022/distill-example/">Sample Blog Post</a> </h3> <p>Your blog post's abstract. This is an example of a distill-style blog post and the main elements it supports.</p> <p class="post-meta"> 15 min read   ·   December 1, 2022 </p> <p class="post-tags"> <a href="/deep-learning-blog.github.io/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </li> <li> <h3> <a class="post-title" href="/deep-learning-blog.github.io/blog/2022/classification-layer-initialization-in-maml/">Strategies for Classification Layer Initialization in Model-Agnostic Meta-Learning</a> </h3> <p>This blog post discusses different strategies for initializing the classification layers parameters before fine-tuning on a new task in Model-Agnostic Meta-Learning.</p> <p class="post-meta"> 19 min read   ·   December 1, 2022 </p> <p class="post-tags"> <a href="/deep-learning-blog.github.io/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </li> <li> <h3> <a class="post-title" href="/deep-learning-blog.github.io/blog/2022/bsuite-applications/">Practical Applications of Bsuite For Reinforcement Learning</a> </h3> <p>In 2019, researchers at DeepMind published a suite of reinforcement learning environments called Behavior Suite for Reinforcement Learning, or bsuite. Each environment is designed to directly test a core capability of a general reinforcement learning agent, such as its ability to generalize from past experience or handle delayed rewards. The authors claim that bsuite can be used to benchmark agents and bridge the gap between theoretical and applied reinforcement learning understanding. In this blog post, we extend their work by providing specific examples of how bsuite can address common challenges faced by reinforcement learning practitioners during the development process. Our work offers pragmatic guidance to researchers and highlights future research directions in reproducible reinforcement learning.</p> <p class="post-meta"> 37 min read   ·   December 1, 2022 </p> <p class="post-tags"> <a href="/deep-learning-blog.github.io/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </li> <li> <h3> <a class="post-title" href="/deep-learning-blog.github.io/blog/2022/autoregressive-neural-pde-solver/">Autoregressive Renaissance in Neural PDE Solvers</a> </h3> <p>Recent developments in the field of neural partial differential equation (PDE) solvers have placed a strong emphasis on neural operators. However, the paper Message Passing Neural PDE Solver by Brandstetter et al. published in ICLR 2022 revisits autoregressive models and designs a message passing graph neural network that is comparable with or outperforms both the state-of-the-art Fourier Neural Operator and traditional classical PDE solvers in its generalization capabilities and performance. This blog post delves into the key contributions of this work, exploring the strategies used to address the common problem of instability in autoregressive models and the design choices of the message passing graph neural network architecture.</p> <p class="post-meta"> 31 min read   ·   December 1, 2022 </p> <p class="post-tags"> <a href="/deep-learning-blog.github.io/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </li> <li> <h3> <a class="post-title" href="/deep-learning-blog.github.io/blog/2022/adamw/">Decay No More</a> </h3> <p>Weight decay is among the most important tuning parameters to reach high accuracy for large-scale machine learning models. In this blog post, we revisit AdamW, the weight decay version of Adam, summarizing empirical findings as well as theoretical motivations from an optimization perspective.</p> <p class="post-meta"> 21 min read   ·   December 1, 2022 </p> <p class="post-tags"> <a href="/deep-learning-blog.github.io/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </li> </ul> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/deep-learning-blog.github.io/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/deep-learning-blog.github.io/assets/js/zoom.js"></script> <script defer src="/deep-learning-blog.github.io/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>