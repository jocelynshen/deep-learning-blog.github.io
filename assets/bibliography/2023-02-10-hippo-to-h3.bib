@misc{hippo,
    title={HiPPO: Recurrent Memory with Optimal Polynomial Projections},
    author={Albert Gu and Tri Dao and Stefano Ermon and Atri Rudra and Christopher Re},
    year={2020},
    eprint={2008.07669},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{s4,
    title={Efficiently Modeling Long Sequences with Structured State Spaces},
    author={Albert Gu and Karan Goel and Christopher Ré},
    year={2021},
    eprint={2111.00396},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{s4d,
    title={On the Parameterization and Initialization of Diagonal State Space Models},
    author={Albert Gu and Ankit Gupta and Karan Goel and Christopher Ré},
    year={2022},
    eprint={2206.11893},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@inproceedings{h3,
  title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
  author={Dao, Tri and Fu, Daniel Y. and Saab, Khaled K. and Thomas, Armin W.
  and Rudra, Atri and R{\'e}, Christopher},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@inproceedings{s4ann,
  author = {Rush, Alexander and Karamcheti, Sidd},
  title = {The Annotated S4},
  booktitle = {ICLR Blog Track},
  year = {2022},
  note = {https://iclr-blog-track.github.io/2022/03/25/annotated-s4/},
  url  = {https://iclr-blog-track.github.io/2022/03/25/annotated-s4/}
}

@misc{flash_attn,
    title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
    author={Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher Ré},
    year={2022},
    eprint={2205.14135},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{gu2021combining,
    title={Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers},
    author={Albert Gu and Isys Johnson and Karan Goel and Khaled Saab and Tri Dao and Atri Rudra and Christopher Ré},
    year={2021},
    eprint={2110.13985},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{goel2022raw,
    title={It's Raw! Audio Generation with State-Space Models},
    author={Karan Goel and Albert Gu and Chris Donahue and Christopher Ré},
    year={2022},
    eprint={2202.09729},
    archivePrefix={arXiv},
    primaryClass={cs.SD}
}

@misc{nguyen2022s4nd,
    title={S4ND: Modeling Images and Videos as Multidimensional Signals Using State Spaces},
    author={Eric Nguyen and Karan Goel and Albert Gu and Gordon W. Downs and Preey Shah and Tri Dao and Stephen A. Baccus and Christopher Ré},
    year={2022},
    eprint={2210.06583},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{wang2020linformer,
    title={Linformer: Self-Attention with Linear Complexity},
    author={Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},
    year={2020},
    eprint={2006.04768},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{lmu,
	author = {Voelker, Aaron and Kaji\'{c}, Ivana and Eliasmith, Chris},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks},
	url = {https://proceedings.neurips.cc/paper/2019/file/952285b9b7e7a1be5aa7849f32ffff05-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper/2019/file/952285b9b7e7a1be5aa7849f32ffff05-Paper.pdf}}


@article{inductionheads,
  title={In-context Learning and Induction Heads},
  author={Catherine Olsson and Nelson Elhage and Neel Nanda and Nicholas Joseph and Nova DasSarma and T. J. Henighan and Benjamin Mann and Amanda Askell and Yuntao Bai and Anna Chen and Tom Conerly and Dawn Drain and Deep Ganguli and Zac Hatfield-Dodds and Danny Hernandez and Scott Johnston and Andy Jones and John Kernion and Liane Lovitt and Kamal Ndousse and Dario Amodei and Tom B. Brown and Jack Clark and Jared Kaplan and Sam McCandlish and Christopher Olah},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.11895}
}

@misc{choromanski2020rethinking,
    title={Rethinking Attention with Performers},
    author={Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and David Belanger and Lucy Colwell and Adrian Weller},
    year={2020},
    eprint={2009.14794},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{Gao2020ThePA,
  title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Leo Gao and Stella Rose Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
  journal={ArXiv},
  year={2020},
  volume={abs/2101.00027}
}

@article{Ba2016UsingFW,
  title={Using Fast Weights to Attend to the Recent Past},
  author={Jimmy Ba and Geoffrey E. Hinton and Volodymyr Mnih and Joel Z. Leibo and Catalin Ionescu},
  journal={ArXiv},
  year={2016},
  volume={abs/1610.06258}
}

@article{Baevski2018AdaptiveIR,
  title={Adaptive Input Representations for Neural Language Modeling},
  author={Alexei Baevski and Michael Auli},
  journal={ArXiv},
  year={2018},
  volume={abs/1809.10853}
}

@article{Vaswani2017AttentionIA,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  journal={ArXiv},
  year={2017},
  volume={abs/1706.03762}
}

@article{Gupta2022DiagonalSS,
  title={Diagonal State Spaces are as Effective as Structured State Spaces},
  author={Ankit Gupta and Jonathan Berant},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.14343}
}

@inproceedings{Black2021GPTNeoLS,
  title={GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow},
  author={Sid Black and Leo Gao and Phil Wang and Connor Leahy and Stella Rose Biderman},
  year={2021}
}

@misc{lssl,
    title={Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers},
    author={Albert Gu and Isys Johnson and Karan Goel and Khaled Saab and Tri Dao and Atri Rudra and Christopher Ré},
    year={2021},
    eprint={2110.13985},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{induction_heads,
  title={In-context Learning and Induction Heads},
  author={Catherine Olsson and Nelson Elhage and Neel Nanda and Nicholas Joseph and Nova DasSarma and T. J. Henighan and Benjamin Mann and Amanda Askell and Yuntao Bai and Anna Chen and Tom Conerly and Dawn Drain and Deep Ganguli and Zac Hatfield-Dodds and Danny Hernandez and Scott Johnston and Andy Jones and John Kernion and Liane Lovitt and Kamal Ndousse and Dario Amodei and Tom B. Brown and Jack Clark and Jared Kaplan and Sam McCandlish and Christopher Olah},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.11895}
}

@article{Kitaev2020ReformerTE,
  title={Reformer: The Efficient Transformer},
  author={Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
  journal={ArXiv},
  year={2020},
  volume={abs/2001.04451}
}

@article{Rabe2021SelfattentionDN,
  title={Self-attention Does Not Need O(n2) Memory},
  author={Markus N. Rabe and Charles Staats},
  journal={ArXiv},
  year={2021},
  volume={abs/2112.05682}
}

@misc{opt,
    title={OPT: Open Pre-trained Transformer Language Models},
    author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
    year={2022},
    eprint={2205.01068},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@software{gpt-neox-lib,
  title = {{GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch}},
  author = {Andonian, Alex and Anthony, Quentin and Biderman, Stella and Black, Sid and Gali, Preetham and Gao, Leo and Hallahan, Eric and Levy-Kramer, Josh and Leahy, Connor and Nestler, Lucas and Parker, Kip and Pieler, Michael and Purohit, Shivanshu and Songz, Tri and Phil, Wang and Weinbach, Samuel},
  url = {https://www.github.com/eleutherai/gpt-neox},
  doi = {10.5281/zenodo.5879544},
  month = {8},
  year = {2021},
  version = {0.0.1},
}

@misc{linattn,
    title={Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},
    author={Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and François Fleuret},
    year={2020},
    eprint={2006.16236},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}