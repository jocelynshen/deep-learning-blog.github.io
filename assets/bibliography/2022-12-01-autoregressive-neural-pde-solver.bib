
@article{cranmerDiscoveringSymbolicModels2020,
	title = {Discovering symbolic models from deep learning with inductive biases},
	volume = {33},
	url = {https://arxiv.org/abs/2006.11287},
	journal = {Advances in Neural Information Processing Systems},
	author = {Cranmer, Miles and Sanchez Gonzalez, Alvaro and Battaglia, Peter and Xu, Rui and Cranmer, Kyle and Spergel, David and Ho, Shirley},
	year = {2020},
	keywords = {Read},
	pages = {17429--17442},
	file = {Full Text:C\:\\Users\\yolan\\Zotero\\storage\\JEEDWUR8\\Cranmer et al. - 2020 - Discovering symbolic models from deep learning wit.pdf:application/pdf;Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\E48UYH7S\\c9f2f917078bd2db12f23c3b413d9cba-Abstract.html:text/html},
}

@inproceedings{thorpeGRANDGraphNeural2021,
	title = {GRAND++: Graph neural diffusion with a source term},
	shorttitle = {GRAND++},
	url = {https://openreview.net/forum?id=EMxu-dzvJk},
	booktitle = {International Conference on Learning Representations},
	author = {Thorpe, Matthew and Nguyen, Tan Minh and Xia, Hedi and Strohmer, Thomas and Bertozzi, Andrea and Osher, Stanley and Wang, Bao},
	year = {2021},
	file = {Full Text:C\:\\Users\\yolan\\Zotero\\storage\\2QIWI8WZ\\Thorpe et al. - 2021 - GRAND++ Graph neural diffusion with a source term.pdf:application/pdf;Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\3GMFQGTG\\forum.html:text/html},
}

@article{rackauckasUniversalDifferentialEquations2020,
	title = {Universal differential equations for scientific machine learning},
	url = {https://arxiv.org/abs/2001.04385},
	journal = {arXiv preprint arXiv:2001.04385},
	author = {Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali and Edelman, Alan},
	year = {2020},
	file = {Full Text:C\:\\Users\\yolan\\Zotero\\storage\\TP2LKX6F\\Rackauckas et al. - 2020 - Universal differential equations for scientific ma.pdf:application/pdf;Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\32JEYVDU\\2001.html:text/html},
}

@article{liPhysicsinformedNeuralOperator2021,
	title = {Physics-informed neural operator for learning partial differential equations},
	url = {https://arxiv.org/abs/2111.03794},
	journal = {arXiv preprint arXiv:2111.03794},
	author = {Li, Zongyi and Zheng, Hongkai and Kovachki, Nikola and Jin, David and Chen, Haoxuan and Liu, Burigede and Azizzadenesheli, Kamyar and Anandkumar, Anima},
	year = {2021},
	file = {Full Text:C\:\\Users\\yolan\\Zotero\\storage\\QDGI4XMG\\Li et al. - 2021 - Physics-informed neural operator for learning part.pdf:application/pdf;Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\6PUYLHMX\\2111.html:text/html},
}

@misc{pilvaLearningTimedependentPDE2022,
	title = {Learning time-dependent PDE solver using Message Passing Graph Neural Networks},
	url = {http://arxiv.org/abs/2204.07651},
	doi = {10.48550/arXiv.2204.07651},
	abstract = {One of the main challenges in solving time-dependent partial differential equations is to develop computationally efficient solvers that are accurate and stable. Here, we introduce a graph neural network approach to finding efficient PDE solvers through learning using message-passing models. We first introduce domain invariant features for PDE-data inspired by classical PDE solvers for an efficient physical representation. Next, we use graphs to represent PDE-data on an unstructured mesh and show that message passing graph neural networks (MPGNN) can parameterize governing equations, and as a result, efficiently learn accurate solver schemes for linear/nonlinear PDEs. We further show that the solvers are independent of the initial trained geometry, i.e. the trained solver can find PDE solution on different complex domains. Lastly, we show that a recurrent graph neural network approach can find a temporal sequence of solutions to a PDE.},
	urldate = {2022-12-06},
	publisher = {arXiv},
	author = {Pilva, Pourya and Zareei, Ahmad},
	month = apr,
	year = {2022},
	note = {arXiv:2204.07651 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {ICLR Rejection Comments 2022
https://openreview.net/forum?id=oaKw-GmBZZ
https://openreview.net/pdf?id=oaKw-GmBZZ
},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\XX28KIPJ\\2204.html:text/html;Learning time-dependent PDE solver using Message Passing Graph Neural Networks_Pilva2022.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Learning time-dependent PDE solver using Message Passing Graph Neural Networks_Pilva2022.pdf:application/pdf},
}

@inproceedings{iakovlevLearningContinuoustimePDEs2021,
	title = {Learning continuous-time PDEs from sparse data with graph neural networks},
	url = {https://openreview.net/forum?id=aUX5Plaq7Oy},
	abstract = {The behavior of many dynamical systems follow complex, yet still unknown partial differential equations (PDEs). While several machine learning methods have been proposed to learn PDEs directly from data, previous methods are limited to discrete-time approximations or make the limiting assumption of the observations arriving at regular grids. We propose a general continuous-time differential model for dynamical systems whose governing equations are parameterized by message passing graph neural networks. The model admits arbitrary space and time discretizations, which removes constraints on the locations of observation points and time intervals between the observations. The model is trained with continuous-time adjoint method enabling efficient neural PDE inference. We demonstrate the model's ability to work with unstructured grids, arbitrary time steps, and noisy observations. We compare our method with existing approaches on several well-known physical systems that involve first and higher-order PDEs with state-of-the-art predictive performance.},
	language = {en},
	urldate = {2022-12-06},
	author = {Iakovlev, Valerii and Heinonen, Markus and Lähdesmäki, Harri},
	month = jan,
	year = {2021},
	file = {Learning continuous-time PDEs from sparse data with graph neural networks_Iakovlev2021.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Learning continuous-time PDEs from sparse data with graph neural networks_Iakovlev2021.pdf:application/pdf;Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\2HW8TE82\\forum.html:text/html},
}

@article{eliasofPdegcnNovelArchitectures2021,
	title = {Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations},
	volume = {34},
	shorttitle = {Pde-gcn},
	url = {https://arxiv.org/abs/2108.01938},
	journal = {Advances in Neural Information Processing Systems},
	author = {Eliasof, Moshe and Haber, Eldad and Treister, Eran},
	year = {2021},
	pages = {3836--3849},
	file = {Pde-gcn_Eliasof2021.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Pde-gcn_Eliasof2021.pdf:application/pdf;Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\P49BEK56\\1f9f9d8ff75205aa73ec83e543d8b571-Abstract.html:text/html},
}

@inproceedings{anandkumarNeuralOperatorGraph2022,
	title = {Neural Operator: Graph Kernel Network for Partial Differential Equations},
	shorttitle = {Neural Operator},
	url = {https://openreview.net/forum?id=fg2ZFmXFO3},
	abstract = {The classical development of neural networks has been primarily for mappings between a finite-dimensional Euclidean space and a set of classes, or between two finite-dimensional Euclidean spaces. The purpose of this work is to generalize neural networks so that they can learn mappings between infinite-dimensional spaces (operators). We formulate approximation of the infinite-dimensional mapping by composing nonlinear activation functions and a class of integral operators. The kernel integration is computed by message passing on graph networks. This approach has substantial practical consequences which we will illustrate in the context of mappings between input data to partial differential equations (PDEs) and their solutions. In this context, such learned networks can generalize among different approximation methods for the PDE (such as finite difference or finite element methods) and among approximations corresponding to different underlying levels of resolution and discretization. Experiments confirm the purposed graph kernel network does have the desired properties and show competitive performance compared to the stat of the art solvers.},
	language = {en},
	urldate = {2022-12-06},
	author = {Anandkumar, Anima and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Kovachki, Nikola and Li, Zongyi and Liu, Burigede and Stuart, Andrew},
	month = jun,
	year = {2022},
	file = {Neural Operator_Anandkumar2022.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Neural Operator_Anandkumar2022.pdf:application/pdf;Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\8MLINA6H\\forum.html:text/html},
}

@misc{kovachkiNeuralOperatorLearning2022,
	title = {Neural Operator: Learning Maps Between Function Spaces},
	shorttitle = {Neural Operator},
	url = {http://arxiv.org/abs/2108.08481},
	doi = {10.48550/arXiv.2108.08481},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite dimensional Euclidean spaces or finite sets. We propose a generalization of neural networks to learn operators, termed neural operators, that map between infinite dimensional function spaces. We formulate the neural operator as a composition of linear integral operators and nonlinear activation functions. We prove a universal approximation theorem for our proposed neural operator, showing that it can approximate any given nonlinear continuous operator. The proposed neural operators are also discretization-invariant, i.e., they share the same model parameters among different discretization of the underlying function spaces. Furthermore, we introduce four classes of efficient parameterization, viz., graph neural operators, multi-pole graph neural operators, low-rank neural operators, and Fourier neural operators. An important application for neural operators is learning surrogate maps for the solution operators of partial differential equations (PDEs). We consider standard PDEs such as the Burgers, Darcy subsurface flow, and the Navier-Stokes equations, and show that the proposed neural operators have superior performance compared to existing machine learning based methodologies, while being several orders of magnitude faster than conventional PDE solvers.},
	urldate = {2022-12-06},
	publisher = {arXiv},
	author = {Kovachki, Nikola and Li, Zongyi and Liu, Burigede and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	month = oct,
	year = {2022},
	note = {arXiv:2108.08481 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\CGRPH5AU\\2108.html:text/html;Neural Operator_Kovachki2022.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Neural Operator_Kovachki2022.pdf:application/pdf},
}

@misc{guptaMultispatiotemporalscaleGeneralizedPDE2022,
	title = {Towards Multi-spatiotemporal-scale Generalized PDE Modeling},
	url = {http://arxiv.org/abs/2209.15616},
	doi = {10.48550/arXiv.2209.15616},
	abstract = {Partial differential equations (PDEs) are central to describing complex physical system simulations. Their expensive solution techniques have led to an increased interest in deep neural network based surrogates. However, the practical utility of training such surrogates is contingent on their ability to model complex multi-scale spatio-temporal phenomena. Various neural network architectures have been proposed to target such phenomena, most notably Fourier Neural Operators (FNOs), which give a natural handle over local \& global spatial information via parameterization of different Fourier modes, and U-Nets which treat local and global information via downsampling and upsampling paths. However, generalizing across different equation parameters or time-scales still remains a challenge. In this work, we make a comprehensive comparison between various FNO, ResNet, and U-Net like approaches to fluid mechanics problems in both vorticity-stream and velocity function form. For U-Nets, we transfer recent architectural improvements from computer vision, most notably from object segmentation and generative modeling. We further analyze the design considerations for using FNO layers to improve performance of U-Net architectures without major degradation of computational cost. Finally, we show promising results on generalization to different PDE parameters and time-scales with a single surrogate model. Source code for our PyTorch benchmark framework is available at https://github.com/microsoft/pdearena.},
	urldate = {2022-12-06},
	publisher = {arXiv},
	author = {Gupta, Jayesh K. and Brandstetter, Johannes},
	month = nov,
	year = {2022},
	note = {arXiv:2209.15616 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {ICLR Rejection Comments 2023
https://openreview.net/forum?id=Uk40pC45YJG
},
	file = {Gupta and Brandstetter - 2022 - Towards Multi-spatiotemporal-scale Generalized PDE.pdf:C\:\\Users\\yolan\\Zotero\\storage\\94FD6ZFE\\Gupta and Brandstetter - 2022 - Towards Multi-spatiotemporal-scale Generalized PDE.pdf:application/pdf},
}

@article{luDeepONetLearningNonlinear2021,
	title = {DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators},
	volume = {3},
	issn = {2522-5839},
	shorttitle = {DeepONet},
	url = {http://arxiv.org/abs/1910.03193},
	doi = {10.1038/s42256-021-00302-5},
	abstract = {While it is widely known that neural networks are universal approximators of continuous functions, a less known and perhaps more powerful result is that a neural network with a single hidden layer can approximate accurately any nonlinear continuous operator. This universal approximation theorem is suggestive of the potential application of neural networks in learning nonlinear operators from data. However, the theorem guarantees only a small approximation error for a sufficient large network, and does not consider the important optimization and generalization errors. To realize this theorem in practice, we propose deep operator networks (DeepONets) to learn operators accurately and efficiently from a relatively small dataset. A DeepONet consists of two sub-networks, one for encoding the input function at a fixed number of sensors \$x\_i, i=1,{\textbackslash}dots,m\$ (branch net), and another for encoding the locations for the output functions (trunk net). We perform systematic simulations for identifying two types of operators, i.e., dynamic systems and partial differential equations, and demonstrate that DeepONet significantly reduces the generalization error compared to the fully-connected networks. We also derive theoretically the dependence of the approximation error in terms of the number of sensors (where the input function is defined) as well as the input function type, and we verify the theorem with computational results. More importantly, we observe high-order error convergence in our computational tests, namely polynomial rates (from half order to fourth order) and even exponential convergence with respect to the training dataset size.},
	number = {3},
	urldate = {2022-12-08},
	journal = {Nature Machine Intelligence},
	author = {Lu, Lu and Jin, Pengzhan and Karniadakis, George Em},
	month = mar,
	year = {2021},
	note = {arXiv:1910.03193 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {218--229},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\P6U86A7R\\1910.html:text/html;DeepONet_Lu2021.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\DeepONet_Lu2021.pdf:application/pdf},
}

@misc{brandstetterLiePointSymmetry2022,
	title = {Lie Point Symmetry Data Augmentation for Neural PDE Solvers},
	url = {http://arxiv.org/abs/2202.07643},
	abstract = {Neural networks are increasingly being used to solve partial differential equations (PDEs), replacing slower numerical solvers. However, a critical issue is that neural PDE solvers require high-quality ground truth data, which usually must come from the very solvers they are designed to replace. Thus, we are presented with a proverbial chicken-and-egg problem. In this paper, we present a method, which can partially alleviate this problem, by improving neural PDE solver sample complexity -- Lie point symmetry data augmentation (LPSDA). In the context of PDEs, it turns out that we are able to quantitatively derive an exhaustive list of data transformations, based on the Lie point symmetry group of the PDEs in question, something not possible in other application areas. We present this framework and demonstrate how it can easily be deployed to improve neural PDE solver sample complexity by an order of magnitude.},
	urldate = {2023-01-07},
	publisher = {arXiv},
	author = {Brandstetter, Johannes and Welling, Max and Worrall, Daniel E.},
	month = may,
	year = {2022},
	note = {arXiv:2202.07643 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Published at ICML 2022, Github: https://github.com/brandstetter-johannes/LPSDA},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\SPLAYTDF\\2202.html:text/html;Lie Point Symmetry Data Augmentation for Neural PDE Solvers_Brandstetter2022.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Lie Point Symmetry Data Augmentation for Neural PDE Solvers_Brandstetter2022.pdf:application/pdf},
}

@misc{liNeuralOperatorGraph2020,
	title = {Neural Operator: Graph Kernel Network for Partial Differential Equations},
	shorttitle = {Neural Operator},
	url = {http://arxiv.org/abs/2003.03485},
	abstract = {The classical development of neural networks has been primarily for mappings between a finite-dimensional Euclidean space and a set of classes, or between two finite-dimensional Euclidean spaces. The purpose of this work is to generalize neural networks so that they can learn mappings between infinite-dimensional spaces (operators). The key innovation in our work is that a single set of network parameters, within a carefully designed network architecture, may be used to describe mappings between infinite-dimensional spaces and between different finite-dimensional approximations of those spaces. We formulate approximation of the infinite-dimensional mapping by composing nonlinear activation functions and a class of integral operators. The kernel integration is computed by message passing on graph networks. This approach has substantial practical consequences which we will illustrate in the context of mappings between input data to partial differential equations (PDEs) and their solutions. In this context, such learned networks can generalize among different approximation methods for the PDE (such as finite difference or finite element methods) and among approximations corresponding to different underlying levels of resolution and discretization. Experiments confirm that the proposed graph kernel network does have the desired properties and show competitive performance compared to the state of the art solvers.},
	urldate = {2023-01-07},
	publisher = {arXiv},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	month = mar,
	year = {2020},
	note = {arXiv:2003.03485 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\FVPQV4EA\\2003.html:text/html;Neural Operator_Li2020.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Neural Operator_Li2020.pdf:application/pdf},
}

@misc{AutoregressiveModelsDeep2019,
	title = {Autoregressive Models in Deep Learning — A Brief Survey},
	url = {https://www.georgeho.org/deep-autoregressive-models/},
	abstract = {My current project involves working with deep autoregressive models: a class of remarkable neural networks that aren’t usually seen on a first pass through deep learning. These notes are a quick write-up of my reading and research: I assume basic familiarity with deep learning, and aim to highlight general trends and similarities across autoregressive models, instead of commenting on individual architectures.
tldr: Deep autoregressive models are sequence models, yet feed-forward (i.},
	language = {en-us},
	urldate = {2023-02-07},
	journal = {⁂ George Ho},
	month = mar,
	year = {2019},
	note = {Section: blog},
	file = {Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\DIARI5SK\\deep-autoregressive-models.html:text/html},
}

@article{bar-sinaiLearningDatadrivenDiscretizations2019,
	title = {Learning data-driven discretizations for partial differential equations},
	volume = {116},
	url = {https://www.pnas.org/doi/10.1073/pnas.1814058116},
	doi = {10.1073/pnas.1814058116},
	abstract = {The numerical solution of partial differential equations (PDEs) is challenging because of the need to resolve spatiotemporal features over wide length- and timescales. Often, it is computationally intractable to resolve the finest features in the solution. The only recourse is to use approximate coarse-grained representations, which aim to accurately represent long-wavelength dynamics while properly accounting for unresolved small-scale physics. Deriving such coarse-grained equations is notoriously difficult and often ad hoc. Here we introduce data-driven discretization, a method for learning optimized approximations to PDEs based on actual solutions to the known underlying equations. Our approach uses neural networks to estimate spatial derivatives, which are optimized end to end to best satisfy the equations on a low-resolution grid. The resulting numerical methods are remarkably accurate, allowing us to integrate in time a collection of nonlinear equations in 1 spatial dimension at resolutions 4× to 8× coarser than is possible with standard finite-difference methods.},
	number = {31},
	urldate = {2023-02-07},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bar-Sinai, Yohai and Hoyer, Stephan and Hickey, Jason and Brenner, Michael P.},
	month = jul,
	year = {2019},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {15344--15349},
	file = {Learning data-driven discretizations for partial differential equations_Bar-Sinai2019.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Learning data-driven discretizations for partial differential equations_Bar-Sinai2019.pdf:application/pdf},
}

@inproceedings{hsiehLearningNeuralPDE2019,
	title = {Learning Neural PDE Solvers with Convergence Guarantees},
	url = {https://openreview.net/forum?id=rklaWn0qK7},
	abstract = {Partial differential equations (PDEs) are widely used across the physical and computational sciences. Decades of research and engineering went into designing fast iterative solution methods. Existing solvers are general purpose, but may be sub-optimal for specific classes of problems. In contrast to existing hand-crafted solutions, we propose an approach to learn a fast iterative solver tailored to a specific domain. We achieve this goal by learning to modify the updates of an existing solver using a deep neural network. Crucially, our approach is proven to preserve strong correctness and convergence guarantees. After training on a single geometry, our model generalizes to a wide variety of geometries and boundary conditions, and achieves 2-3 times speedup compared to state-of-the-art solvers.},
	language = {en},
	urldate = {2023-02-07},
	author = {Hsieh, Jun-Ting and Zhao, Shengjia and Eismann, Stephan and Mirabella, Lucia and Ermon, Stefano},
	month = apr,
	year = {2019},
	file = {Learning Neural PDE Solvers with Convergence Guarantees_Hsieh2019.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Learning Neural PDE Solvers with Convergence Guarantees_Hsieh2019.pdf:application/pdf},
}

@misc{ruthottoDeepNeuralNetworks2018,
	title = {Deep Neural Networks Motivated by Partial Differential Equations},
	url = {http://arxiv.org/abs/1804.04272},
	doi = {10.48550/arXiv.1804.04272},
	abstract = {Partial differential equations (PDEs) are indispensable for modeling many physical phenomena and also commonly used for solving image processing tasks. In the latter area, PDE-based approaches interpret image data as discretizations of multivariate functions and the output of image processing algorithms as solutions to certain PDEs. Posing image processing problems in the infinite dimensional setting provides powerful tools for their analysis and solution. Over the last few decades, the reinterpretation of classical image processing problems through the PDE lens has been creating multiple celebrated approaches that benefit a vast area of tasks including image segmentation, denoising, registration, and reconstruction. In this paper, we establish a new PDE-interpretation of a class of deep convolutional neural networks (CNN) that are commonly used to learn from speech, image, and video data. Our interpretation includes convolution residual neural networks (ResNet), which are among the most promising approaches for tasks such as image classification having improved the state-of-the-art performance in prestigious benchmark challenges. Despite their recent successes, deep ResNets still face some critical challenges associated with their design, immense computational costs and memory requirements, and lack of understanding of their reasoning. Guided by well-established PDE theory, we derive three new ResNet architectures that fall into two new classes: parabolic and hyperbolic CNNs. We demonstrate how PDE theory can provide new insights and algorithms for deep learning and demonstrate the competitiveness of three new CNN architectures using numerical experiments.},
	urldate = {2023-02-07},
	publisher = {arXiv},
	author = {Ruthotto, Lars and Haber, Eldad},
	month = dec,
	year = {2018},
	note = {arXiv:1804.04272 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, 65K10, 68T45, Mathematics - Optimization and Control},
	annote = {Comment: 9 pages, 4 figures, 1 table},
	annote = {Comment: 9 pages, 4 figures, 1 table},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\U7M94NGT\\1804.html:text/html;Deep Neural Networks Motivated by Partial Differential Equations_Ruthotto2018.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Deep Neural Networks Motivated by Partial Differential Equations_Ruthotto2018.pdf:application/pdf},
}

@misc{HeatHeatEquation,
	title = {Heat #1: The Heat Equation (and a Classical Solver) {\textbar} Inductiva Research Labs},
	url = {https://inductiva.ai/blog/article/heat-1-an-introduction},
	urldate = {2023-02-07},
	file = {Heat #1\: The Heat Equation (and a Classical Solver) | Inductiva Research Labs:C\:\\Users\\yolan\\Zotero\\storage\\NZCHEMT5\\heat-1-an-introduction.html:text/html},
}

@misc{goswamiPhysicsInformedDeepNeural2022,
	title = {Physics-Informed Deep Neural Operator Networks},
	url = {http://arxiv.org/abs/2207.05748},
	abstract = {Standard neural networks can approximate general nonlinear operators, represented either explicitly by a combination of mathematical operators, e.g., in an advection-diffusion-reaction partial differential equation, or simply as a black box, e.g., a system-of-systems. The first neural operator was the Deep Operator Network (DeepONet), proposed in 2019 based on rigorous approximation theory. Since then, a few other less general operators have been published, e.g., based on graph neural networks or Fourier transforms. For black box systems, training of neural operators is data-driven only but if the governing equations are known they can be incorporated into the loss function during training to develop physics-informed neural operators. Neural operators can be used as surrogates in design problems, uncertainty quantification, autonomous systems, and almost in any application requiring real-time inference. Moreover, independently pre-trained DeepONets can be used as components of a complex multi-physics system by coupling them together with relatively light training. Here, we present a review of DeepONet, the Fourier neural operator, and the graph neural operator, as well as appropriate extensions with feature expansions, and highlight their usefulness in diverse applications in computational mechanics, including porous media, fluid mechanics, and solid mechanics.},
	urldate = {2023-02-07},
	publisher = {arXiv},
	author = {Goswami, Somdatta and Bora, Aniruddha and Yu, Yue and Karniadakis, George Em},
	month = jul,
	year = {2022},
	note = {arXiv:2207.05748 [cs, math]
version: 2},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	annote = {Comment: 33 pages, 14 figures. arXiv admin note: text overlap with arXiv:2204.00997 by other authors},
}

@misc{UnderstandingLSTMNetworks,
	title = {Understanding LSTM Networks -- colah's blog},
	url = {https://colah.github.io/posts/2015-08-Understanding-LSTMs/},
	urldate = {2023-02-07},
	file = {Understanding LSTM Networks -- colah's blog:C\:\\Users\\yolan\\Zotero\\storage\\EL4NIQWC\\2015-08-Understanding-LSTMs.html:text/html},
}

@misc{WaveNetGenerativeModel,
	title = {WaveNet: A generative model for raw audio},
	shorttitle = {WaveNet},
	url = {https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio},
	abstract = {This post presents WaveNet, a deep generative model of raw audio waveforms. We show that WaveNets are able to generate speech which mimics any human voice and which sounds more natural than the best existing Text-to-Speech systems, reducing the gap with human performance by over 50\%.},
	language = {en},
	urldate = {2023-02-07},
}

@misc{joshiTransformersAreGraph2020,
	title = {Transformers are Graph Neural Networks},
	url = {https://graphdeeplearning.github.io/post/transformers-are-gnns/},
	abstract = {Engineer friends often ask me: Graph Deep Learning sounds great, but are there any big commercial success stories? Is it being deployed in practical applications?
Besides the obvious ones–recommendation systems at Pinterest, Alibaba and Twitter–a slightly nuanced success story is the Transformer architecture, which has taken the NLP industry by storm.
Through this post, I want to establish links between Graph Neural Networks (GNNs) and Transformers. I’ll talk about the intuitions behind model architectures in the NLP and GNN communities, make connections using equations and figures, and discuss how we could work together to drive progress.},
	language = {en-us},
	urldate = {2023-02-08},
	journal = {NTU Graph Deep Learning Lab},
	author = {Joshi, Chaitanya},
	month = feb,
	year = {2020},
	file = {Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\2RKB8T9W\\transformers-are-gnns.html:text/html},
}

@misc{brandstetterMessagePassingNeural2022a,
	title = {Message Passing Neural PDE Solvers},
	url = {http://arxiv.org/abs/2202.03376},
	doi = {10.48550/arXiv.2202.03376},
	abstract = {The numerical solution of partial differential equations (PDEs) is difficult, having led to a century of research so far. Recently, there have been pushes to build neural--numerical hybrid solvers, which piggy-backs the modern trend towards fully end-to-end learned systems. Most works so far can only generalize over a subset of properties to which a generic solver would be faced, including: resolution, topology, geometry, boundary conditions, domain discretization regularity, dimensionality, etc. In this work, we build a solver, satisfying these properties, where all the components are based on neural message passing, replacing all heuristically designed components in the computation graph with backprop-optimized neural function approximators. We show that neural message passing solvers representationally contain some classical methods, such as finite differences, finite volumes, and WENO schemes. In order to encourage stability in training autoregressive models, we put forward a method that is based on the principle of zero-stability, posing stability as a domain adaptation problem. We validate our method on various fluid-like flow problems, demonstrating fast, stable, and accurate performance across different domain topologies, discretization, etc. in 1D and 2D. Our model outperforms state-of-the-art numerical solvers in the low resolution regime in terms of speed and accuracy.},
	urldate = {2023-02-08},
	publisher = {arXiv},
	author = {Brandstetter, Johannes and Worrall, Daniel and Welling, Max},
	month = mar,
	year = {2022},
	note = {arXiv:2202.03376 [cs, math]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Mathematics - Numerical Analysis},
	annote = {Comment: Published at ICLR 2022 (Spotlight paper), Github: https://github.com/brandstetter-johannes/MP-Neural-PDE-Solvers},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\TRPXYAKP\\2202.html:text/html;Message Passing Neural PDE Solvers_Brandstetter2022.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Message Passing Neural PDE Solvers_Brandstetter22.pdf:application/pdf},
}

@incollection{hairerClassicalMathematicalTheory1993,
	address = {Berlin, Heidelberg},
	series = {Springer Series in Computational Mathematics},
	title = {Classical Mathematical Theory},
	isbn = {978-3-540-78862-1},
	url = {https://doi.org/10.1007/978-3-540-78862-1_1},
	abstract = {This first chapter contains the classical theory of differential equations, which we judge useful and important for a profound understanding of numerical processes and phenomena. It will also be the occasion of presenting interesting examples of differential equations and their properties.},
	language = {en},
	urldate = {2023-02-08},
	booktitle = {Solving Ordinary Differential Equations I: Nonstiff Problems},
	publisher = {Springer},
	editor = {Hairer, Ernst and Wanner, Gerhard and Nørsett, Syvert P.},
	year = {1993},
	doi = {10.1007/978-3-540-78862-1_1},
	pages = {1--128},
}

@misc{chamberlainGRANDGraphNeural2021a,
	title = {GRAND: Graph Neural Diffusion},
	shorttitle = {GRAND},
	url = {http://arxiv.org/abs/2106.10934},
	doi = {10.48550/arXiv.2106.10934},
	abstract = {We present Graph Neural Diffusion (GRAND) that approaches deep learning on graphs as a continuous diffusion process and treats Graph Neural Networks (GNNs) as discretisations of an underlying PDE. In our model, the layer structure and topology correspond to the discretisation choices of temporal and spatial operators. Our approach allows a principled development of a broad new class of GNNs that are able to address the common plights of graph learning models such as depth, oversmoothing, and bottlenecks. Key to the success of our models are stability with respect to perturbations in the data and this is addressed for both implicit and explicit discretisation schemes. We develop linear and nonlinear versions of GRAND, which achieve competitive results on many standard graph benchmarks.},
	urldate = {2023-02-08},
	publisher = {arXiv},
	author = {Chamberlain, Benjamin Paul and Rowbottom, James and Gorinova, Maria and Webb, Stefan and Rossi, Emanuele and Bronstein, Michael M.},
	month = sep,
	year = {2021},
	note = {arXiv:2106.10934 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 15 pages, 4 figures. Proceedings of the 38th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s)},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\ADI2FQ6V\\2106.html:text/html;GRAND_Chamberlain2021.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\GRAND_Chamberlain22.pdf:application/pdf},
}

@book{straussPartialDifferentialEquations2007,
	title = {Partial differential equations: An introduction},
	shorttitle = {Partial differential equations},
	publisher = {John Wiley \& Sons},
	author = {Strauss, Walter A.},
	year = {2007},
	file = {Partial differential equations_Strauss2007.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Partial differential equations_Strauss2007.pdf:application/pdf;Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\P22Q24X5\\books.html:text/html},
}

@inproceedings{greenfeldLearningOptimizeMultigrid2019a,
	title = {Learning to Optimize Multigrid PDE Solvers},
	url = {https://proceedings.mlr.press/v97/greenfeld19a.html},
	abstract = {Constructing fast numerical solvers for partial differential equations (PDEs) is crucial for many scientific disciplines. A leading technique for solving large-scale PDEs is using multigrid methods. At the core of a multigrid solver is the prolongation matrix, which relates between different scales of the problem. This matrix is strongly problem-dependent, and its optimal construction is critical to the efficiency of the solver. In practice, however, devising multigrid algorithms for new problems often poses formidable challenges. In this paper we propose a framework for learning multigrid solvers. Our method learns a (single) mapping from discretized PDEs to prolongation operators for a broad class of 2D diffusion problems. We train a neural network once for the entire class of PDEs, using an efficient and unsupervised loss function. Our tests demonstrate improved convergence rates compared to the widely used Black-Box multigrid scheme, suggesting that our method successfully learned rules for constructing prolongation matrices.},
	language = {en},
	urldate = {2023-02-08},
	booktitle = {Proceedings of the 36th International Conference on Machine Learning},
	publisher = {PMLR},
	author = {Greenfeld, Daniel and Galun, Meirav and Basri, Ronen and Yavneh, Irad and Kimmel, Ron},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2415--2423},
	file = {Learning to Optimize Multigrid PDE Solvers_Greenfeld2019.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Learning to Optimize Multigrid PDE Solvers_Greenfeld3.pdf:application/pdf;Supplementary PDF:C\:\\Users\\yolan\\Zotero\\storage\\AUFTMT63\\Greenfeld et al. - 2019 - Learning to Optimize Multigrid PDE Solvers.pdf:application/pdf},
}

@misc{liFourierNeuralOperator2021,
	title = {Fourier Neural Operator for Parametric Partial Differential Equations},
	url = {http://arxiv.org/abs/2010.08895},
	doi = {10.48550/arXiv.2010.08895},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.},
	urldate = {2023-02-08},
	publisher = {arXiv},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	month = may,
	year = {2021},
	note = {arXiv:2010.08895 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\RNE3GAEH\\2010.html:text/html;Fourier Neural Operator for Parametric Partial Differential Equations_Li2021.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Fourier Neural Operator for Parametric Partial Differential Equations_Li22.pdf:application/pdf},
}

@book{chakravertyAdvancedNumericalSemiAnalytical2019,
	title = {Advanced Numerical and Semi-Analytical Methods for Differential Equations},
	isbn = {978-1-119-42342-3},
	abstract = {Examines numerical and semi-analytical methods for differential equations that can be used for solving practical ODEs and PDEs This student-friendly book deals with various approaches for solving differential equations numerically or semi-analytically depending on the type of equations and offers simple example problems to help readers along. Featuring both traditional and recent methods, Advanced Numerical and Semi Analytical Methods for Differential Equations begins with a review of basic numerical methods. It then looks at Laplace, Fourier, and weighted residual methods for solving differential equations. A new challenging method of Boundary Characteristics Orthogonal Polynomials (BCOPs) is introduced next. The book then discusses Finite Difference Method (FDM), Finite Element Method (FEM), Finite Volume Method (FVM), and Boundary Element Method (BEM). Following that, analytical/semi analytic methods like Akbari Ganji's Method (AGM) and Exp-function are used to solve nonlinear differential equations. Nonlinear differential equations using semi-analytical methods are also addressed, namely Adomian Decomposition Method (ADM), Homotopy Perturbation Method (HPM), Variational Iteration Method (VIM), and Homotopy Analysis Method (HAM). Other topics covered include: emerging areas of research related to the solution of differential equations based on differential quadrature and wavelet approach; combined and hybrid methods for solving differential equations; as well as an overview of fractal differential equations. Further, uncertainty in term of intervals and fuzzy numbers have also been included, along with the interval finite element method. This book:  Discusses various methods for solving linear and nonlinear ODEs and PDEs Covers basic numerical techniques for solving differential equations along with various discretization methods Investigates nonlinear differential equations using semi-analytical methods Examines differential equations in an uncertain environment Includes a new scenario in which uncertainty (in term of intervals and fuzzy numbers) has been included in differential equations Contains solved example problems, as well as some unsolved problems for self-validation of the topics covered   Advanced Numerical and Semi Analytical Methods for Differential Equations is an excellent text for graduate as well as post graduate students and researchers studying various methods for solving differential equations, numerically and semi-analytically.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Chakraverty, Snehashish and Mahato, Nisha and Karunakar, Perumandla and Rao, Tharasi Dilleswar},
	month = apr,
	year = {2019},
	note = {Google-Books-ID: j6uMDwAAQBAJ},
	keywords = {Mathematics / Differential Equations / General, Mathematics / Mathematical Analysis},
}

@book{bartelsNumericalApproximationPartial,
	title = {Numerical Approximation of Partial Differential Equations},
	url = {https://link.springer.com/book/10.1007/978-3-319-32354-1},
	language = {en},
	urldate = {2023-02-08},
	author = {Bartels, Soren},
	file = {Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\P2T8CAGU\\978-3-319-32354-1.html:text/html},
}

@misc{morrisWeisfeilerLemanGo2022,
	title = {Weisfeiler and Leman go Machine Learning: The Story so far},
	shorttitle = {Weisfeiler and Leman go Machine Learning},
	url = {http://arxiv.org/abs/2112.09992},
	doi = {10.48550/arXiv.2112.09992},
	abstract = {In recent years, algorithms and neural architectures based on the Weisfeiler-Leman algorithm, a well-known heuristic for the graph isomorphism problem, have emerged as a powerful tool for machine learning with graphs and relational data. Here, we give a comprehensive overview of the algorithm's use in a machine-learning setting, focusing on the supervised regime. We discuss the theoretical background, show how to use it for supervised graph and node representation learning, discuss recent extensions, and outline the algorithm's connection to (permutation-)equivariant neural architectures. Moreover, we give an overview of current applications and future directions to stimulate further research.},
	urldate = {2023-02-08},
	publisher = {arXiv},
	author = {Morris, Christopher and Lipman, Yaron and Maron, Haggai and Rieck, Bastian and Kriege, Nils M. and Grohe, Martin and Fey, Matthias and Borgwardt, Karsten},
	month = dec,
	year = {2022},
	note = {arXiv:2112.09992 [cs, stat]},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\GR7XCVZA\\2112.html:text/html;Weisfeiler and Leman go Machine Learning_Morris2022.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Weisfeiler and Leman go Machine Learning_Morris2022.pdf:application/pdf},
}
