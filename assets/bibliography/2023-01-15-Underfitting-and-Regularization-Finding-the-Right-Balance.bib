@inproceedings{DBLP:conf/iclr/CaiG0022,
  author    = {Han Cai and
               Chuang Gan and
               Ji Lin and
               Song Han},
  title     = {Network Augmentation for Tiny Deep Learning},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
               2022, Virtual Event, April 25-29, 2022},
  publisher = {OpenReview.net},
  year      = {2022},
  url       = {https://openreview.net/forum?id=TYw3-OlrRm-},
  timestamp = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/CaiG0022.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{10.5555/2627435.2670313,
  author     = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  title      = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  year       = {2014},
  issue_date = {January 2014},
  publisher  = {JMLR.org},
  volume     = {15},
  number     = {1},
  issn       = {1532-4435},
  abstract   = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  journal    = {J. Mach. Learn. Res.},
  month      = {jan},
  pages      = {1929–1958},
  numpages   = {30},
  keywords   = {deep learning, model combination, neural networks, regularization}
}
@article{9043731,
  author  = {Deng, Lei and Li, Guoqi and Han, Song and Shi, Luping and Xie, Yuan},
  journal = {Proceedings of the IEEE},
  title   = {Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey},
  year    = {2020},
  volume  = {108},
  number  = {4},
  pages   = {485-532},
  doi     = {10.1109/JPROC.2020.2976475}
}
@inproceedings{jacob2018quantization,
  title     = {Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author    = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {2704--2713},
  year      = {2018}
}
@misc{https://doi.org/10.48550/arxiv.1712.04621,
  doi       = {10.48550/ARXIV.1712.04621},
  url       = {https://arxiv.org/abs/1712.04621},
  author    = {Perez, Luis and Wang, Jason},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {The Effectiveness of Data Augmentation in Image Classification using Deep Learning},
  publisher = {arXiv},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{10.1007/s10462-019-09784-7,
  author     = {Moradi, Reza and Berangi, Reza and Minaei, Behrouz},
  title      = {A Survey of Regularization Strategies for Deep Models},
  year       = {2020},
  issue_date = {Aug 2020},
  publisher  = {Kluwer Academic Publishers},
  address    = {USA},
  volume     = {53},
  number     = {6},
  issn       = {0269-2821},
  url        = {https://doi.org/10.1007/s10462-019-09784-7},
  doi        = {10.1007/s10462-019-09784-7},
  abstract   = {The most critical concern in machine learning is how to make an algorithm that performs well both on training data and new data. No free lunch theorem implies that each specific task needs its own tailored machine learning algorithm to be designed. A set of strategies and preferences are built into learning machines to tune them for the problem at hand. These strategies and preferences, with the core concern of generalization improvement, are collectively known as regularization. In deep learning, because of a considerable number of parameters, a great many forms of regularization methods are available to the deep learning community. Developing more effective regularization strategies has been the subject of significant research efforts in recent years. However, it is difficult for developers to choose the most suitable strategy for their problem at hand, because there is no comparative study regarding the performance of different strategies. In this paper, at the first step, the most effective regularization methods and their variants are presented and analyzed in a systematic approach. At the second step, comparative research on regularization techniques is presented in which the testing errors and computational costs are evaluated in a convolutional neural network, using CIFAR-10 () dataset. In the end, different regularization methods are compared in terms of accuracy of the network, the number of epochs for the network to be trained and the number of operations per input sample. Also, the results are discussed and interpreted based on the employed strategy. The experiment results showed that weight decay and data augmentation regularizations have little computational side effects so can be used in most applications. In the case of enough computational resources, Dropout family methods are rational to be used. Moreover, in the case of abundant computational resources, batch normalization family and ensemble methods are reasonable strategies to be employed.},
  journal    = {Artif. Intell. Rev.},
  month      = {aug},
  pages      = {3947–3986},
  numpages   = {40},
  keywords   = {Generalization, Convolutional neural network, Overfitting, Deep learning, Regularization}
}
@article{yang2021speeding,
  title   = {Speeding up deep model training by sharing weights and then unsharing},
  author  = {Yang, Shuo and Hou, Le and Song, Xiaodan and Liu, Qiang and Zhou, Denny},
  journal = {arXiv preprint arXiv:2110.03848},
  year    = {2021}
}
@inproceedings{guo2020single,
  title        = {Single path one-shot neural architecture search with uniform sampling},
  author       = {Guo, Zichao and Zhang, Xiangyu and Mu, Haoyuan and Heng, Wen and Liu, Zechun and Wei, Yichen and Sun, Jian},
  booktitle    = {Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XVI 16},
  pages        = {544--560},
  year         = {2020},
  organization = {Springer}
}
@article{hinton2015distilling,
  title   = {Distilling the knowledge in a neural network},
  author  = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal = {arXiv preprint arXiv:1503.02531},
  year    = {2015}
}
@article{lin2020mcunet,
  title   = {Mcunet: Tiny deep learning on iot devices},
  author  = {Lin, Ji and Chen, Wei-Ming and Lin, Yujun and Gan, Chuang and Han, Song},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  year    = {2020}
}
@inproceedings{lin2021mcunetv2,
  title     = {MCUNetV2: Memory-Efficient Patch-based Inference for Tiny Deep Learning},
  author    = {Lin, Ji and Chen, Wei-Ming and Cai, Han and Gan, Chuang and Han, Song},
  booktitle = {Annual Conference on Neural Information Processing Systems (NeurIPS)},
  year      = {2021}
} 
@article{lin2022ondevice,
  title   = {On-Device Training Under 256KB Memory},
  author  = {Lin, Ji and Zhu, Ligeng and Chen, Wei-Ming and Wang, Wei-Chen and Gan, Chuang and Han, Song},
  journal = {arXiv:2206.15472 [cs]},
  url     = {https://arxiv.org/abs/2206.15472},
  year    = {2022}
}
@article{https://doi.org/10.48550/arxiv.1808.05377,
  doi       = {10.48550/ARXIV.1808.05377},
  url       = {https://arxiv.org/abs/1808.05377},
  author    = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  keywords  = {Machine Learning (stat.ML), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Neural Architecture Search: A Survey},
  publisher = {arXiv},
  year      = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}