
@Article{Zhuang2022,
  author  = {Zhenxun Zhuang and Mingrui Liu and Ashok Cutkosky and Francesco Orabona},
  journal = {Transactions on Machine Learning Research},
  title   = {Understanding AdamW through Proximal Methods and Scale-Freeness},
  year    = {2022},
  url     = {https://openreview.net/forum?id=IKhEPWGdwK},
}

@InProceedings{Loshchilov2019,
  author    = {Ilya Loshchilov and Frank Hutter},
  booktitle = {7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019},
  title     = {Decoupled Weight Decay Regularization},
  year      = {2019},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/iclr/LoshchilovH19.bib},
  timestamp = {Thu, 25 Jul 2019 14:26:04 +0200},
}

@InProceedings{Kingma2015,
  author    = {Diederik P. Kingma and Jimmy Ba},
  booktitle = {3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  title     = {Adam: A Method for Stochastic Optimization},
  year      = {2015},
  editor    = {Yoshua Bengio and Yann LeCun},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  groups    = {stochastic_methods},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
}

@InCollection{Paszke2019,
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  publisher = {Curran Associates, Inc.},
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  year      = {2019},
  pages     = {8024--8035},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
}

@InProceedings{He2016,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Deep Residual Learning for Image Recognition},
  year      = {2016},
  pages     = {770-778},
  doi       = {10.1109/CVPR.2016.90},
}

@Misc{Abadi2015,
  author = {Martin Abadi and Ashish Agarwal and Paul Barham and Eugene Brevdo and Zhifeng   Chen and Craig   Citro and Greg   S.   Corrado and Andy   Davis and Jeffrey   Dean and Matthieu   Devin and Sanjay   Ghemawat and Ian   Goodfellow and Andrew   Harp and Geoffrey   Irving and Michael   Isard and Yangqing Jia and Rafal   Jozefowicz and Lukasz   Kaiser and Manjunath   Kudlur and Josh   Levenberg and Dandelion   Mane and Rajat   Monga and Sherry   Moore and Derek   Murray and Chris   Olah and Mike   Schuster and Jonathon   Shlens and Benoit   Steiner and Ilya   Sutskever and Kunal   Talwar and Paul   Tucker and Vincent   Vanhoucke and Vijay   Vasudevan and Fernanda   Viegas and Oriol   Vinyals and Pete   Warden and Martin   Wattenberg and Martin   Wicke and Yuan   Yu and Xiaoqiang   Zheng},
  note   = {Software available from tensorflow.org},
  title  = {TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems},
  year   = {2015},
  url    = {https://www.tensorflow.org/},
}

@Article{Rockafellar1976,
  author     = {Rockafellar, R. Tyrrell},
  journal    = {SIAM Journal on Control and Optimization},
  title      = {Monotone operators and the proximal point algorithm},
  year       = {1976},
  issn       = {0363-0129},
  number     = {5},
  pages      = {877--898},
  volume     = {14},
  doi        = {10.1137/0314056},
  fjournal   = {SIAM Journal on Control and Optimization},
  groups     = {prox_point},
  mrclass    = {47H05 (49D45)},
  mrnumber   = {410483},
  mrreviewer = {G. M. Ewing},
}

@Book{Rockafellar1998,
  author     = {Rockafellar, R. Tyrrell and Wets, Roger J.-B.},
  publisher  = {Springer-Verlag, Berlin},
  title      = {Variational analysis},
  year       = {1998},
  isbn       = {3-540-62772-3},
  series     = {Grundlehren der mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences]},
  volume     = {317},
  doi        = {10.1007/978-3-642-02431-3},
  mrclass    = {49-02 (46N10 47N10 49J52 49K40 90C30)},
  mrnumber   = {1491362},
  mrreviewer = {Francis H. Clarke},
  pages      = {xiv+733},
}

@Article{Gastaldi2017,
  author        = {Xavier Gastaldi},
  title         = {Shake-Shake regularization},
  year          = {2017},
  month         = may,
  abstract      = {The method introduced in this paper aims at helping deep learning practitioners faced with an overfit problem. The idea is to replace, in a multi-branch network, the standard summation of parallel branches with a stochastic affine combination. Applied to 3-branch residual networks, shake-shake regularization improves on the best single shot published results on CIFAR-10 and CIFAR-100 by reaching test errors of 2.86% and 15.85%. Experiments on architectures without skip connections or Batch Normalization show encouraging results and open the door to a large set of applications. Code is available at https://github.com/xgastaldi/shake-shake},
  archiveprefix = {arXiv},
  eprint        = {1705.07485},
  file          = {:http\://arxiv.org/pdf/1705.07485v2:PDF},
  keywords      = {cs.LG, cs.CV},
  primaryclass  = {cs.LG},
}

@InProceedings{Zhang2019,
  author    = {Guodong Zhang and Chaoqi Wang and Bowen Xu and Roger B. Grosse},
  booktitle = {7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019},
  title     = {Three Mechanisms of Weight Decay Regularization},
  year      = {2019},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/iclr/ZhangWXG19.bib},
  timestamp = {Thu, 25 Jul 2019 14:25:49 +0200},
  url       = {https://openreview.net/forum?id=B1lz-3Rct7},
}

@inproceedings{pieterjan2022normalizationisdead,
  author = {Hoedt, Pieter-Jan and Hochreiter, Sepp and Klambauer, Günter},
  title = {Normalization is dead, long live normalization!},
  booktitle = {ICLR Blog Track},
  year = {2022},
  note = {https://iclr-blog-track.github.io/2022/03/25/unnormalized-resnets/},
  url  = {https://iclr-blog-track.github.io/2022/03/25/unnormalized-resnets/}
}

@Article{Defossez2022,
  author  = {Alexandre Defossez and Leon Bottou and Francis Bach and Nicolas Usunier},
  journal = {Transactions on Machine Learning Research},
  title   = {A Simple Convergence Proof of Adam and Adagrad},
  year    = {2022},
  url     = {https://openreview.net/forum?id=ZPQhzTSWA7},
}

@InProceedings{Reddi2018,
  author    = {Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
  booktitle = {International Conference on Learning Representations},
  title     = {On the Convergence of Adam and Beyond},
  year      = {2018},
  url       = {https://openreview.net/forum?id=ryQu7f-RZ},
}

@InProceedings{Anonymous2023,
  author    = {Anonymous},
  booktitle = {Submitted to The Eleventh International Conference on Learning Representations},
  title     = {Towards Understanding Convergence and Generalization of AdamW},
  year      = {2023},
  note      = {under review},
  url       = {https://openreview.net/forum?id=EfTN2tSGlF},
}

@InProceedings{Ioffe2015,
  author    = {Ioffe, Sergey and Szegedy, Christian},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  title     = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  year      = {2015},
  address   = {Lille, France},
  editor    = {Bach, Francis and Blei, David},
  month     = {07--09 Jul},
  pages     = {448--456},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {37},
  abstract  = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.},
  pdf       = {http://proceedings.mlr.press/v37/ioffe15.pdf},
  url       = {https://proceedings.mlr.press/v37/ioffe15.html},
}

@InProceedings{Bos1996,
  author    = {Bos, S. and Chug, E.},
  booktitle = {Proceedings of International Conference on Neural Networks (ICNN'96)},
  title     = {Using weight decay to optimize the generalization ability of a perceptron},
  year      = {1996},
  pages     = {241-246 vol.1},
  volume    = {1},
  doi       = {10.1109/ICNN.1996.548898},
}

@InProceedings{Krogh1991,
  author    = {Krogh, Anders and Hertz, John},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {A Simple Weight Decay Can Improve Generalization},
  year      = {1991},
  editor    = {J. Moody and S. Hanson and R.P. Lippmann},
  publisher = {Morgan-Kaufmann},
  volume    = {4},
  url       = {https://proceedings.neurips.cc/paper/1991/file/8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf},
}
