@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{tirumala2022memorization,
  title={Memorization without overfitting: Analyzing the training dynamics of large language models},
  author={Tirumala, Kushal and Markosyan, Aram H and Zettlemoyer, Luke and Aghajanyan, Armen},
  journal={arXiv preprint arXiv:2205.10770},
  year={2022}
}
@article{strubell2019energy,
  title={Energy and policy considerations for deep learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={arXiv preprint arXiv:1906.02243},
  year={2019}
}
@article{chen2021bert2bert,
  title={bert2bert: Towards reusable pretrained language models},
  author={Chen, Cheng and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Qin, Yujia and Wang, Fengyu and Wang, Zhi and Chen, Xiao and Liu, Zhiyuan and Liu, Qun},
  journal={arXiv preprint arXiv:2110.07143},
  year={2021}
}
@article{bahdanau2017learning,
  title={Learning to compute word embeddings on the fly},
  author={Bahdanau, Dzmitry and Bosc, Tom and Jastrz{\k{e}}bski, Stanis{\l}aw and Grefenstette, Edward and Vincent, Pascal and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1706.00286},
  year={2017}
}
@article{gong2018frage,
  title={Frage: Frequency-agnostic word representation},
  author={Gong, Chengyue and He, Di and Tan, Xu and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{khassanov2019constrained,
  title={Constrained output embeddings for end-to-end code-switching speech recognition with only monolingual data},
  author={Khassanov, Yerbolat and Xu, Haihua and Pham, Van Tung and Zeng, Zhiping and Chng, Eng Siong and Ni, Chongjia and Ma, Bin},
  journal={arXiv preprint arXiv:1904.03802},
  year={2019}
}
@article{schick2020s,
  title={It's not just size that matters: Small language models are also few-shot learners},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2009.07118},
  year={2020}
}
@inproceedings{wu2021taking,
  title={Taking notes on the fly helps language pre-training},
  author={Wu, Qiyu and Xing, Chen and Li, Yatao and Ke, Guolin and He, Di and Liu, Tie-Yan},
  booktitle={International Conference on Learning Representations},
  year={2021}
}
@article{sennrich2015neural,
  title={Neural machine translation of rare words with subword units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1508.07909},
  year={2015}
}
@article{clark2020electra,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  journal={arXiv preprint arXiv:2003.10555},
  year={2020}
}

@inproceedings{gong2019efficient,
  title={Efficient training of bert by progressively stacking},
  author={Gong, Linyuan and He, Di and Li, Zhuohan and Qin, Tao and Wang, Liwei and Liu, Tieyan},
  booktitle={International conference on machine learning},
  pages={2337--2346},
  year={2019},
  organization={PMLR}
}

@article{strubell2019energy,
  title={Energy and policy considerations for deep learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={arXiv preprint arXiv:1906.02243},
  year={2019}
}

@article{gao2019representation,
  title={Representation degeneration problem in training natural language generation models},
  author={Gao, Jun and He, Di and Tan, Xu and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:1907.12009},
  year={2019}
}

@article{gong2018frage,
  title={Frage: Frequency-agnostic word representation},
  author={Gong, Chengyue and He, Di and Tan, Xu and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{makany2009optimising,
  title={Optimising the use of note-taking as an external cognitive aid for increasing learning},
  author={Makany, Tamas and Kemp, Jonathan and Dror, Itiel E},
  journal={British Journal of Educational Technology},
  volume={40},
  number={4},
  pages={619--635},
  year={2009},
  publisher={Wiley Online Library}
}
@article{feng2022memory,
  title={Memory Augmented Lookup Dictionary based Language Modeling for Automatic Speech Recognition},
  author={Feng, Yukun and Tu, Ming and Xia, Rui and Huang, Chuanzeng and Wang, Yuxuan},
  journal={arXiv preprint arXiv:2301.00066},
  year={2022}
}
@article{fevry2020entities,
  title={Entities as experts: Sparse memory access with entity supervision},
  author={F{\'e}vry, Thibault and Soares, Livio Baldini and FitzGerald, Nicholas and Choi, Eunsol and Kwiatkowski, Tom},
  journal={arXiv preprint arXiv:2004.07202},
  year={2020}
}
@inproceedings{guu2020retrieval,
  title={Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={International conference on machine learning},
  pages={3929--3938},
  year={2020},
  organization={PMLR}
}
@article{khandelwal2019generalization,
  title={Generalization through memorization: Nearest neighbor language models},
  author={Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
  journal={arXiv preprint arXiv:1911.00172},
  year={2019}
}

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{zipf1932selected,
  title={Selected studies of the principle of relative frequency in language},
  author={Zipf, George Kingsley},
  year={1932},
  publisher={Harvard university press}
}
@misc{zipf,
  title = {Logarithmical: Zipfâ€™s Law and the mathematics of punctuation},
  url = {\url{https://shadycharacters.co.uk/2015/10/zipfs-law}},
  author={Houston, Keith},
}




@InProceedings{pmlr-v32-santos14,
  title = 	 {Learning Character-level Representations for Part-of-Speech Tagging},
  author = 	 {Santos, Cicero Dos and Zadrozny, Bianca},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {1818--1826},
  year = 	 {2014},
  editor = 	 {Xing, Eric P. and Jebara, Tony},
  volume = 	 {32},
  number =       {2},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/santos14.pdf},
  url = 	 {https://proceedings.mlr.press/v32/santos14.html},
  abstract = 	 {Distributed word representations have recently been proven to be an invaluable resource for NLP. These representations are normally learned using neural networks and capture syntactic and semantic information about words. Information about word morphology and shape is normally ignored when learning word representations. However, for tasks like part-of-speech tagging, intra-word information is extremely useful, specially when dealing with morphologically rich languages. In this paper, we propose a deep neural network that learns character-level representation of words and associate them with usual word representations to perform POS tagging. Using the proposed approach, while avoiding the use of any handcrafted feature, we produce state-of-the-art POS taggers for two languages: English, with 97.32% accuracy on the Penn Treebank WSJ corpus; and Portuguese, with 97.47% accuracy on the Mac-Morpho corpus, where the latter represents an error reduction of 12.2% on the best previous known result.}
}

@inproceedings{kim2016character,
  title={Character-aware neural language models},
  author={Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M},
  booktitle={Thirtieth AAAI conference on artificial intelligence},
  year={2016}
}

@inproceedings{el2019parsimonious,
  title={Parsimonious morpheme segmentation with an application to enriching word embeddings},
  author={El-Kishky, Ahmed and Xu, Frank and Zhang, Aston and Han, Jiawei},
  booktitle={2019 IEEE International Conference on Big Data (Big Data)},
  pages={64--73},
  year={2019},
  organization={IEEE}
}



